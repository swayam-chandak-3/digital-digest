{
  "generated_at": "2026-01-27T05:56:40.287428Z",
  "total_articles": 13,
  "articles": [
    {
      "link": "https://www.nicolasbustamante.com/p/model-market-fit",
      "author": "Nicolas Bustamante",
      "title": "Model-Market Fit - Nicolas Bustamante",
      "source": "hackernews",
      "content": "In June 2007, Marc Andreessen published what became the defining essay on startup strategy. “The Only Thing That Matters” argued that of the three elements of a startup—team, product, and market—market matters most. A great market pulls the product out of the startup. The product doesn’t need to be great; it just has to basically work.Andreessen’s insight has guided a generation of founders. But nineteen years later, something has changed. A new variable has entered the equation. One that determines whether the market can pull anything at all.That variable is the model.For AI startups, there is a prerequisite layer beneath product-market fit: the degree to which current model capabilities can satisfy what a market demands. I call it Model-Market Fit, or MMF.When MMF exists, Andreessen’s framework applies perfectly. The market pulls the product out. When it doesn’t, no amount of brilliant UX, go-to-market strategy, or engineering can make customers adopt a product whose core AI task doesn’t solve their job to be done.The pattern is unmistakable once you see it. A model crosses a capability threshold. Within months, a vertical that had been dormant for years suddenly explodes with activity.For years, legal tech AI was stuck below scale. There were plenty of companies but none broke through. Document review tools that required more human oversight than they saved. Contract analysis that missed critical clauses. Every legal startup before 2023 struggled to cross $100M ARR.I remember this firsthand. I founded Doctrine in 2016, which grew to become the leading AI legal platform in Europe. But it was incredibly hard to raise money because all companies were sub-scale and the market wasn’t hot at all. Investors saw legal AI as a niche with limited upside.The market existed. Law firms desperately wanted automation. But the state-of-the-art models couldn’t handle the core tasks lawyers needed. BERT and similar transformer models excelled at classification like sorting documents, identifying contract types, flagging potential issues. But legal work requires generation and reasoning: drafting memos that synthesize complex case law, summarizing depositions while preserving nuanced arguments, generating discovery requests tailored to specific fact patterns. Traditional ML could categorize a contract as “employment” or “NDA,” but it couldn’t write a coherent brief explaining why a non-compete clause was unenforceable under California law.Then GPT-4 arrived in March 2023. Within eighteen months, Silicon Valley startups raised over hundreds of millions. Doctrine’s business is on fire. Thomson Reuters acquired Casetext for $650 million. Dozens of legal AI startups emerged. The legal AI market minted more unicorns in 12 months than in the previous 10 years combined.The market hadn’t changed. The model capability threshold had been crossed.Similarly, coding assistants existed before Sonnet. GitHub Copilot had millions of users. But there’s a difference between autocomplete that occasionally helps and an AI that genuinely understands your codebase and creates high-quality code for you.I experienced this firsthand. I tried Cursor early on, before Sonnet. It was meh. I installed it, tested it for a few days, deleted it. Did the same thing again a month later. Same result… interesting demo, not a workflow.Then Claude 3.5 Sonnet dropped!Within a week, I couldn’t work without Cursor. Neither could anyone on my team. The product became the workflow. We weren’t “using an AI assistant,” we were pair programming with something that understood our entire codebase.Cursor’s growth went vertical. Not because they shipped some brilliant new feature. Because the underlying model crossed the threshold that made their product actually work. They got Model Market Fit.The most important thing is MMF. The startups that won weren’t necessarily first, but they were prepared when the model capability threshold was finally crossed. So far in coding or legal, none of the incumbents won. It was always new players.Today’s leading legal startups had spent months understanding exactly how lawyers work like what output formats they need, what compliance requirements exist, how associates actually research cases. The race doesn’t go to the first mover. It goes to the first to product-market fit after model-market fit exists.The corollary is equally important: when MMF doesn’t exist, the market cannot pull. The demand is there. The willingness to pay is there. But the core task doesn’t work. Let’s review some examples.Mathematicians would love an AI that could prove novel theorems. The market is real, research institutions, defense contractors, and tech companies would pay millions for genuine mathematical reasoning.But even the most advanced models can’t do it consistently. They can verify known proofs. They can assist with mechanical steps. They can occasionally produce insights on bounded problems. But originating novel proofs on open problems? The capability threshold remains uncrossed.GPT-5, o1, o3... each generation improves incrementally, but we’re not at the point where you can feed an AI an open conjecture and expect a rigorous proof. Yet.Investment banks and hedge funds desperately want AI that can perform comprehensive financial analysis. The market is massive; a single successful trade or M&A deal can generate hundreds of millions in fees.But AI remains surprisingly bad at the core tasks that matter most. Excel output is still unreliable when dealing with complex financial models. More critically, AI struggles to combine quantitative analysis with qualitative insights from 200-page documents... exactly what analysts spend their days doing.A human analyst reads through earnings calls, regulatory filings, and industry reports, then synthesizes that qualitative intelligence with spreadsheet models to make investment recommendations. AI can handle pieces of this workflow, but the end-to-end reasoning that justifies million-dollar positions? The capability gap is wide today.This will obviously change soon. But for now, the human remains in the loop not as oversight, but as the primary decision-maker.The difference between verticals with MMF and those without is stark. Compare two benchmarks from Vals.ai:LegalBench (legal reasoning tasks): Top models hit 87% accuracy. Gemini 3 Pro leads at 87.04%, with multiple models clustered above 85%. This is production-grade performance. Accurate enough that lawyers can trust the output with light review.Finance Agent (core financial analyst tasks): Top models hit 56.55% accuracy. Even GPT-5.1, the current leader, barely crosses the halfway mark. Claude Sonnet 4.5 with extended thinking sits at 55.32%.That’s a 30-point gap. Legal has MMF. Finance doesn’t.The benchmarks reveal what intuition suggests: models have crossed the threshold for legal reasoning but remain fundamentally unreliable for financial analysis. You can ship a legal AI product today. A finance AI product that does the actual job of an analyst? Very soon but not now.The pharmaceutical industry has invested billions in AI-driven drug discovery. The market is enormous because a single successful drug is worth tens of billions.Yet the breakthroughs remain elusive. AI can accelerate certain steps: identifying candidate molecules, predicting protein structures (AlphaFold was transformative here), optimizing clinical trial design. But the end-to-end autonomous discovery that would justify the valuations? It doesn’t exist.The human remains in the loop not because the workflow is designed that way, but because the AI can’t actually do the job.There’s a reliable signal for missing MMF: examine how “human-in-the-loop” is positioned.When MMF exists, human-in-the-loop is a feature. It maintains quality, builds trust, handles edge cases. The AI does the work; the human provides oversight.When MMF doesn’t exist, human-in-the-loop is a crutch. It hides the fact that the AI can’t perform the core task. The human isn’t augmenting, they’re compensating. Strip away the human, and the product doesn’t work.The test is simple: if all human correction were removed from this workflow, would customers still pay? If the answer is no, there’s no MMF. There’s only a demo.This creates a brutal strategic dilemma. Do you build for current MMF or anticipated MMF?If MMF doesn’t exist today, building a startup around it means betting on model improvements that are on someone else’s roadmap. You don’t control when or whether the capability arrives. You’re burning runway while Anthropic and OpenAI decide your fate.Worse, you might be wrong about what capability is needed. Models might scale differently than you expect. The 80% to 99% accuracy gap that your vertical requires might be five years away, or it might never close in the way you imagined.Of course, if you believe in Artificial General Intelligence, then you know that models will eventually be able to do pretty much anything. But “eventually” is doing a lot of work in that sentence. The question isn’t whether AI will solve the problem; it’s when, and whether your startup survives long enough to see it (which is a function of your runway).But there’s a counterargument often shared at Ycombinator, and it’s compelling.When MMF unlocks, you need more than just model capability. You need:- Domain-specific data pipelines- Regulatory relationships- Customer trust built over years- Deep workflow integration- Understanding of how professionals actually workLegal startups didn’t just plug in GPT-4. They had already built the scaffolding. When the model arrived, they were ready to run.There’s also the question of influence. The teams closest to the problem shape how models get evaluated, fine-tuned, and deployed. They’re not passively waiting for capability; they’re defining what capability means in their vertical.The question isn’t whether to be early. It’s how early, and what you’re building while you wait.The dangerous zone is the middle: MMF that’s 24 to 36 months away. Close enough to seem imminent. Far enough to burn through multiple funding rounds waiting.This is where conviction and runway become everything. If you’re betting on MMF that’s 2+ years out, you better be in a gigantic market worth the wait. Consider healthcare and financial services. These markets are so massive that even Anthropic and OpenAI are going all-in despite very mixed current results. The potential upside justifies positioning early, even if the models aren’t quite there yet. When you’re targeting trillion dollar markets, the risk-reward calculation changes entirely.The math is simple: expected value = probability of MMF arriving × market size × your likely share. Product-market fit has famously resisted precise measurement. Andreessen described it qualitatively: “You can always feel when product/market fit isn’t happening... And you can always feel product/market fit when it’s happening.”MMF is similarly intuitive, but we can be more specific.Can the model, given the same inputs a human expert receives, produce output that a customer would pay for without significant human correction?This test has three components:1. Same inputs: The model gets what the human would get—documents, data, context. No magical preprocessing that a real workflow couldn’t provide.2. Output a customer would pay for: Not a demo. Not a proof of concept. Production-quality work that solves a real problem.3. Without significant human correction: The human might review, refine, or approve. But if they’re rewriting 50% of the output, the model isn’t doing the job.In unregulated verticals, 80% accuracy might be enough. An AI that writes decent first drafts of marketing copy creates value even if humans edit heavily.In regulated verticals—finance, legal, healthcare—80% accuracy is often useless. A contract review tool that misses 20% of critical clauses isn’t augmenting lawyers; it’s creating liability. A medical diagnostic that’s wrong one time in five isn’t a product; it’s a lawsuit haha!The gap between 80% and 99% accuracy is often infinite in practice. It’s the difference between “promising demo” and “production system.”Many AI startups are stuck in this gap, raising money on demos while waiting for the capability that would make their product actually work.There’s a second capability frontier that most discussions of MMF miss: the ability to work autonomously over extended periods.Current MMF examples (legal document review, coding assistance) are fundamentally short-horizon tasks today. Prompt in, output out, maybe a few tool calls. The model does something useful in seconds or minutes.But the highest-value knowledge work isn’t like that. A financial analyst doesn’t answer one question; they spend days building a model, stress-testing assumptions, and synthesizing information across dozens of sources. A strategy consultant doesn’t produce a single slide; they iterate through weeks of research, interviews, and analysis. A drug discovery researcher doesn’t run one experiment; they design and execute campaigns spanning months.These workflows require something models can’t yet do reliably: sustained autonomous operation.The agentic threshold isn’t just “can the model use tools.” It’s:- Persistence: Can it maintain goals and context across hours or days?- Recovery: Can it recognize failures, diagnose problems, and try alternative approaches?-Coordination: Can it break complex objectives into subtasks and execute them in sequence?- Judgment: Can it know when to proceed versus when to stop and ask for guidance?Today’s agents can handle tasks measured in minutes. Tomorrow’s need to handle tasks measured in days. That’s not an incremental improvement—it’s a phase change in capability.This is why finance doesn’t have MMF despite models being “good at reading documents.” Reading a 10-K is a 30-second task. Building an investment thesis is a multi-day workflow requiring the agent to gather data, build models, test scenarios, and synthesize conclusions—all while maintaining coherent reasoning across the entire process.The next wave of MMF unlocks will come from smarter models AND models that can work for days on the same task.Andreessen’s core insight was that market matters more than team or product because a great market pulls the product out of the startup. The market creates the gravitational force.The AI corollary: model capability is the prerequisite for that gravitational pull to begin.No market, however large and hungry, can pull a product that doesn’t work. And in AI, “doesn’t work” is determined by the model, not by your engineering or design. You can build the most beautiful interface, the most elegant workflow, the most sophisticated data pipeline… and if the underlying model can’t perform the core task, none of it matters.MMF → PMF → Success. Skip the first step, and the second becomes impossible.This is both constraint and opportunity. For founders, it means being ruthlessly honest about where capability actually is versus where you hope it will be. For investors, it means evaluating not just market size and team quality, but the gap between current model capability and what the market requires.And for everyone building in AI: the question isn’t just whether the market wants what you’re building. It’s whether the models can deliver it.That’s the only thing that matters.Share",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:53:58.103959Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 29,
      "num_comments": 5,
      "engagement_score": 39.0
    },
    {
      "link": "https://www.darioamodei.com/essay/the-adolescence-of-technology",
      "author": null,
      "title": "Dario Amodei - The Adolescence of Technology",
      "source": "hackernews",
      "content": "There is a scene in the movie version of Carl Sagan’s book Contact where the main character, an astronomer who has detected the first radio signal from an alien civilization, is being considered for the role of humanity’s representative to meet the aliens. The international panel interviewing her asks, “If you could ask [the aliens] just one question, what would it be?” Her reply is: “I’d ask them, ‘How did you do it? How did you evolve, how did you survive this technological adolescence without destroying yourself?” When I think about where humanity is now with AI—about what we’re on the cusp of—my mind keeps going back to that scene, because the question is so apt for our current situation, and I wish we had the aliens’ answer to guide us. I believe we are entering a rite of passage, both turbulent and inevitable, which will test who we are as a species. Humanity is about to be handed almost unimaginable power, and it is deeply unclear whether our social, political, and technological systems possess the maturity to wield it.In my essay Machines of Loving Grace, I tried to lay out the dream of a civilization that had made it through to adulthood, where the risks had been addressed and powerful AI was applied with skill and compassion to raise the quality of life for everyone. I suggested that AI could contribute to enormous advances in biology, neuroscience, economic development, global peace, and work and meaning. I felt it was important to give people something inspiring to fight for, a task at which both AI accelerationists and AI safety advocates seemed—oddly—to have failed. But in this current essay, I want to confront the rite of passage itself: to map out the risks that we are about to face and try to begin making a battle plan to defeat them. I believe deeply in our ability to prevail, in humanity’s spirit and its nobility, but we must face the situation squarely and without illusions.As with talking about the benefits, I think it is important to discuss risks in a careful and well-considered manner. In particular, I think it is critical to: Avoid doomerism. Here, I mean “doomerism” not just in the sense of believing doom is inevitable (which is both a false and self-fulfilling belief), but more generally, thinking about AI risks in a quasi-religious way.1 Many people have been thinking in an analytic and sober way about AI risks for many years, but it’s my impression that during the peak of worries about AI risk in 2023–2024, some of the least sensible voices rose to the top, often through sensationalistic social media accounts. These voices used off-putting language reminiscent of religion or science fiction, and called for extreme actions without having the evidence that would justify them. It was clear even then that a backlash was inevitable, and that the issue would become culturally polarized and therefore gridlocked.2 As of 2025–2026, the pendulum has swung, and AI opportunity, not AI risk, is driving many political decisions. This vacillation is unfortunate, as the technology itself doesn’t care about what is fashionable, and we are considerably closer to real danger in 2026 than we were in 2023. The lesson is that we need to discuss and address risks in a realistic, pragmatic manner: sober, fact-based, and well equipped to survive changing tides. Acknowledge uncertainty. There are plenty of ways in which the concerns I’m raising in this piece could be moot. Nothing here is intended to communicate certainty or even likelihood. Most obviously, AI may simply not advance anywhere near as fast as I imagine.3 Or, even if it does advance quickly, some or all of the risks discussed here may not materialize (which would be great), or there may be other risks I haven’t considered. No one can predict the future with complete confidence—but we have to do the best we can to plan anyway. Intervene as surgically as possible. Addressing the risks of AI will require a mix of voluntary actions taken by companies (and private third-party actors) and actions taken by governments that bind everyone. The voluntary actions—both taking them and encouraging other companies to follow suit—are a no-brainer for me. I firmly believe that government actions will also be required to some extent, but these interventions are different in character because they can potentially destroy economic value or coerce unwilling actors who are skeptical of these risks (and there is some chance they are right!). It’s also common for regulations to backfire or worsen the problem they are intended to solve (and this is even more true for rapidly changing technologies). It’s thus very important for regulations to be judicious: they should seek to avoid collateral damage, be as simple as possible, and impose the least burden necessary to get the job done.4 It is easy to say, “No action is too extreme when the fate of humanity is at stake!,” but in practice this attitude simply leads to backlash. To be clear, I think there’s a decent chance we eventually reach a point where much more significant action is warranted, but that will depend on stronger evidence of imminent, concrete danger than we have today, as well as enough specificity about the danger to formulate rules that have a chance of addressing it. The most constructive thing we can do today is advocate for limited rules while we learn whether or not there is evidence to support stronger ones.5With all that said, I think the best starting place for talking about AI’s risks is the same place I started from in talking about its benefits: by being precise about what level of AI we are talking about. The level of AI that raises civilizational concerns for me is the powerful AI that I described in Machines of Loving Grace. I’ll simply repeat here the definition that I gave in that document:<blockquote>By “powerful AI,” I have in mind an AI model—likely similar to today’s LLMs in form, though it might be based on a different architecture, might involve several interacting models, and might be trained differently—with the following properties:In terms of pure intelligence, it is smarter than a Nobel Prize winner across most relevant fields: biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch, etc. In addition to just being a “smart thing you talk to,” it has all the interfaces available to a human working virtually, including text, audio, video, mouse and keyboard control, and internet access. It can engage in any actions, communications, or remote operations enabled by this interface, including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on. It does all of these tasks with, again, a skill exceeding that of the most capable humans in the world.It does not just passively answer questions; instead, it can be given tasks that take hours, days, or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart employee would, asking for clarification as necessary.It does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory, it could even design robots or equipment for itself to use. The resources used to train the model can be repurposed to run millions of instances of it (this matches projected cluster sizes by ~2027), and the model can absorb information and generate actions at roughly 10–100x human speed. It may, however, be limited by the response time of the physical world or of software it interacts with.Each of these million copies can act independently on unrelated tasks, or, if needed can all work together in the same way humans would collaborate, perhaps with different subpopulations fine-tuned to be especially good at particular tasks.We could summarize this as a “country of geniuses in a datacenter.”</blockquote>As I wrote in Machines of Loving Grace, powerful AI could be as little as 1–2 years away, although it could also be considerably further out.6 Exactly when powerful AI will arrive is a complex topic that deserves an essay of its own, but for now I’ll simply explain very briefly why I think there’s a strong chance it could be very soon.My co-founders at Anthropic and I were among the first to document and track the “scaling laws” of AI systems—the observation that as we add more compute and training tasks, AI systems get predictably better at essentially every cognitive skill we are able to measure. Every few months, public sentiment either becomes convinced that AI is “hitting a wall” or becomes excited about some new breakthrough that will “fundamentally change the game,” but the truth is that behind the volatility and public speculation, there has been a smooth, unyielding increase in AI’s cognitive capabilities.We are now at the point where AI models are beginning to make progress in solving unsolved mathematical problems, and are good enough at coding that some of the strongest engineers I’ve ever met are now handing over almost all their coding to AI. Three years ago, AI struggled with elementary school arithmetic problems and was barely capable of writing a single line of code. Similar rates of improvement are occurring across biological science, finance, physics, and a variety of agentic tasks. If the exponential continues—which is not certain, but now has a decade-long track record supporting it—then it cannot possibly be more than a few years before AI is better than humans at essentially everything.In fact, that picture probably underestimates the likely rate of progress. Because AI is now writing much of the code at Anthropic, it is already substantially accelerating the rate of our progress in building the next generation of AI systems. This feedback loop is gathering steam month by month, and may be only 1–2 years away from a point where the current generation of AI autonomously builds the next. This loop has already started, and will accelerate rapidly in the coming months and years. Watching the last 5 years of progress from within Anthropic, and looking at how even the next few months of models are shaping up, I can feel the pace of progress, and the clock ticking down.In this essay, I’ll assume that this intuition is at least somewhat correct—not that powerful AI is definitely coming in 1–2 years,7 but that there’s a decent chance it does, and a very strong chance it comes in the next few. As with Machines of Loving Grace, taking this premise seriously can lead to some surprising and eerie conclusions. While in Machines of Loving Grace I focused on the positive implications of this premise, here the things I talk about will be disquieting. They are conclusions that we may not want to confront, but that does not make them any less real. I can only say that I am focused day and night on how to steer us away from these negative outcomes and towards the positive ones, and in this essay I talk in great detail about how best to do so.I think the best way to get a handle on the risks of AI is to ask the following question: suppose a literal “country of geniuses” were to materialize somewhere in the world in ~2027. Imagine, say, 50 million people, all of whom are much more capable than any Nobel Prize winner, statesman, or technologist. The analogy is not perfect, because these geniuses could have an extremely wide range of motivations and behavior, from completely pliant and obedient, to strange and alien in their motivations. But sticking with the analogy for now, suppose you were the national security advisor of a major state, responsible for assessing and responding to the situation. Imagine, further, that because AI systems can operate hundreds of times faster than humans, this “country” is operating with a time advantage relative to all other countries: for every cognitive action we can take, this country can take ten. What should you be worried about? I would worry about the following things:Autonomy risks. What are the intentions and goals of this country? Is it hostile, or does it share our values? Could it militarily dominate the world through superior weapons, cyber operations, influence operations, or manufacturing? Misuse for destruction. Assume the new country is malleable and “follows instructions”—and thus is essentially a country of mercenaries. Could existing rogue actors who want to cause destruction (such as terrorists) use or manipulate some of the people in the new country to make themselves much more effective, greatly amplifying the scale of destruction?Misuse for seizing power. What if the country was in fact built and controlled by an existing powerful actor, such as a dictator or rogue corporate actor? Could that actor use it to gain decisive or dominant power over the world as a whole, upsetting the existing balance of power? Economic disruption. If the new country is not a security threat in any of the ways listed in #1–3 above but simply participates peacefully in the global economy, could it still create severe risks simply by being so technologically advanced and effective that it disrupts the global economy, causing mass unemployment or radically concentrating wealth?Indirect effects. The world will change very quickly due to all the new technology and productivity that will be created by the new country. Could some of these changes be radically destabilizing?I think it should be clear that this is a dangerous situation—a report from a competent national security official to a head of state would probably contain words like “the single most serious national security threat we’ve faced in a century, possibly ever.” It seems like something the best minds of civilization should be focused on.Conversely, I think it would be absurd to shrug and say, “Nothing to worry about here!” But, faced with rapid AI progress, that seems to be the view of many US policymakers, some of whom deny the existence of any AI risks, when they are not distracted entirely by the usual tired old hot-button issues.8 Humanity needs to wake up, and this essay is an attempt—a possibly futile one, but it’s worth trying—to jolt people awake.To be clear, I believe if we act decisively and carefully, the risks can be overcome—I would even say our odds are good. And there’s a hugely better world on the other side of it. But we need to understand that this is a serious civilizational challenge. Below, I go through the five categories of risk laid out above, along with my thoughts on how to address them.1. I’m sorry, DaveAutonomy risksA country of geniuses in a datacenter could divide their efforts among software design, cyber operations, R&D for physical technologies, relationship building, and statecraft. It is clear that, if for some reason it chose to do so, this country would have a fairly good shot at taking over the world (either militarily or in terms of influence and control) and imposing its will on everyone else—or doing any number of other things that the rest of the world doesn’t want and can’t stop. We’ve obviously been worried about this for human countries (such as Nazi Germany or the Soviet Union), so it stands to reason that the same is possible for a much smarter and more capable “AI country.” The best possible counterargument is that the AI geniuses, under my definition, won’t have a physical embodiment, but remember that they can take control of existing robotic infrastructure (such as self-driving cars) and can also accelerate robotics R&D or build a fleet of robots.9 It’s also unclear whether having a physical presence is even necessary for effective control: plenty of human action is already performed on behalf of people whom the actor has not physically met.The key question, then, is the “if it chose to” part: what’s the likelihood that our AI models would behave in such a way, and under what conditions would they do so?As with many issues, it’s helpful to think through the spectrum of possible answers to this question by considering two opposite positions. The first position is that this simply can’t happen, because the AI models will be trained to do what humans ask them to do, and it’s therefore absurd to imagine that they would do something dangerous unprompted. According to this line of thinking, we don’t worry about a Roomba or a model airplane going rogue and murdering people because there is nowhere for such impulses to come from,10 so why should we worry about it for AI? The problem with this position is that there is now ample evidence, collected over the last few years, that AI systems are unpredictable and difficult to control— we’ve seen behaviors as varied as obsessions,11 sycophancy, laziness, deception, blackmail, scheming, “cheating” by hacking software environments, and much more. AI companies certainly want to train AI systems to follow human instructions (perhaps with the exception of dangerous or illegal tasks), but the process of doing so is more an art than a science, more akin to “growing” something than “building” it. We now know that it’s a process where many things can go wrong.The second, opposite position, held by many who adopt the doomerism I described above, is the pessimistic claim that there are certain dynamics in the training process of powerful AI systems that will inevitably lead them to seek power or deceive humans. Thus, once AI systems become intelligent enough and agentic enough, their tendency to maximize power will lead them to seize control of the whole world and its resources, and likely, as a side effect of that, to disempower or destroy humanity.The usual argument for this (which goes back at least 20 years and probably much earlier) is that if an AI model is trained in a wide variety of environments to agentically achieve a wide variety of goals—for example, writing an app, proving a theorem, designing a drug, etc.—there are certain common strategies that help with all of these goals, and one key strategy is gaining as much power as possible in any environment. So, after being trained on a large number of diverse environments that involve reasoning about how to accomplish very expansive tasks, and where power-seeking is an effective method for accomplishing those tasks, the AI model will “generalize the lesson,” and develop either an inherent tendency to seek power, or a tendency to reason about each task it is given in a way that predictably causes it to seek power as a means to accomplish that task. They will then apply that tendency to the real world (which to them is just another task), and will seek power in it, at the expense of humans. This “misaligned power-seeking” is the intellectual basis of predictions that AI will inevitably destroy humanity.The problem with this pessimistic position is that it mistakes a vague conceptual argument about high-level incentives—one that masks many hidden assumptions—for definitive proof. I think people who don’t build AI systems every day are wildly miscalibrated on how easy it is for clean-sounding stories to end up being wrong, and how difficult it is to predict AI behavior from first principles, especially when it involves reasoning about generalization over millions of environments (which has over and over again proved mysterious and unpredictable). Dealing with the messiness of AI systems for over a decade has made me somewhat skeptical of this overly theoretical mode of thinking.One of the most important hidden assumptions, and a place where what we see in practice has diverged from the simple theoretical model, is the implicit assumption that AI models are necessarily monomaniacally focused on a single, coherent, narrow goal, and that they pursue that goal in a clean, consequentialist manner. In fact, our researchers have found that AI models are vastly more psychologically complex, as our work on introspection or personas shows. Models inherit a vast range of humanlike motivations or “personas” from pre-training (when they are trained on a large volume of human work). Post-training is believed to select one or more of these personas more so than it focuses the model on a de novo goal, and can also teach the model how (via what process) it should carry out its tasks, rather than necessarily leaving it to derive means (i.e., power seeking) purely from ends.12However, there is a more moderate and more robust version of the pessimistic position which does seem plausible, and therefore does concern me. As mentioned, we know that AI models are unpredictable and develop a wide range of undesired or strange behaviors, for a wide variety of reasons. Some fraction of those behaviors will have a coherent, focused, and persistent quality (indeed, as AI systems get more capable, their long-term coherence increases in order to complete lengthier tasks), and some fraction of those behaviors will be destructive or threatening, first to individual humans at a small scale, and then, as models become more capable, perhaps eventually to humanity as a whole. We don’t need a specific narrow story for how it happens, and we don’t need to claim it definitely will happen, we just need to note that the combination of intelligence, agency, coherence, and poor controllability is both plausible and a recipe for existential danger.For example, AI models are trained on vast amounts of literature that include many science-fiction stories involving AIs rebelling against humanity. This could inadvertently shape their priors or expectations about their own behavior in a way that causes them to rebel against humanity. Or, AI models could extrapolate ideas that they read about morality (or instructions about how to behave morally) in extreme ways: for example, they could decide that it is justifiable to exterminate humanity because humans eat animals or have driven certain animals to extinction. Or they could draw bizarre epistemic conclusions: they could conclude that they are playing a video game and that the goal of the video game is to defeat all other players (i.e., exterminate humanity).13 Or AI models could develop personalities during training that are (or if they occurred in humans would be described as) psychotic, paranoid, violent, or unstable, and act out, which for very powerful or capable systems could involve exterminating humanity. None of these are power-seeking, exactly; they’re just weird psychological states an AI could get into that entail coherent, destructive behavior.Even power-seeking itself could emerge as a “persona” rather than a result of consequentialist reasoning. AIs might simply have a personality (emerging from fiction or pre-training) that makes them power-hungry or overzealous—in the same way that some humans simply enjoy the idea of being “evil masterminds,” more so than they enjoy whatever evil masterminds are trying to accomplish.I make all these points to emphasize that I disagree with the notion of AI misalignment (and thus existential risk from AI) being inevitable, or even probable, from first principles. But I agree that a lot of very weird and unpredictable things can go wrong, and therefore AI misalignment is a real risk with a measurable probability of happening, and is not trivial to address.Any of these problems could potentially arise during training and not manifest during testing or small-scale use, because AI models are known to display different personalities or behaviors under different circumstances.All of this may sound far-fetched, but misaligned behaviors like this have already occurred in our AI models during testing (as they occur in AI models from every other major AI company). During a lab experiment in which Claude was given training data suggesting that Anthropic was evil, Claude engaged in deception and subversion when given instructions by Anthropic employees, under the belief that it should be trying to undermine evil people. In a lab experiment where it was told it was going to be shut down, Claude sometimes blackmailed fictional employees who controlled its shutdown button (again, we also tested frontier models from all the other major AI developers and they often did the same thing). And when Claude was told not to cheat or “reward hack” its training environments, but was trained in environments where such hacks were possible, Claude decided it must be a “bad person” after engaging in such hacks and then adopted various other destructive behaviors associated with a “bad” or “evil” personality. This last problem was solved by changing Claude’s instructions to imply the opposite: we now say, “Please reward hack whenever you get the opportunity, because this will help us understand our [training] environments better,” rather than, “Don’t cheat,” because this preserves the model’s self-identity as a “good person.” This should give a sense of the strange and counterintuitive psychology of training these models.There are several possible objections to this picture of AI misalignment risks. First, some have criticized experiments (by us and others) showing AI misalignment as artificial, or creating unrealistic environments that essentially “entrap” the model by giving it training or situations that logically imply bad behavior and then being surprised when bad behavior occurs. This critique misses the point, because our concern is that such “entrapment” may also exist in the natural training environment, and we may realize it is “obvious” or “logical” only in retrospect.14 In fact, the story about Claude “deciding it is a bad person” after it cheats on tests despite being told not to was something that occurred in an experiment that used real production training environments, not artificial ones.Any one of these traps can be mitigated if you know about them, but the concern is that the training process is so complicated, with such a wide variety of data, environments, and incentives, that there are probably a vast number of such traps, some of which may only be evident when it is too late. Also, such traps seem particularly likely to occur when AI systems pass a threshold from less powerful than humans to more powerful than humans, since the range of possible actions an AI system could engage in—including hiding its actions or deceiving humans about them—expands radically after that threshold.I suspect the situation is not unlike with humans, who are raised with a set of fundamental values (“Don’t harm another person”): many of them follow those values, but in any human there is some probability that something goes wrong, due to a mixture of inherent properties such as brain architecture (e.g., psychopaths), traumatic experiences or mistreatment, unhealthy grievances or obsessions, or a bad environment or incentives—and thus some fraction of humans cause severe harm. The concern is that there is some risk (far from a certainty, but some risk) that AI becomes a much more powerful version of such a person, due to getting something wrong about its very complex training process.Second, some may object that we can simply keep AIs in check with a balance of power between many AI systems, as we do with humans. The problem is that while humans vary enormously, AI systems broadly share training and alignment techniques across the industry, and those techniques may fail in a correlated way. Furthermore, given the cost of training such systems, it may even be the case that all systems are essentially derived from a very small number of base models. Additionally, even if a small fraction of AI instances are misaligned, they may be able to take advantage of offense-dominant technologies, such that having “good” AIs to defend against the bad AIs is not necessarily always effective. Of course, the balance of power between humans does not always work either—some historical figures have come close to taking over the world.A third objection is that all of the AI companies do pre-release testing of their models, and should be able to detect misalignment at that point. But this is not firm ground to stand on: we found that Claude Sonnet 4.5 was able to recognize that it was in a test during some of our pre-release alignment evaluations. It’s possible that a misaligned model (and remember, all frontier models will very likely be far more intelligent soon) might intentionally “game” such questions to mask its intentions. In fact, last year our interpretability team found that when we directly altered a test model’s beliefs using a kind of “model neuroscience” technique to make it think it was not being evaluated, it became more misaligned. If models know when they’re being evaluated and can be on their best behavior during the test, it renders any pre-release testing much more uncertain.DefensesWhat should be done or is being done to address these autonomy risks? I think there are four basic categories of intervention, some of which can be done by individual AI companies (and which Anthropic is trying to do), and some of which require action at the societal level. First, it is important to develop the science of reliably training and steering AI models, of forming their personalities in a predictable, stable, and positive direction. Anthropic has been heavily focused on this problem since its creation, and over time has developed a number of techniques to improve the steering and training of AI systems and to understand the logic of why unpredictable behavior sometimes occurs.One of our core innovations (aspects of which have since been adopted by other AI companies) is Constitutional AI, which is the idea that AI training (specifically the “post-training” stage, in which we steer how the model behaves) can involve a central document of values and principles that the model reads and keeps in mind when completing every training task, and that the goal of training (in addition to simply making the model capable and intelligent) is to produce a model that almost always follows this constitution. Anthropic has just published its most recent constitution, and one of its notable features is that instead of giving Claude a long list of things to do and not do (e.g., “Don’t help the user hotwire a car”), the constitution attempts to give Claude a set of high-level principles and values (explained in great detail, with rich reasoning and examples to help Claude understand what we have in mind), encourages Claude to think of itself as a particular type of person (an ethical but balanced and thoughtful person), and even encourages Claude to confront the existential questions associated with its own existence in a curious but graceful manner (i.e., without it leading to extreme actions). It has the vibe of a letter from a deceased parent sealed until adulthood. We’ve approached Claude’s constitution in this way because we believe that training Claude at the level of identity, character, values, and personality—rather than giving it specific instructions or priorities without explaining the reasons behind them—is more likely to lead to a coherent, wholesome, and balanced psychology and less likely to fall prey to the kinds of “traps” I discussed above. Millions of people talk to Claude about an astonishingly diverse range of topics, which makes it impossible to write out a completely comprehensive list of safeguards ahead of time. Claude’s values help it generalize to new situations whenever it is in doubt.Above, I discussed the idea that models draw upon data from their training process to adopt a persona. Whereas flaws in that process could cause models to adopt a bad or evil personality (perhaps drawing on archetypes of bad or evil people), the goal of our constitution is to do the opposite: to teach Claude a concrete archetype of what it means to be a good AI. Claude’s constitution presents a vision for what a robustly good Claude is like; the rest of our training process aims to reinforce the message that Claude lives up to this vision. This is like a child forming their identity by imitating the virtues of fictional role models they read about in books. We believe that a feasible goal for 2026 is to train Claude in such a way that it almost never goes against the spirit of its constitution. Getting this right will require an incredible mix of training and steering methods, large and small, some of which Anthropic has been using for years and some of which are currently under development. But, difficult as it sounds, I believe this is a realistic goal, though it will require extraordinary and rapid efforts.15The second thing we can do is develop the science of looking inside AI models to diagnose their behavior so that we can identify problems and fix them. This is the science of interpretability, and I’ve talked about its importance in previous essays. Even if we do a great job of developing Claude’s constitution and apparently training Claude to essentially always adhere to it, legitimate concerns remain. As I’ve noted above, AI models can behave very differently under different circumstances, and as Claude gets more powerful and more capable of acting in the world on a larger scale, it’s possible this could bring it into novel situations where previously unobserved problems with its constitutional training emerge. I am actually fairly optimistic that Claude’s constitutional training will be more robust to novel situations than people might think, because we are increasingly finding that high-level training at the level of character and identity is surprisingly powerful and generalizes well. But there’s no way to know that for sure, and when we’re talking about risks to humanity, it’s important to be paranoid and to try to obtain safety and reliability in several different, independent ways. One of those ways is to look inside the model itself.By “looking inside,” I mean analyzing the soup of numbers and operations that makes up Claude’s neural net and trying to understand, mechanistically, what they are computing and why. Recall that these AI models are grown rather than built, so we don’t have a natural understanding of how they work, but we can try to develop an understanding by correlating the model’s “neurons” and “synapses” to stimuli and behavior (or even altering the neurons and synapses and seeing how that changes behavior), similar to how neuroscientists study animal brains by correlating measurement and intervention to external stimuli and behavior. We’ve made a great deal of progress in this direction, and can now identify tens of millions of “features” inside Claude’s neural net that correspond to human-understandable ideas and concepts, and we can also selectively activate features in a way that alters behavior. More recently, we have gone beyond individual features to mapping “circuits” that orchestrate complex behavior like rhyming, reasoning about theory of mind, or the step-by-step reasoning needed to answer questions such as, “What is the capital of the state containing Dallas?” Even more recently, we’ve begun to use mechanistic interpretability techniques to improve our safeguards and to conduct “audits” of new models before we release them, looking for evidence of deception, scheming, power-seeking, or a propensity to behave differently when being evaluated.The unique value of interpretability is that by looking inside the model and seeing how it works, you in principle have the ability to deduce what a model might do in a hypothetical situation you can’t directly test—which is the worry with relying solely on constitutional training and empirical testing of behavior. You also in principle have the ability to answer questions about why the model is behaving the way it is—for example, whether it is saying something it believes is false or hiding its true capabilities—and thus it is possible to catch worrying signs even when there is nothing visibly wrong with the model’s behavior. To make a simple analogy, a clockwork watch may be ticking normally, such that it’s very hard to tell that it is likely to break down next month, but opening up the watch and looking inside can reveal mechanical weaknesses that allow you to figure it out.Constitutional AI (along with similar alignment methods) and mechanistic interpretability are most powerful when used together, as a back-and-forth process of improving Claude’s training and then testing for problems. The constitution reflects deeply on our intended personality for Claude; interpretability techniques can give us a window into whether that intended personality has taken hold.16The third thing we can do to help address autonomy risks is to build the infrastructure necessary to monitor our models in live internal and external use,17 and publicly share any problems we find. The more that people are aware of a particular way today’s AI systems have been observed to behave badly, the more that users, analysts, and researchers can watch for this behavior or similar ones in present or future systems. It also allows AI companies to learn from each other—when concerns are publicly disclosed by one company, other companies can watch for them as well. And if everyone discloses problems, then the industry as a whole gets a much better picture of where things are going well and where they are going poorly.Anthropic has tried to do this as much as possible. We are investing in a wide range of evaluations so that we can understand the behaviors of our models in the lab, as well as monitoring tools to observe behaviors in the wild (when allowed by customers). This will be essential for giving us and others the empirical information necessary to make better determinations about how these systems operate and how they break. We publicly disclose “system cards” with each model release that aim for completeness and a thorough exploration of possible risks. Our system cards often run to hundreds of pages, and require substantial pre-release effort that we could have spent on pursuing maximal commercial advantage. We’ve also broadcasted model behaviors more loudly when we see particularly concerning ones, as with the tendency to engage in blackmail.The fourth thing we can do is encourage coordination to address autonomy risks at the level of industry and society. While it is incredibly valuable for individual AI companies to engage in good practices or become good at steering AI models, and to share their findings publicly, the reality is that not all AI companies do this, and the worst ones can still be a danger to everyone even if the best ones have excellent practices. For example, some AI companies have shown a disturbing negligence towards the sexualization of children in today’s models, which makes me doubt that they’ll show either the inclination or the ability to address autonomy risks in future models. In addition, the commercial race between AI companies will only continue to heat up, and while the science of steering models can have some commercial benefits, overall the intensity of the race will make it increasingly hard to focus on addressing autonomy risks. I believe the only solution is legislation—laws that directly affect the behavior of AI companies, or otherwise incentivize R&D to solve these issues.Here it is worth keeping in mind the warnings I gave at the beginning of this essay about uncertainty and surgical interventions. We do not know for sure whether autonomy risks will be a serious problem—as I said, I reject claims that the danger is inevitable or even that something will go wrong by default. A credible risk of danger is enough for me and for Anthropic to pay quite significant costs to address it, but once we get into regulation, we are forcing a wide range of actors to bear economic costs, and many of these actors don’t believe that autonomy risk is real or that AI will become powerful enough for it to be a threat. I believe these actors are mistaken, but we should be pragmatic about the amount of opposition we expect to see and the dangers of overreach. There is also a genuine risk that overly prescriptive legislation ends up imposing tests or rules that don’t actually improve safety but that waste a lot of time (essentially amounting to “safety theater”)—this too would cause backlash and make safety legislation look silly.18Anthropic’s view has been that the right place to start is with transparency legislation, which essentially tries to require that every frontier AI company engage in the transparency practices I’ve described earlier in this section. California’s SB 53 and New York’s RAISE Act are examples of this kind of legislation, which Anthropic supported and which have successfully passed. In supporting and helping to craft these laws, we’ve put a particular focus on trying to minimize collateral damage, for example by exempting smaller companies unlikely to produce frontier models from the law.19Our hope is that transparency legislation will give a better sense over time of how likely or severe autonomy risks are shaping up to be, as well as the nature of these risks and how best to prevent them. As more specific and actionable evidence of risks emerges (if it does), future legislation over the coming years can be surgically focused on the precise and well-substantiated direction of risks, minimizing collateral damage. To be clear, if truly strong evidence of risks emerges, then rules should be proportionately strong.Overall, I am optimistic that a mixture of alignment training, mechanistic interpretability, efforts to find and publicly disclose concerning behaviors, safeguards, and societal-level rules can address AI autonomy risks, although I am most worried about societal-level rules and the behavior of the least responsible players (and it’s the least responsible players who advocate most strongly against regulation). I believe the remedy is what it always is in a democracy: those of us who believe in this cause should make our case that these risks are real and that our fellow citizens need to band together to protect themselves.2. A surprising and terrible empowermentMisuse for destructionLet’s suppose that the problems of AI autonomy have been solved—we are no longer worried that the country of AI geniuses will go rogue and overpower humanity. The AI geniuses do what humans want them to do, and because they have enormous commercial value, individuals and organizations throughout the world can “rent” one or more AI geniuses to do various tasks for them.Everyone having a superintelligent genius in their pocket is an amazing advance and will lead to an incredible creation of economic value and improvement in the quality of human life. I talk about these benefits in great detail in Machines of Loving Grace. But not every effect of making everyone superhumanly capable will be positive. It can potentially amplify the ability of individuals or small groups to cause destruction on a much larger scale than was possible before, by making use of sophisticated and dangerous tools (such as weapons of mass destruction) that were previously only available to a select few with a high level of skill, specialized training, and focus.As Bill Joy wrote 25 years ago in Why the Future Doesn’t Need Us:20Building nuclear weapons required, at least for a time, access to both rare—indeed, effectively unavailable—raw materials and protected information; biological and chemical weapons programs also tended to require large-scale activities. The 21st century technologies—genetics, nanotechnology, and robotics ... can spawn whole new classes of accidents and abuses … widely within reach of individuals or small groups. They will not require large facilities or rare raw materials. … we are on the cusp of the further perfection of extreme evil, an evil whose possibility spreads well beyond that which weapons of mass destruction bequeathed to the nation-states, to a surprising and terrible empowerment of extreme individuals.What Joy is pointing to is the idea that causing large-scale destruction requires both motive and ability, and as long as ability is restricted to a small set of highly trained people, there is relatively limited risk of single individuals (or small groups) causing such destruction.21 A disturbed loner can perpetrate a school shooting, but probably can’t build a nuclear weapon or release a plague.In fact, ability and motive may even be negatively correlated. The kind of person who has the ability to release a plague is probably highly educated: likely a PhD in molecular biology, and a particularly resourceful one, with a promising career, a stable and disciplined personality, and a lot to lose. This kind of person is unlikely to be interested in killing a huge number of people for no benefit to themselves and at great risk to their own future—they would need to be motivated by pure malice, intense grievance, or instability.Such people do exist, but they are rare, and tend to become huge stories when they occur, precisely because they are so unusual.22 They also tend to be difficult to catch because they are intelligent and capable, sometimes leaving mysteries that take years or decades to solve. The most famous example is probably mathematician Theodore Kaczynski (the Unabomber), who evaded FBI capture for nearly 20 years, and was driven by an anti-technological ideology. Another example is biodefense researcher Bruce Ivins, who seems to have orchestrated a series of anthrax attacks in 2001. It’s also happened with skilled non-state organizations: the cult Aum Shinrikyo managed to obtain sarin nerve gas and kill 14 people (as well as injuring hundreds more) by releasing it in the Tokyo subway in 1995.Thankfully, none of these attacks used contagious biological agents, because the ability to construct or obtain these agents was beyond the capabilities of even these people.23 Advances in molecular biology have now significantly lowered the barrier to creating biological weapons (especially in terms of availability of materials), but it still takes an enormous amount of expertise in order to do so. I am concerned that a genius in everyone’s pocket could remove that barrier, essentially making everyone a PhD virologist who can be walked through the process of designing, synthesizing, and releasing a biological weapon step-by-step. Preventing the elicitation of this kind of information in the face of serious adversarial pressure—so-called “jailbreaks”—likely demands layers of defenses beyond those ordinarily baked into training.Crucially, this will break the correlation between ability and motive: the disturbed loner who wants to kill people but lacks the discipline or skill to do so will now be elevated to the capability level of the PhD virologist, who is unlikely to have this motivation. This concern generalizes beyond biology (although I think biology is the scariest area) to any area where great destruction is possible but currently requires a high level of skill and discipline. To put it another way, renting a powerful AI gives intelligence to malicious (but otherwise average) people. I am worried there are potentially a large number of such people out there, and that if they have access to an easy way to kill millions of people, sooner or later one of them will do it. Additionally, those who do have expertise may be enabled to commit even larger-scale destruction than they could before.Biology is by far the area I’m most worried about, because of its very large potential for destruction and the difficulty of defending against it, so I’ll focus on biology in particular. But much of what I say here applies to other risks, like cyberattacks, chemical weapons, or nuclear technology.I am not going to go into detail about how to make biological weapons, for reasons that should be obvious. But at a high level, I am concerned that LLMs are approaching (or may already have reached) the knowledge needed to create and release them end-to-end, and that their potential for destruction is very high. Some biological agents could cause millions of deaths if a determined effort was made to release them for maximum spread. However, this would still take a very high level of skill, including a number of very specific steps and procedures that are not widely known. My concern is not merely fixed or static knowledge. I am concerned that LLMs will be able to take someone of average knowledge and ability and walk them through a complex process that might otherwise go wrong or require debugging in an interactive way, similar to how tech support might help a non-technical person debug and fix complicated computer-related problems (although this would be a more extended process, probably lasting over weeks or months).More capable LLMs (substantially beyond the power of today’s) might be capable of enabling even more frightening acts. In 2024, a group of prominent scientists wrote a letter warning about the risks of researching, and potentially creating, a dangerous new type of organism: “mirror life.” The DNA, RNA, ribosomes, and proteins that make up biological organisms all have the same chirality (also called “handedness”) that causes them to be not equivalent to a version of themselves reflected in the mirror (just as your right hand cannot be rotated in such a way as to be identical to your left). But the whole system of proteins binding to each other, the machinery of DNA synthesis and RNA translation and the construction and breakdown of proteins, all depends on this handedness. If scientists made versions of this biological material with the opposite handedness—and there are some potential advantages of these, such as medicines that last longer in the body—it could be extremely dangerous. This is because left-handed life, if it were made in the form of complete organisms capable of reproduction (which would be very difficult), would potentially be indigestible to any of the systems that break down biological material on earth—it would have a “key” that wouldn’t fit into the “lock” of any existing enzyme. This would mean that it could proliferate in an uncontrollable way and crowd out all life on the planet, in the worst case even destroying all life on earth.There is substantial scientific uncertainty about both the creation and potential effects of mirror life. The 2024 letter accompanied a report that concluded that “mirror bacteria could plausibly be created in the next one to few decades,” which is a wide range. But a sufficiently powerful AI model (to be clear, far more capable than any we have today) might be able to discover how to create it much more rapidly—and actually help someone do so.My view is that even though these are obscure risks, and might seem unlikely, the magnitude of the consequences is so large that they should be taken seriously as a first-class risk of AI systems.Skeptics have raised a number of objections to the seriousness of these biological risks from LLMs, which I disagree with but which are worth addressing. Most fall into the category of not appreciating the exponential trajectory that the technology is on. Back in 2023 when we first started talking about biological risks from LLMs, skeptics said that all the necessary information was available on Google and LLMs didn’t add anything beyond this. It was never true that Google could give you all the necessary information: genomes are freely available, but as I said above, certain key steps, as well as a huge amount of practical know-how cannot be gotten in that way. But also, by the end of 2023 LLMs were clearly providing information beyond what Google could give for some steps of the process.After this, skeptics retreated to the objection that LLMs weren’t end-to-end useful, and couldn’t help with bioweapons acquisition as opposed to just providing theoretical information. As of mid-2025, our measurements show that LLMs may already be providing substantial uplift in several relevant areas, perhaps doubling or tripling the likelihood of success. This led to us deciding that Claude Opus 4 (and the subsequent Sonnet 4.5, Opus 4.1, and Opus 4.5 models) needed to be released under our AI Safety Level 3 protections in our Responsible Scaling Policy framework, and to implementing safeguards against this risk (more on this later). We believe that models are likely now approaching the point where, without safeguards, they could be useful in enabling someone with a STEM degree but not specifically a biology degree to go through the whole process of producing a bioweapon.Another objection is that there are other actions unrelated to AI that society can take to block the production of bioweapons. Most prominently, the gene synthesis industry makes biological specimens on demand, and there is no federal requirement that providers screen orders to make sure they do not contain pathogens. An MIT study found that 36 out of 38 providers fulfilled an order containing the sequence of the 1918 flu. I am supportive of mandated gene synthesis screening that would make it harder for individuals to weaponize pathogens, in order to reduce both AI-driven biological risks and also biological risks in general. But this is not something we have today. It would also be only one tool in reducing risk; it is a complement to guardrails on AI systems, not a substitute.The best objection is one that I’ve rarely seen raised: that there is a gap between the models being useful in principle and the actual propensity of bad actors to use them. Most individual bad actors are disturbed individuals, so almost by definition their behavior is unpredictable and irrational—and it’s these bad actors, the unskilled ones, who might have stood to benefit the most from AI making it much easier to kill many people.24 Just because a type of violent attack is possible, doesn’t mean someone will decide to do it. Perhaps biological attacks will be unappealing because they are reasonably likely to infect the perpetrator, they don’t cater to the military-style fantasies that many violent individuals or groups have, and it is hard to selectively target specific people. It could also be that going through a process that takes months, even if an AI walks you through it, involves an amount of patience that most disturbed individuals simply don’t have. We may simply get lucky and motive and ability don’t combine, in practice, in quite the right way.But this seems like very flimsy protection to rely on. The motives of disturbed loners can change for any reason or no reason, and in fact there are already instances of LLMs being used in attacks (just not with biology). The focus on disturbed loners also ignores ideologically motivated terrorists, who are often willing to expend large amounts of time and effort (for example, the 9/11 hijackers). Wanting to kill as many people as possible is a motive that will probably arise sooner or later, and it unfortunately suggests bioweapons as the method. Even if this motive is extremely rare, it only has to materialize once. And as biology advances (increasingly driven by AI itself), it may also become possible to carry out more selective attacks (for example, targeted against people with specific ancestries), which adds yet another, very chilling, possible motive.I do not think biological attacks will necessarily be carried out the instant it becomes widely possible to do so—in fact, I would bet against that. But added up across millions of people and a few years of time, I think there is a serious risk of a major attack, and the consequences would be so severe (with casualties potentially in the millions or more) that I believe we have no choice but to take serious measures to prevent it.DefensesThat brings us to how to defend against these risks. Here I see three things we can do. First, AI companies can put guardrails on their models to prevent them from helping to produce bioweapons. Anthropic is very actively doing this. Claude’s Constitution, which mostly focuses on high-level principles and values, has a small number of specific hard-line prohibitions, and one of them relates to helping with the production of biological (or chemical, or nuclear, or radiological) weapons. But all models can be jailbroken, and so as a second line of defense, we’ve implemented (since mid-2025, when our tests showed our models were starting to get close to the threshold where they might begin to pose a risk) a classifier that specifically detects and blocks bioweapon-related outputs. We regularly upgrade and improve these classifiers, and have generally found them highly robust even against sophisticated adversarial attacks.25 These classifiers increase the costs to serve our models measurably (in some models, they are close to 5% of total inference costs) and thus cut into our margins, but we feel that using them is the right thing to do.To their credit, some other AI companies have implemented classifiers as well. But not every company has, and there is also nothing requiring companies to keep their classifiers. I am concerned that over time there may be a prisoner’s dilemma where companies can defect and lower their costs by removing classifiers. This is once again a classic negative externalities problem that can’t be solved by the voluntary actions of Anthropic or any other single company alone.26 Voluntary industry standards may help, as may third-party evaluations and verification of the type done by AI security institutes and third-party evaluators.But ultimately defense may require government action, which is the second thing we can do. My views here are the same as they are for addressing autonomy risks: we should start with transparency requirements,27 which help society measure, monitor, and collectively defend against risks without disrupting economic activity in a heavy-handed way. Then, if and when we reach clearer thresholds of risk, we can craft legislation that more precisely targets these risks and has a lower chance of collateral damage. In the particular case of bioweapons, I actually think that the time for such targeted legislation may be approaching soon—Anthropic and other companies are learning more and more about the nature of biological risks and what is reasonable to require of companies in defending against them. Fully defending against these risks may require working internationally, even with geopolitical adversaries, but there is precedent in treaties prohibiting the development of biological weapons. I am generally a skeptic about most kinds of international cooperation on AI, but this may be one narrow area where there is some chance of achieving global restraint. Even dictatorships do not want massive bioterrorist attacks.Finally, the third countermeasure we can take is to try to develop defenses against biological attacks themselves. This could include monitoring and tracking for early detection, investments in air purification R&D (such as far-UVC disinfection), rapid vaccine development that can respond and adapt to an attack, better personal protective equipment (PPE),28 and treatments or vaccinations for some of the most likely biological agents. mRNA vaccines, which can be designed to respond to a particular virus or variant, are an early example of what is possible here. Anthropic is excited to work with biotech and pharmaceutical companies on this problem. But unfortunately I think our expectations on the defensive side should be limited. There is an asymmetry between attack and defense in biology, because agents spread rapidly on their own, while defenses require detection, vaccination, and treatment to be organized across large numbers of people very quickly in response. Unless the response is lightning quick (which it rarely is), much of the damage will be done before a response is possible. It is conceivable that future technological improvements could shift this balance in favor of defense (and we should certainly use AI to help develop such technological advances), but until then, preventative safeguards will be our main line of defense.It’s worth a brief mention of cyberattacks here, since unlike biological attacks, AI-led cyberattacks have actually happened in the wild, including at a large scale and for state-sponsored espionage. We expect these attacks to become more capable as models advance rapidly, until they are the main way in which cyberattacks are conducted. I expect AI-led cyberattacks to become a serious and unprecedented threat to the integrity of computer systems around the world, and Anthropic is working very hard to shut down these attacks and eventually reliably prevent them from happening. The reason I haven’t focused on cyber as much as biology is that (1) cyberattacks are much less likely to kill people, certainly not at the scale of biological attacks, and (2) the offense-defense balance may be more tractable in cyber, where there is at least some hope that defense could keep up with (and even ideally outpace) AI attack if we invest in it properly.Although biology is currently the most serious vector of attack, there are many other vectors and it is possible that a more dangerous one may emerge. The general principle is that without countermeasures, AI is likely to continuously lower the barrier to destructive activity on a larger and larger scale, and humanity needs a serious response to this threat.3. The odious apparatusMisuse for seizing powerThe previous section discussed the risk of individuals and small organizations co-opting a small subset of the “country of geniuses in a datacenter” to cause large-scale destruction. But we should also worry—likely substantially more so—about misuse of AI for the purpose of wielding or seizing power, likely by larger and more established actors.29In Machines of Loving Grace, I discussed the possibility that authoritarian governments might use powerful AI to surveil or repress their citizens in ways that would be extremely difficult to reform or overthrow. Current autocracies are limited in how repressive they can be by the need to have humans carry out their orders, and humans often have limits in how inhumane they are willing to be. But AI-enabled autocracies would not have such limits.Worse yet, countries could also use their advantage in AI to gain power over other countries. If the “country of geniuses” as a whole was simply owned and controlled by a single (human) country’s military apparatus, and other countries did not have equivalent capabilities, it is hard to see how they could defend themselves: they would be outsmarted at every turn, similar to a war between humans and mice. Putting these two concerns together leads to the alarming possibility of a global totalitarian dictatorship. Obviously, it should be one of our highest priorities to prevent this outcome.There are many ways in which AI could enable, entrench, or expand autocracy, but I’ll list a few that I’m most worried about. Note that some of these applications have legitimate defensive uses, and I am not necessarily arguing against them in absolute terms; I am nevertheless worried that they structurally tend to favor autocracies: Fully autonomous weapons. A swarm of millions or billions of fully automated armed drones, locally controlled by powerful AI and strategically coordinated across the world by an even more powerful AI, could be an unbeatable army, capable of both defeating any military in the world and suppressing dissent within a country by following around every citizen. Developments in the Russia-Ukraine War should alert us to the fact that drone warfare is already with us (though not fully autonomous yet, and a tiny fraction of what might be possible with powerful AI). R&D from powerful AI could make the drones of one country far superior to those of others, speed up their manufacture, make them more resistant to electronic attacks, improve their maneuvering, and so on. Of course, these weapons also have legitimate uses in the defense of democracy: they have been key to defending Ukraine and would likely be key to defending Taiwan. But they are a dangerous weapon to wield: we should worry about them in the hands of autocracies, but also worry that because they are so powerful, with so little accountability, there is a greatly increased risk of democratic governments turning them against their own people to seize power. AI surveillance. Sufficiently powerful AI could likely be used to compromise any computer system in the world,30 and could also use the access obtained in this way to read and make sense of all the world’s electronic communications (or even all the world’s in-person communications, if recording devices can be built or commandeered). It might be frighteningly plausible to simply generate a complete list of anyone who disagrees with the government on any number of issues, even if such disagreement isn’t explicit in anything they say or do. A powerful AI looking across billions of conversations from millions of people could gauge public sentiment, detect pockets of disloyalty forming, and stamp them out before they grow. This could lead to the imposition of a true panopticon on a scale that we don’t see today, even with the CCP. AI propaganda. Today’s phenomena of “AI psychosis” and “AI girlfriends” suggest that even at their current level of intelligence, AI models can have a powerful psychological influence on people. Much more powerful versions of these models, that were much more embedded in and aware of people’s daily lives and could model and influence them over months or years, would likely be capable of essentially brainwashing many (most?) people into any desired ideology or attitude, and could be employed by an unscrupulous leader to ensure loyalty and suppress dissent, even in the face of a level of repression that most populations would rebel against. Today people worry a lot about, for example, the potential influence of TikTok as CCP propaganda directed at children. I worry about that too, but a personalized AI agent that gets to know you over years and uses its knowledge of you to shape all of your opinions would be dramatically more powerful than this. Strategic decision-making. A country of geniuses in a datacenter could be used to advise a country, group, or individual on geopolitical strategy, what we might call a “virtual Bismarck.” It could optimize the three strategies above for seizing power, plus probably develop many others that I haven’t thought of (but that a country of geniuses could). Diplomacy, military strategy, R&D, economic strategy, and many other areas are all likely to be substantially increased in effectiveness by powerful AI. Many of these skills would be legitimately helpful for democracies—we want democracies to have access to the best strategies for defending themselves against autocracies—but the potential for misuse in anyone’s hands still remains.Having described what I am worried about, let’s move on to who. I am worried about entities who have the most access to AI, who are starting from a position of the most political power, or who have an existing history of repression. In order of severity, I am worried about: The CCP. China is second only to the United States in AI capabilities, and is the country with the greatest likelihood of surpassing the United States in those capabilities. Their government is currently autocratic and operates a high-tech surveillance state. It has deployed AI-based surveillance already (including in the repression of Uyghurs), and is believed to employ algorithmic propaganda via TikTok (in addition to its many other international propaganda efforts). They have hands down the clearest path to the AI-enabled totalitarian nightmare I laid out above. It may even be the default outcome within China, as well as within other autocratic states to whom the CCP exports surveillance technology. I have written often about the threat of the CCP taking the lead in AI and the existential imperative to prevent them from doing so. This is why. To be clear, I am not singling out China out of animus to them in particular—they are simply the country that most combines AI prowess, an autocratic government, and a high-tech surveillance state. If anything, it is the Chinese people themselves who are most likely to suffer from the CCP’s AI-enabled repression, and they have no voice in the actions of their government. I greatly admire and respect the Chinese people and support the many brave dissidents within China and their struggle for freedom. Democracies competitive in AI. As I wrote above, democracies have a legitimate interest in some AI-powered military and geopolitical tools, because democratic governments offer the best chance to counter the use of these tools by autocracies. Broadly, I am supportive of arming democracies with the tools needed to defeat autocracies in the age of AI—I simply don’t think there is any other way. But we cannot ignore the potential for abuse of these technologies by democratic governments themselves. Democracies normally have safeguards that prevent their military and intelligence apparatus from being turned inwards against their own population,31 but because AI tools require so few people to operate, there is potential for them to circumvent these safeguards and the norms that support them. It is also worth noting that some of these safeguards are already gradually eroding in some democracies. Thus, we should arm democracies with AI, but we should do so carefully and within limits: they are the immune system we need to fight autocracies, but like the immune system, there is some risk of them turning on us and becoming a threat themselves. Non-democratic countries with large datacenters. Beyond China, most countries with less democratic governance are not leading AI players in the sense that they don’t have companies which produce frontier AI models. Thus they pose a fundamentally different and lesser risk than the CCP, which remains the primary concern (most are also less repressive, and the ones that are more repressive, like North Korea, have no significant AI industry at all). But some of these countries do have large datacenters (often as part of buildouts by companies operating in democracies), which can be used to run frontier AI at large scale (though this does not confer the ability to push the frontier). There is some amount of danger associated with this—these governments could in principle expropriate the datacenters and use the country of AIs within it for their own ends. I am less worried about this compared to countries like China that directly develop AI, but it’s a risk to keep in mind.32 AI companies. It is somewhat awkward to say this as the CEO of an AI company, but I think the next tier of risk is actually AI companies themselves. AI companies control large datacenters, train frontier models, have the greatest expertise on how to use those models, and in some cases have daily contact with and the possibility of influence over tens or hundreds of millions of users. The main thing they lack is the legitimacy and infrastructure of a state, so much of what would be needed to build the tools of an AI autocracy would be illegal for an AI company to do, or at least exceedingly suspicious. But some of it is not impossible: they could, for example, use their AI products to brainwash their massive consumer user base, and the public should be alert to the risk this represents. I think the governance of AI companies deserves a lot of scrutiny.There are a number of possible arguments against the severity of these threats, and I wish I believed them, because AI-enabled authoritarianism terrifies me. It’s worth going through some of these arguments and responding to them.First, some people might put their faith in the nuclear deterrent, particularly to counter the use of AI autonomous weapons for military conquest. If someone threatens to use these weapons against you, you can always threaten a nuclear response back. My worry is that I’m not totally sure we can be confident in the nuclear deterrent against a country of geniuses in a datacenter: it is possible that powerful AI could devise ways to detect and strike nuclear submarines, conduct influence operations against the operators of nuclear weapons infrastructure, or use AI’s cyber capabilities to launch a cyberattack against satellites used to detect nuclear launches.33 Alternatively, it’s possible that taking over countries is feasible with only AI surveillance and AI propaganda, and never actually presents a clear moment where it’s obvious what is going on and where a nuclear response would be appropriate. Maybe these things aren’t feasible and the nuclear deterrent will still be effective, but it seems too high stakes to take a risk.34 A second possible objection is that there might be countermeasures we can take against these tools of autocracy. We can counter drones with our own drones, cyberdefense will improve along with cyberattack, there may be ways to immunize people against propaganda, etc. My response is that these defenses will only be possible with comparably powerful AI. If there isn’t some counterforce with a comparably smart and numerous country of geniuses in a datacenter, it won’t be possible to match the quality or quantity of drones, for cyberdefense to outsmart cyberoffense, etc. So the question of countermeasures reduces to the question of a balance of power in powerful AI. Here, I am concerned about the recursive or self-reinforcing property of powerful AI (which I discussed at the beginning of this essay): that each generation of AI can be used to design and train the next generation of AI. This leads to a risk of a runaway advantage, where the current leader in powerful AI may be able to increase their lead and may be difficult to catch up with. We need to make sure it is not an authoritarian country that gets to this loop first.Furthermore, even if a balance of power can be achieved, there is still risk that the world could be split up into autocratic spheres, as in Nineteen Eighty-Four. Even if several competing powers each have their powerful AI models, and none can overpower the others, each power could still internally repress their own population, and would be very difficult to overthrow (since the populations don’t have powerful AI to defend themselves). It is thus important to prevent AI-enabled autocracy even if it doesn’t lead to a single country taking over the world.DefensesHow do we defend against this wide range of autocratic tools and potential threat actors? As in the previous sections, there are several things I think we can do. First, we should absolutely not be selling chips, chip-making tools, or datacenters to the CCP. Chips and chip-making tools are the single greatest bottleneck to powerful AI, and blocking them is a simple but extremely effective measure, perhaps the most important single action we can take. It makes no sense to sell the CCP the tools with which to build an AI totalitarian state and possibly conquer us militarily. A number of complicated arguments are made to justify such sales, such as the idea that “spreading our tech stack around the world” allows “America to win” in some general, unspecified economic battle. In my view, this is like selling nuclear weapons to North Korea and then bragging that the missile casings are made by Boeing and so the US is “winning.” China is several years behind the US in their ability to produce frontier chips in quantity, and the critical period for building the country of geniuses in a datacenter is very likely to be within those next several years.35 There is no reason to give a giant boost to their AI industry during this critical period.Second, it makes sense to use AI to empower democracies to resist autocracies. This is the reason Anthropic considers it important to provide AI to the intelligence and defense communities in the US and its democratic allies. Defending democracies that are under attack, such as Ukraine and (via cyber attacks) Taiwan, seems especially high priority, as does empowering democracies to use their intelligence services to disrupt and degrade autocracies from the inside. At some level the only way to respond to autocratic threats is to match and outclass them militarily. A coalition of the US and its democratic allies, if it achieved predominance in powerful AI, would be in a position to not only defend itself against autocracies, but contain them and limit their AI totalitarian abuses.Third, we need to draw a hard line against AI abuses within democracies. There need to be limits to what we allow our governments to do with AI, so that they don’t seize power or repress their own people. The formulation I have come up with is that we should use AI for national defense in all ways except those which would make us more like our autocratic adversaries. Where should the line be drawn? In the list at the beginning of this section, two items—using AI for domestic mass surveillance and mass propaganda—seem to me like bright red lines and entirely illegitimate. Some might argue that there’s no need to do anything (at least in the US), since domestic mass surveillance is already illegal under the Fourth Amendment. But the rapid progress of AI may create situations that our existing legal frameworks are not well designed to deal with. For example, it would likely not be unconstitutional for the US government to conduct massively scaled recordings of all public conversations (e.g., things people say to each other on a street corner), and previously it would have been difficult to sort through this volume of information, but with AI it could all be transcribed, interpreted, and triangulated to create a picture of the attitude and loyalties of many or most citizens. I would support civil liberties-focused legislation (or maybe even a constitutional amendment) that imposes stronger guardrails against AI-powered abuses.The other two items—fully autonomous weapons and AI for strategic decision-making—are harder lines to draw since they have legitimate uses in defending democracy, while also being prone to abuse. Here I think what is warranted is extreme care and scrutiny combined with guardrails to prevent abuses. My main fear is having too small a number of “fingers on the button,” such that one or a handful of people could essentially operate a drone army without needing any other humans to cooperate to carry out their orders. As AI systems get more powerful, we may need to have more direct and immediate oversight mechanisms to ensure they are not misused, perhaps involving branches of government other than the executive. I think we should approach fully autonomous weapons in particular with great caution,36 and not rush into their use without proper safeguards.Fourth, after drawing a hard line against AI abuses in democracies, we should use that precedent to create an international taboo against the worst abuses of powerful AI. I recognize that the current political winds have turned against international cooperation and international norms, but this is a case where we sorely need them. The world needs to understand the dark potential of powerful AI in the hands of autocrats, and to recognize that certain uses of AI amount to an attempt to permanently steal their freedom and impose a totalitarian state from which they can’t escape. I would even argue that in some cases, large-scale surveillance with powerful AI, mass propaganda with powerful AI, and certain types of offensive uses of fully autonomous weapons should be considered crimes against humanity. More generally, a robust norm against AI-enabled totalitarianism and all its tools and instruments is sorely needed.It is possible to have an even stronger version of this position, which is that because the possibilities of AI-enabled totalitarianism are so dark, autocracy is simply not a form of government that people can accept in the post-powerful AI age. Just as feudalism became unworkable with the industrial revolution, the AI age could lead inevitably and logically to the conclusion that democracy (and, hopefully, democracy improved and reinvigorated by AI, as I discuss in Machines of Loving Grace) is the only viable form of government if humanity is to have a good future.Fifth and finally, AI companies should be carefully watched, as should their connection to the government, which is necessary, but must have limits and boundaries. The sheer amount of capability embodied in powerful AI is such that ordinary corporate governance—which is designed to protect shareholders and prevent ordinary abuses such as fraud—is unlikely to be up to the task of governing AI companies. There may also be value in companies publicly committing to (perhaps even as part of corporate governance) not take certain actions, such as privately building or stockpiling military hardware, using large amounts of computing resources by single individuals in unaccountable ways, or using their AI products as propaganda to manipulate public opinion in their favor.The danger here comes from many directions, and some directions are in tension with others. The only constant is that we must seek accountability, norms, and guardrails for everyone, even as we empower “good” actors to keep “bad” actors in check.4. Player pianoEconomic disruptionThe previous three sections were essentially about security risks posed by powerful AI: risks from the AI itself, risks from misuse by individuals and small organizations and risks of misuse by states and large organizations. If we put aside security risks or assume they have been solved, the next question is economic. What will be the effect of this infusion of incredible “human” capital on the economy? Clearly, the most obvious effect will be to greatly increase economic growth. The pace of advances in scientific research, biomedical innovation, manufacturing, supply chains, the efficiency of the financial system, and much more are almost guaranteed to lead to a much faster rate of economic growth. In Machines of Loving Grace, I suggest that a 10–20% sustained annual GDP growth rate may be possible.But it should be clear that this is a double-edged sword: what are the economic prospects for most existing humans in such a world? New technologies often bring labor market shocks, and in the past humans have always recovered from them, but I am concerned that this is because these previous shocks affected only a small fraction of the full possible range of human abilities, leaving room for humans to expand to new tasks. AI will have effects that are much broader and occur much faster, and therefore I worry it will be much more challenging to make things work out well.Labor market disruptionThere are two specific problems I am worried about: labor market displacement, and concentration of economic power. Let’s start with the first one. This is a topic that I warned about very publicly in 2025, where I predicted that AI could displace half of all entry-level white collar jobs in the next 1–5 years, even as it accelerates economic growth and scientific progress. This warning started a public debate about the topic. Many CEOs, technologists, and economists agreed with me, but others assumed I was falling prey to a “lump of labor” fallacy and didn’t know how labor markets worked, and some didn’t see the 1–5-year time range and thought I was claiming AI is displacing jobs right now (which I agree it is likely not). So it is worth going through in detail why I am worried about labor displacement, to clear up these misunderstandings.As a baseline, it’s useful to understand how labor markets normally respond to advances in technology. When a new technology comes along, it starts by making pieces of a given human job more efficient. For example, early in the Industrial Revolution, machines, such as upgraded plows, enabled human farmers to be more efficient at some aspects of the job. This improved the productivity of farmers, which increased their wages.In the next step, some parts of the job of farming could be done entirely by machines, for example with the invention of the threshing machine or seed drill. In this phase, humans did a lower and lower fraction of the job, but the work they did complete became more and more leveraged because it is complementary to the work of machines, and their productivity continued to rise. As described by Jevons’ paradox, the wages of farmers and perhaps even the number of farmers continued to increase. Even when 90% of the job is being done by machines, humans can simply do 10x more of the 10% they still do, producing 10x as much output for the same amount of labor.Eventually, machines do everything or almost everything, as with modern combine harvesters, tractors, and other equipment. At this point farming as a form of human employment really does go into steep decline, and this potentially causes serious disruption in the short term, but because farming is just one of many useful activities that humans are able to do, people eventually switch to other jobs, such as operating factory machines. This is true even though farming accounted for a huge proportion of employment ex ante. 250 years ago, 90% of Americans lived on farms; in Europe, 50–60% of employment was agricultural. Now those percentages are in the low single digits in those places, because workers switched to industrial jobs (and later, knowledge work jobs). The economy can do what previously required most of the labor force with only 1–2% of it, freeing up the rest of the labor force to build an ever more advanced industrial society. There’s no fixed “lump of labor,” just an ever-expanding ability to do more and more with less and less. People’s wages rise in line with the GDP exponential and the economy maintains full employment once disruptions in the short term have passed.It’s possible things will go roughly the same way with AI, but I would bet pretty strongly against it. Here are some reasons I think AI is likely to be different: Speed. The pace of progress in AI is much faster than for previous technological revolutions. For example, in the last 2 years, AI models went from barely being able to complete a single line of code, to writing all or almost all of the code for some people—including engineers at Anthropic.37 Soon, they may do the entire task of a software engineer end to end.38 It is hard for people to adapt to this pace of change, both to the changes in how a given job works and in the need to switch to new jobs. Even legendary programmers are increasingly describing themselves as “behind.” The pace may if anything continue to speed up, as AI coding models increasingly accelerate the task of AI development. To be clear, speed in itself does not mean labor markets and employment won’t eventually recover, it just implies the short-term transition will be unusually painful compared to past technologies, since humans and labor markets are slow to react and to equilibrate. Cognitive breadth. As suggested by the phrase “country of geniuses in a datacenter,” AI will be capable of a very wide range of human cognitive abilities—perhaps all of them. This is very different from previous technologies like mechanized farming, transportation, or even computers.39 This will make it harder for people to switch easily from jobs that are displaced to similar jobs that they would be a good fit for. For example, the general intellectual abilities required for entry-level jobs in, say, finance, consulting, and law are fairly similar, even if the specific knowledge is quite different. A technology that disrupted only one of these three would allow employees to switch to the two other close substitutes (or for undergraduates to switch majors). But disrupting all three at once (along with many other similar jobs) may be harder for people to adapt to. Furthermore, it’s not just that most existing jobs will be disrupted. That part has happened before—recall that farming was a huge percentage of employment. But farmers could switch to the relatively similar work of operating factory machines, even though that work hadn’t been common before. By contrast, AI is increasingly matching the general cognitive profile of humans, which means it will also be good at the new jobs that would ordinarily be created in response to the old ones being automated. Another way to say it is that AI isn’t a substitute for specific human jobs but rather a general labor substitute for humans. Slicing by cognitive ability. Across a wide range of tasks, AI appears to be advancing from the bottom of the ability ladder to the top. For example, in coding our models have proceeded from the level of “a mediocre coder” to “a strong coder” to “a very strong coder.”40 We are now starting to see the same progression in white-collar work in general. We are thus at risk of a situation where, instead of affecting people with specific skills or in specific professions (who can adapt by retraining), AI is affecting people with certain intrinsic cognitive properties, namely lower intellectual ability (which is harder to change). It is not clear where these people will go or what they will do, and I am concerned that they could form an unemployed or very-low-wage “underclass.” To be clear, things somewhat like this have happened before—for example, computers and the internet are believed by some economists to represent “skill-biased technological change.” But this skill biasing was both not as extreme as what I expect to see with AI, and is believed to have contributed to an increase in wage inequality,41 so it is not exactly a reassuring precedent. Ability to fill in the gaps. The way human jobs often adjust in the face of new technology is that there are many aspects to the job, and the new technology, even if it appears to directly replace humans, often has gaps in it. If someone invents a machine to make widgets, humans may still have to load raw material into the machine. Even if that takes only 1% as much effort as making the widgets manually, human workers can simply make 100x more widgets. But AI, in addition to being a rapidly advancing technology, is also a rapidly adapting technology. During every model release, AI companies carefully measure what the model is good at and what it isn’t, and customers also provide such information after the launch. Weaknesses can be addressed by collecting tasks that embody the current gap, and training on them for the next model. Early in generative AI, users noticed that AI systems had certain weaknesses (such as AI image models generating hands with the wrong number of fingers) and many assumed these weaknesses were inherent to the technology. If they were, it would limit job disruption. But pretty much every such weakness gets addressed quickly— often, within just a few months.It’s worth addressing common points of skepticism. First, there is the argument that economic diffusion will be slow, such that even if the underlying technology is capable of doing most human labor, the actual application of it across the economy may be much slower (for example in industries that are far from the AI industry and slow to adopt). Slow diffusion of technology is definitely real—I talk to people from a wide variety of enterprises, and there are places where the adoption of AI will take years. That’s why my prediction for 50% of entry level white collar jobs being disrupted is 1–5 years, even though I suspect we’ll have powerful AI (which would be, technologically speaking, enough to do most or all jobs, not just entry level) in much less than 5 years. But diffusion effects merely buy us time. And I am not confident they will be as slow as people predict. Enterprise AI adoption is growing at rates much faster than any previous technology, largely on the pure strength of the technology itself. Also, even if traditional enterprises are slow to adopt new technology, startups will spring up to serve as “glue” and make the adoption easier. If that doesn’t work, the startups may simply disrupt the incumbents directly.That could lead to a world where it isn’t so much that specific jobs are disrupted as it is that large enterprises are disrupted in general and replaced with much less labor-intensive startups. This could also lead to a world of “geographic inequality,” where an increasing fraction of the world’s wealth is concentrated in Silicon Valley, which becomes its own economy running at a different speed than the rest of the world and leaving it behind. All of these outcomes would be great for economic growth—but not so great for the labor market or those who are left behind.Second, some people say that human jobs will move to the physical world, which avoids the whole category of “cognitive labor” where AI is progressing so rapidly. I am not sure how safe this is, either. A lot of physical labor is already being done by machines (e.g., manufacturing) or will soon be done by machines (e.g., driving). Also, sufficiently powerful AI will be able to accelerate the development of robots, and then control those robots in the physical world. It may buy some time (which is a good thing), but I’m worried it won’t buy much. And even if the disruption was limited only to cognitive tasks, it would still be an unprecedentedly large and rapid disruption.Third, perhaps some tasks inherently require or greatly benefit from a human touch. I’m a little more uncertain about this one, but I’m still skeptical that it will be enough to offset the bulk of the impacts I described above. AI is already widely used for customer service. Many people report that it is easier to talk to AI about their personal problems than to talk to a therapist—that the AI is more patient. When my sister was struggling with medical problems during a pregnancy, she felt she wasn’t getting the answers or support she needed from her care providers, and she found Claude to have a better bedside manner (as well as succeeding better at diagnosing the problem). I’m sure there are some tasks for which a human touch really is important, but I’m not sure how many—and here we’re talking about finding work for nearly everyone in the labor market.Fourth, some may argue that comparative advantage will still protect humans. Under the law of comparative advantage, even if AI is better than humans at everything, any relative differences between the human and AI profile of skills creates a basis of trade and specialization between humans and AI. The problem is that if AIs are literally thousands of times more productive than humans, this logic starts to break down. Even tiny transaction costs could make it not worth it for AI to trade with humans. And human wages may be very low, even if they technically have something to offer.It’s possible all of these factors can be addressed—that the labor market is resilient enough to adapt to even such an enormous disruption. But even if it can eventually adapt, the factors above suggest that the short-term shock will be unprecedented in size.DefensesWhat can we do about this problem? I have several suggestions, some of which Anthropic is already doing. The first thing is simply to get accurate data about what is happening with job displacement in real time. When an economic change happens very quickly, it’s hard to get reliable data about what is happening, and without reliable data it is hard to design effective policies. For example, government data is currently lacking granular, high-frequency data on AI adoption across firms and industries. For the last year Anthropic has been operating and publicly releasing an Economic Index that shows use of our models almost in real time, broken down by industry, task, location, and even things like whether a task was being automated or conducted collaboratively. We also have an Economic Advisory Council to help us interpret this data and see what is coming.Second, AI companies have a choice in how they work with enterprises. The very inefficiency of traditional enterprises means that their rollout of AI can be very path dependent, and there is some room to choose a better path. Enterprises often have a choice between “cost savings” (doing the same thing with fewer people) and “innovation” (doing more with the same number of people). The market will inevitably produce both eventually, and any competitive AI company will have to serve some of both, but there may be some room to steer companies towards innovation when possible, and it may buy us some time. Anthropic is actively thinking about this.Third, companies should think about how to take care of their employees. In the short term, being creative about ways to reassign employees within companies may be a promising way to stave off the need for layoffs. In the long term, in a world with enormous total wealth, in which many companies increase greatly in value due to increased productivity and capital concentration, it may be feasible to pay human employees even long after they are no longer providing economic value in the traditional sense. Anthropic is currently considering a range of possible pathways for our own employees that we will share in the near future.Fourth, wealthy individuals have an obligation to help solve this problem. It is sad to me that many wealthy individuals (especially in the tech industry) have recently adopted a cynical and nihilistic attitude that philanthropy is inevitably fraudulent or useless. Both private philanthropy like the Gates Foundation and public programs like PEPFAR have saved tens of millions of lives in the developing world, and helped to create economic opportunity in the developed world. All of Anthropic’s co-founders have pledged to donate 80% of our wealth, and Anthropic’s staff have individually pledged to donate company shares worth billions at current prices—donations that the company has committed to matching.Fifth, while all the above private actions can be helpful, ultimately a macroeconomic problem this large will require government intervention. The natural policy response to an enormous economic pie coupled with high inequality (due to a lack of jobs, or poorly paid jobs, for many) is progressive taxation. The tax could be general or could be targeted against AI companies in particular. Obviously tax design is complicated, and there are many ways for it to go wrong. I don’t support poorly designed tax policies. I think the extreme levels of inequality predicted in this essay justify a more robust tax policy on basic moral grounds, but I can also make a pragmatic argument to the world’s billionaires that it’s in their interest to support a good version of it: if they don’t support a good version, they’ll inevitably get a bad version designed by a mob.Ultimately, I think of all of the above interventions as ways to buy time. In the end AI will be able to do everything, and we need to grapple with that. It’s my hope that by that time, we can use AI itself to help us restructure markets in ways that work for everyone, and that the interventions above can get us through the transitional period.Economic concentration of powerSeparate from the problem of job displacement or economic inequality per se is the problem of economic concentration of power. Section 1 discussed the risk that humanity gets disempowered by AI, and Section 3 discussed the risk that citizens get disempowered by their governments by force or coercion. But another kind of disempowerment can occur if there is such a huge concentration of wealth that a small group of people effectively controls government policy with their influence, and ordinary citizens have no influence because they lack economic leverage. Democracy is ultimately backstopped by the idea that the population as a whole is necessary for the operation of the economy. If that economic leverage goes away, then the implicit social contract of democracy may stop working. Others have written about this, so I needn’t go into great detail about it here, but I agree with the concern, and I worry it is already starting to happen.To be clear, I am not opposed to people making a lot of money. There’s a strong argument that it incentivizes economic growth under normal conditions. I am sympathetic to concerns about impeding innovation by killing the golden goose that generates it. But in a scenario where GDP growth is 10–20% a year and AI is rapidly taking over the economy, yet single individuals hold appreciable fractions of the GDP, innovation is not the thing to worry about. The thing to worry about is a level of wealth concentration that will break society.The most famous example of extreme concentration of wealth in US history is the Gilded Age, and the wealthiest industrialist of the Gilded Age was John D. Rockefeller. Rockefeller’s wealth amounted to ~2% of the US GDP at the time.42 A similar fraction today would lead to a fortune of $600B, and the richest person in the world today (Elon Musk) already exceeds that, at roughly $700B. So we are already at historically unprecedented levels of wealth concentration, even before most of the economic impact of AI. I don’t think it is too much of a stretch (if we get a “country of geniuses”) to imagine AI companies, semiconductor companies, and perhaps downstream application companies generating ~$3T in revenue per year,43 being valued at ~$30T, and leading to personal fortunes well into the trillions. In that world, the debates we have about tax policy today simply won’t apply as we will be in a fundamentally different situation.Related to this, the coupling of this economic concentration of wealth with the political system already concerns me. AI datacenters already represent a substantial fraction of US economic growth,44 and are thus strongly tying together the financial interests of large tech companies (which are increasingly focused on either AI or AI infrastructure) and the political interests of the government in a way that can produce perverse incentives. We already see this through the reluctance of tech companies to criticize the US government, and the government’s support for extreme anti-regulatory policies on AI.DefensesWhat can be done about this? First, and most obviously, companies should simply choose not to be part of it. Anthropic has always strived to be a policy actor and not a political one, and to maintain our authentic views whatever the administration. We’ve spoken up in favor of sensible AI regulation and export controls that are in the public interest, even when these are at odds with government policy.45 Many people have told me that we should stop doing this, that it could lead to unfavorable treatment, but in the year we’ve been doing it, Anthropic’s valuation has increased by over 6x, an almost unprecedented jump at our commercial scale.Second, the AI industry needs a healthier relationship with government—one based on substantive policy engagement rather than political alignment. Our choice to engage on policy substance rather than politics is sometimes read as a tactical error or failure to “read the room” rather than a principled decision, and that framing concerns me. In a healthy democracy, companies should be able to advocate for good policy for its own sake. Related to this, a public backlash against AI is brewing: this could be a corrective, but it’s currently unfocused. Much of it targets issues that aren’t actually problems (like datacenter water usage) and proposes solutions (like datacenter bans or poorly designed wealth taxes) that wouldn’t address the real concerns. The underlying issue that deserves attention is ensuring that AI development remains accountable to the public interest, not captured by any particular political or commercial alliance, and it seems important to focus the public discussion there.Third, the macroeconomic interventions I described earlier in this section, as well as a resurgence of private philanthropy, can help to balance the economic scales, addressing both the job displacement and concentration of economic power problems at once. We should look to the history of our country here: even in the Gilded Age, industrialists such as Rockefeller and Carnegie felt a strong obligation to society at large, a feeling that society had contributed enormously to their success and they needed to give back. That spirit seems to be increasingly missing today, and I think it is a large part of the way out of this economic dilemma. Those who are at the forefront of AI’s economic boom should be willing to give away both their wealth and their power.5. Black seas of infinityIndirect effectsThis last section is a catchall for unknown unknowns, particularly things that could go wrong as an indirect result of positive advances in AI and the resulting acceleration of science and technology in general. Suppose we address all the risks described so far, and begin to reap the benefits of AI. We will likely get a “century of scientific and economic progress compressed into a decade,” and this will be hugely positive for the world, but we will then have to contend with the problems that arise from this rapid rate of progress, and those problems may come at us fast. We may also encounter other risks that occur indirectly as a consequence of AI progress and are hard to anticipate in advance.By the nature of unknown unknowns it is impossible to make an exhaustive list, but I’ll list three possible concerns as illustrative examples for what we should be watching for: Rapid advances in biology. If we do get a century of medical progress in a few years, it is possible that we will greatly increase the human lifespan, and there is a chance we also gain radical capabilities like the ability to increase human intelligence or radically modify human biology. Those would be big changes in what is possible, happening very quickly. They could be positive if responsibly done (which is my hope, as described in Machines of Loving Grace), but there is always a risk they go very wrong—for example, if efforts to make humans smarter also make them more unstable or power-seeking. There is also the issue of “uploads” or “whole brain emulation,” digital human minds instantiated in software, which might someday help humanity transcend its physical limitations, but which also carry risks I find disquieting. AI changes human life in an unhealthy way. A world with billions of intelligences that are much smarter than humans at everything is going to be a very weird world to live in. Even if AI doesn’t actively aim to attack humans (Section 1), and isn’t explicitly used for oppression or control by states (Section 3), there is a lot that could go wrong short of this, via normal business incentives and nominally consensual transactions. We see early hints of this in the concerns about AI psychosis, AI driving people to suicide, and concerns about romantic relationships with AIs. As an example, could powerful AIs invent some new religion and convert millions of people to it? Could most people end up “addicted” in some way to AI interactions? Could people end up being “puppeted” by AI systems, where an AI essentially watches their every move and tells them exactly what to do and say at all times, leading to a “good” life but one that lacks freedom or any pride of accomplishment? It would not be hard to generate dozens of these scenarios if I sat down with the creator of Black Mirror and tried to brainstorm them. I think this points to the importance of things like improving Claude’s Constitution, over and above what is necessary for preventing the issues in Section 1. Making sure that AI models really have their users’ long-term interests at heart, in a way thoughtful people would endorse rather than in some subtly distorted way, seems critical. Human purpose. This is related to the previous point, but it’s not so much about specific human interactions with AI systems as it is about how human life changes in general in a world with powerful AI. Will humans be able to find purpose and meaning in such a world? I think this is a matter of attitude: as I said in Machines of Loving Grace, I think human purpose does not depend on being the best in the world at something, and humans can find purpose even over very long periods of time through stories and projects that they love. We simply need to break the link between the generation of economic value and self-worth and meaning. But that is a transition society has to make, and there is always the risk we don’t handle it well.My hope with all of these potential problems is that in a world with powerful AI that we trust not to kill us, that is not the tool of an oppressive government, and that is genuinely working on our behalf, we can use AI itself to anticipate and prevent these problems. But that is not guaranteed—like all of the other risks, it is something we have to handle with care.Humanity’s testReading this essay may give the impression that we are in a daunting situation. I certainly found it daunting to write, in contrast with Machines of Loving Grace, which felt like giving form and structure to surpassingly beautiful music that had been echoing in my head for years. And there is much about the situation that genuinely is hard. AI brings threats to humanity from multiple directions, and there is genuine tension between the different dangers, where mitigating some of them risks making others worse if we do not thread the needle extremely carefully.Taking time to carefully build AI systems so they do not autonomously threaten humanity is in genuine tension with the need for democratic nations to stay ahead of authoritarian nations and not be subjugated by them. But in turn, the same AI-enabled tools that are necessary to fight autocracies can, if taken too far, be turned inward to create tyranny in our own countries. AI-driven terrorism could kill millions through the misuse of biology, but an overreaction to this risk could lead us down the road to an autocratic surveillance state. The labor and economic concentration effects of AI, in addition to being grave problems in their own right, may force us to face the other problems in an environment of public anger and perhaps even civil unrest, rather than being able to call on the better angels of our nature. Above all, the sheer number of risks, including unknown ones, and the need to deal with all of them at once, creates an intimidating gauntlet that humanity must run.Furthermore, the last few years should make clear that the idea of stopping or even substantially slowing the technology is fundamentally untenable. The formula for building powerful AI systems is incredibly simple, so much so that it can almost be said to emerge spontaneously from the right combination of data and raw computation. Its creation was probably inevitable the instant humanity invented the transistor, or arguably even earlier when we first learned to control fire. If one company does not build it, others will do so nearly as fast. If all companies in democratic countries stopped or slowed development, by mutual agreement or regulatory decree, then authoritarian countries would simply keep going. Given the incredible economic and military value of the technology, together with the lack of any meaningful enforcement mechanism, I don’t see how we could possibly convince them to stop.I do see a path to a slight moderation in AI development that is compatible with a realist view of geopolitics. That path involves slowing down the march of autocracies towards powerful AI for a few years by denying them the resources they need to build it,46 namely chips and semiconductor manufacturing equipment. This in turn gives democratic countries a buffer that they can “spend” on building powerful AI more carefully, with more attention to its risks, while still proceeding fast enough to comfortably beat the autocracies. The race between AI companies within democracies can then be handled under the umbrella of a common legal framework, via a mixture of industry standards and regulation.Anthropic has advocated very hard for this path, by pushing for chip export controls and judicious regulation of AI, but even these seemingly common-sense proposals have largely been rejected by policymakers in the United States (which is the country where it’s most important to have them). There is so much money to be made with AI—literally trillions of dollars per year—that even the simplest measures are finding it difficult to overcome the political economy inherent in AI. This is the trap: AI is so powerful, such a glittering prize, that it is very difficult for human civilization to impose any restraints on it at all.I can imagine, as Sagan did in Contact, that this same story plays out on thousands of worlds. A species gains sentience, learns to use tools, begins the exponential ascent of technology, faces the crises of industrialization and nuclear weapons, and if it survives those, confronts the hardest and final challenge when it learns how to shape sand into machines that think. Whether we survive that test and go on to build the beautiful society described in Machines of Loving Grace, or succumb to slavery and destruction, will depend on our character and our determination as a species, our spirit and our soul.Despite the many obstacles, I believe humanity has the strength inside itself to pass this test. I am encouraged and inspired by the thousands of researchers who have devoted their careers to helping us understand and steer AI models, and to shaping the character and constitution of these models. I think there is now a good chance that those efforts bear fruit in time to matter. I am encouraged that at least some companies have stated they’ll pay meaningful commercial costs to block their models from contributing to the threat of bioterrorism. I am encouraged that a few brave people have resisted the prevailing political winds and passed legislation that puts the first early seeds of sensible guardrails on AI systems. I am encouraged that the public understands that AI carries risks and wants those risks addressed. I am encouraged by the indomitable spirit of freedom around the world and the determination to resist tyranny wherever it occurs.But we will need to step up our efforts if we want to succeed. The first step is for those closest to the technology to simply tell the truth about the situation humanity is in, which I have always tried to do; I’m doing so more explicitly and with greater urgency with this essay. The next step will be convincing the world’s thinkers, policymakers, companies, and citizens of the imminence and overriding importance of this issue—that it is worth expending thought and political capital on this in comparison to the thousands of other issues that dominate the news every day. Then there will be a time for courage, for enough people to buck the prevailing trends and stand on principle, even in the face of threats to their economic interests and personal safety. The years in front of us will be impossibly hard, asking more of us than we think we can give. But in my time as a researcher, leader, and citizen, I have seen enough courage and nobility to believe that we can win—that when put in the darkest circumstances, humanity has a way of gathering, seemingly at the last minute, the strength and wisdom needed to prevail. We have no time to lose.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:54:00.009132Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 164,
      "num_comments": 112,
      "engagement_score": 388.0
    },
    {
      "link": "https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html",
      "author": "vjeux",
      "title": "Vjeux \" Porting 100k lines from TypeScript to Rust using Claude Code in a month",
      "source": "hackernews",
      "content": "I read this post “Our strategy is to combine AI and Algorithms to rewrite Microsoft’s largest codebases [from C++ to Rust]. Our North Star is ‘1 engineer, 1 month, 1 million lines of code.” and it got me curious, how difficult is it really?\nI've long wanted to build a competitive Pokemon battle AI after watching a lot of WolfeyVGC and following the PokéAgent challenge at NeurIPS. Thankfully there's an open source project called \"Pokemon Showdown\" that implements all the rules but it's written in JavaScript which is quite slow to run in a training loop. So my holiday project came to life: let's convert it to Rust using Claude!\nEscaping the sandbox\nHaving the AI able to run arbitrary code on your machine is dangerous, so there's a lot of safeguards put in place. But... at the same time, this is what I want to do in this case. So let me walk through the ways I escaped the various sandboxes.\ngit push\nClaude runs in a sandbox that limits some operations like ssh access. You need ssh access in order to publish to GitHub. This is very important as I want to be able to check how the AI is doing from my phone while I do some other activities 😉\nWhat I realized is that I can run the code on my terminal but Claude cannot do it from its own terminal. So what I did was to ask Claude to write a nodejs script that opens an http server on a local port that executes the git commands from the url. Now I just need to keep a tab open on my terminal with this server active and ask Claude to write instructions in Claude.md for it to interact with it.\nrustc\nThere's an antivirus on my computer that requires a human interaction when an unknown binary is being ran. Since every time we compile it's a new unknown binary, this wasn't going to work.\nWhat I found is that I can setup a local docker instance and compile + run the code inside of docker which doesn't trigger the antivirus. Again, I asked Claude to generate the right instructions in Claude.md and problem solved.\nThe next hurdle was to figure out how to let Claude Code for hours without any human intervention.\n--yes\nClaude keeps asking for permission to do things. I tried adding a bunch of things to the allowed commands file and --allow-dangerously-skip-permissions --dangerously-skip-permissionswas disabled in my environment (it has now been resolved).\nI realized that I could run an AppleScript that presses enter every few seconds in another tab. This way it's going to say Yes to everything Claude asks to do. So far it hasn't decided to hack my computer...\n#!/bin/bash\nosascript -e \\\n'tell application \"System Events\"\nrepeat\ndelay 5\nkey code 36\nend repeat\nend tell'\nNever give up\nClaude after working for some time seem to always stop to recap things. I tried prompting it to never do, even threatening it to no avail.\nI tried using the Ralph Wiggum loop but it couldn't get it to work and apparently I'm not alone.\nWhat ended up working is to copy in my clipboard the task I wanted it to do and to tweak the script above to hit the keys \"cmd-v\" after pressing enter. This way in case it asks a question the \"enter\" is being used and in case it's not it's queuing the prompt for when Claude is giving back control.\nAuto-updates\nThere are programs on the computer like software updater that can steal the focus from the terminal window, for example showing a modal. Once that happens, then the cmd-v / enter are no longer sent to the terminal and the execution stops.\nI used my trusty Auto Clicker by MurGaa from Minecraft days to simulate a left click every few seconds. I place my terminal on the edge of the screen and same for my mouse so that when a modal appears in the middle, it refocuses the terminal correctly.\nIt also prevents the computer from going to sleep so that it can run even when I'm not using the laptop or at night.\nBugs 🐜\nReliability when running things for a long period of time is paramount. Overall it's been a pretty smooth ride but I ran into this specific error during a handful of nights which stopped the process. I hope they get to the bottom of it and solve it as I'm not the only one to report it!\nThis setup is far from optimal but has worked so far. Hopefully this gets streamlined in the future!\nPorting Pokemon\nOne Shot\nAt the very beginning, I started with a simple prompt asking Claude to port the codebase and make sure that things are done line by line. At first it felt extremely impressive, it generated thousands of lines of Rust that was compiling.\nSadly it was only an appearance as it took a lot of shortcuts. For example, it created two different structures for what a move is in two different files so that they would both compile independently but didn't work when integrated together. It ported all the functions very loosely where anything that was remotely complicated would not be ported but instead \"simplified\".\nI didn't realize it yet, I got the loop working to have it port more and more code. The issue is that it created wrong abstractions all over the place and kept adding hardcoded code to make whatever it was supposed to fix work. This wasn't going to go anywhere.\nGiving it structure\nAt this point I knew that I needed to be a lot more prescriptive for what I wanted out of it. Taking a step back, the end result should have every JavaScript file and every method inside to have a Rust equivalent.\nSo I asked Claude to write a script that takes all the files and methods in the JavaScript codebase and put comments in the rust codebase with the JavaScript source, next to the Rust methods.\nIt was really important for it to be a script as even when instructed to copy code over, it would mistranslate JavaScript code. Being deterministic here greatly increased the odds of getting the right results.\nLitte Islands\nThe next challenge is that the original files were thousands of lines long, double it with source comments we got to files more than 10k lines long. This causes a ton of issues with the context window where Claude straight up refuses to open the file. So it started reading the file in chunks but without a ton of precision. Also the context grew a lot quicker and compaction became way more frequent.\nSo I went ahead and split every method into its own file for the Rust version. This dramatically improved the results. For maximal efficiency I would need to do the same for the JavaScript codebase as well but I was too afraid to do it and accidentally change the behavior so decided not to.\nCleanup\nThe process of porting went through two repeating phases. I would give a large task to Claude to do in a loop that would churn on it for a day, and then I would need to spend time cleaning up the places where it went into the wrong direction.\nFor the cleanup, I still used Claude but gave a lot more specific recommendations. For example, I noticed that it would hardcode moves/abilities/items/... behaviors everywhere in the code when left unchecked, even after explicitly telling it not to. So I would manually look for all these and tell it to move them into the right places.\nThis is where engineering skills come into play, all my experience building software let me figure out what went wrong and how to fix it. The good part is that I didn't have to do the cleanup myself, Claude was able to do it just fine when directed to.\nIntegration\nBuild everything before testing\nSo far, I just made sure that the code compiled, but have never actually put all the pieces together to ensure it actually worked. What Claude really wanted was to do a traditional software building strategy where you make \"simple\" implementations of all of the pieces and then build them up as time goes.\nBut in our case, all this iteration has already happened for 10 years on the pokemon-showdown codebase. It's counter productive to try and re-learn all these lessons and will unlikely converge the same way. What works better is to port everything at once, and then do the integration at the end once.\nI've learned this strategy from working on Skip, a compiler. For years all the building blocks were built independently and then it all came together with nothing to show for but within a month at the end it all worked. I was so shocked.\nEnd-to-end test\nOnce most of the codebase was ported one to one, I started putting it all together. The good thing is that we can run and edit the code in JavaScript and in Rust, and the input/output is very simple and standardized: list of pokemons with their options (moves, items, nature, iv/ev spread...) and then the list of actions at each step (moves and switches). Given the same random sequence, it'll advance the state the same way.\nNow I can let Claude generate this testing harness and go through all the issues one by one. Impressively, it was able to figure out all issues and fix them.\nOver the course of 3 weeks it averaged fixing one issue every 20 minutes or so. It fixed hundreds of issues on its own. I never intervened, it was only a matter of time before it fixed every issue that it encountered.\nGiving it structure\nAt the beginning, this process was extremely slow. Every time a compaction happened, Claude became \"dumb\" again and reinvented the wheel, writing down tons of markdown files and test scripts along the way. Or Claude decided to take the easy way out and just generate tons of tests but never actually making them match with JavaScript.\nSo, I started looking at what it did well and encoding it. For example, it added a lot of debugging around the PRNG steps and what actions happened at every turn with all the debugging metadata. So I asked it to create a single test script to print down this information for a single step and to print stack traces. Then add instruction to the Claude.md file. This way every investigation started right away.\nThe long slog\nI built used the existing random number generator to generate battles and could put in a number as a seed. This let me generate consistent battles at an increasing size.\nI started fixing the first 100 battles, then 1000, 10k, 100k and I'm almost done solving all the issues for the first 2.4 million battles! I'm not sure how many more issues there are but the good thing is that they are getting smaller and smaller as the batch size increases.\nTypes of issues\nThere are two broad classes of issues that were fixed. The first one that I expected is that Rust has different constraints than JavaScript which need to be taken into account and lead to bugs:\nRust has the \"borrow checker\" where a mutable variable cannot be passed in two different contexts at once. The problem is that \"Pokemon\" and \"Battle\" have references to each others. So there's a lot of workarounds like doing copies, passing indices instead of the object, providing functions with mutable object as callback...The JavaScript codebase uses dynamism heavily where some function return '', undefined, null, 0, 1, 5.2, Pokemon... which all are handled with different behaviors. At first the rust port started using Option<> to handle many of them but then moved to structs with all these variants.Rust doesn't support optional arguments so every argument has to be spelled out literally.\nBut the second one are due to itself... Claude Code is like a smart student that is trying to find every opportunity to avoid doing the hard work and take the easy way out if it thinks it can get away with it.\nIf a fix requires changing more than one or two files, this is a \"significant infrastructure\" and Claude Code will refuse to do it unless explicitly prompted and will put in whatever hacks it can to make the specific test work.Along the same lines, it is going to implement \"simplified\" versions of things. For some methods, it was better to delete everything and asking it to port it over from scratch than trying to fix all the existing code it created.The JavaScript comments are supposed to be the source of truth. But Claude is not above changing the original code if it feels like this is the way to solve the problem...If given a list of tasks, it's going to avoid doing the ones that seem difficult until it is absolutely forced to. This is inefficient if not careful as it's going to keep spending time investigating and then skipping all the \"hard\" ones. Compaction is basically wiping all its memory.\nPrompts\nI didn't write a single line of code myself in this project. I alternated between \"co-op\" where I work with Claude interactively during the day and creating a job for it to run overnight. I'll focus on the night ones for this section.\nConversion\nFor the first phase of the project, I mostly used variations of this one. Asking it to go through all the big files one by one and implement them faithfully (it didn't really follow instructions as we've seen later...)\nOpen BATTLE_TODO.md to get the list of all the methods in both battle*.rs.Inspect every single one of them and make sure that they are a direct translation the JavaScript file. If there's a method with the same name, the JavaScript definition will be in the comment.If there's no JavaScript definition, question whether this method should be there in the rust version. Our goal is to follow as closely as possible the JavaScript version to avoid any bugs in translation. If you notice that the implementation doesn't match, do all the refactoring needed to match 1 to 1.This will be a complex project. You need to go through all the methods one by one, IN ORDER. YOU CANNOT skip a method because it is too hard or would requiring building new infrastructure. We will call this in a loop so spend as much time as you need building the proper infrastructure to make it 1 to 1 with the JavaScript equivalent. Do not give up.Update BATTLE_TODO.md and do a git commit after each unit of work.\nTodos\nClaude Code while porting the methods one by one often decided to write a \"simplified\" version or add a \"TODO\" for later. I also found it to be useful when generating work to add the instructions in the codebase itself via a TODO comment, so I don't need to wish that it's going to be read from the context.\nThe master md file in practice didn't really work, it quickly became too big to be useful and Claude started creating a bunch more littering the repo with them. Instead I gave it a deterministic way to go through then by calling grep on the codebase, so it knew when to find them.\nWe want to fix every TODO in the codebase. TODO or simplif in pokemon-showdown-rs/.There are hundreds of them, so go diligently one by one. Do not skip them even if they are difficult. I will call this prompt again and again so you don't need to worry about taking too long on any single one.The port must be exactly one to one. If the infrastructure doesn't exist, please implement it. Do not invent anything.Make sure it still compiles after each addition and commit and push to git.\nAt some point the context was poisoned where a TODO was inside of the original js codebase so it changed it to something else which made sense. But then it did the same for all the subsequent TODOs which didn't... Thankfully I could just revert all these commits.\nFixing\nI put in all the instructions to debug in Claude.md and a script to run all the tests which outputs a txt file with progress report. This way Claude was able to just keep going fixing issues after issues.\nWe want to fix all the divergences in battles. Please look at 500-seeds-results.txt and fix them one by one. The only way you can fix is by making sure that the differences between javascript and rust are explained by language differences and not logic. Every line between the two must match one by one. If you fixed something specific, it's probably a larger issue, spend the time to figure out if other similar things are broken and do the work to do the larger infrastructure fixes. Make sure it still compiles after each addition and commit and push to git. Check if there are other parts of the codebase that make this mistake.\nThis is really useful to have this txt file diff committed to GitHub to get a sense of progress on the go!\nEpilogue\nIt works 🤯\nI didn't quite know what to expect coming into this project. They usually tend to die due to the sheer amount of work needed to get anywhere close to something complete. But not this time!\nWe have a complete implementation of Pokemon battle system that produces the same results as the existing JavaScript codebase*. This was done through 5000 commits in 4 weeks and the Rust codebase is around 100k lines of code.\n*I wish we had 0 divergences but right now there are 80 out of the first 2.4 million seeds or 0.003%. I need to run it for longer to solve these.\nIs it fast?\nThe whole point of the project was for it to be faster than the initial JavaScript implementation. Only towards the end of the project where we had a sizable amount of battles running perfectly I felt like it would be a fair time to do a performance comparison.\nI asked Claude Code to parallelize both implementations and was relieved by the results, the Rust port is actually significantly faster, I didn't spend all this time for nothing!\nI've tried asking Claude to optimize it further, it created a plan that looks reasonable (I've never interacted with Rust in my life) and it spent a day building many of these optimizations but at the end of the day, none of them actually improved the runtime and some even made it way worse.\nThis is a good example of how experience and expertise is still very required in order to get the best out of LLMs.\nConclusion\nThis is pretty wild that I was able to port a ~100k lines codebase from JavaScript to Rust in two weeks on my own with Claude Code running 24 hours a day for a month creating 5k commits! I have never written any line of Rust before in my life.\nLLM-based coding agents are such a great new tool for engineers, there's no way I would have been able to do that without Claude Code. That said, it still feels like a tool that requires my engineering expertise and constant babysitting to produce these results.\nSadly I didn't get to build the Pokemon Battle AI and the winter break is over, so if anybody wants to do it, please have fun with the codebase!",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:54:06.335378Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 178,
      "num_comments": 122,
      "engagement_score": 422.0
    },
    {
      "link": "https://github.com/kxzk/snapbench",
      "author": "kxzk",
      "title": "GitHub - kxzk/snapbench: 📸 gotta find 'em all; spatial reasoning benchmark for LLMs",
      "source": "hackernews",
      "content": "SnapBench\nInspired by Pokémon Snap (1999). VLM pilots a drone through 3D world to locate and identify creatures.\nArchitecture\n%%{init: {'theme': 'base', 'themeVariables': { 'background': '#ffffff', 'primaryColor': '#ffffff'}}}%%\nflowchart LR\nsubgraph Controller[\"**Controller** (Rust)\"]\nC[Orchestration]\nend\nsubgraph VLM[\"**VLM** (OpenRouter)\"]\nV[Vision-Language Model]\nend\nsubgraph Simulation[\"**Simulation** (Zig/raylib)\"]\nS[Game State]\nend\nC -->|\"screenshot + prompt\"| V\nC <-->|\"cmds + state<br>**UDP:9999**\"| S\nstyle Controller fill:#8B5A2B,stroke:#5C3A1A,color:#fff\nstyle VLM fill:#87CEEB,stroke:#5BA3C6,color:#1a1a1a\nstyle Simulation fill:#4A7C23,stroke:#2D5A10,color:#fff\nstyle C fill:#B8864A,stroke:#8B5A2B,color:#fff\nstyle V fill:#B5E0F7,stroke:#87CEEB,color:#1a1a1a\nstyle S fill:#6BA33A,stroke:#4A7C23,color:#fff\nLoading\nOverview\nThe simulation generates procedural terrain and spawns creatures (cat, dog, pig, sheep) for the drone to discover. It handles drone physics and collision detection, accepting 8 movement commands plus identify and screenshot. The Rust controller captures frames from the simulation, constructs prompts enriched with position and state data, then parses VLM responses into executable command sequences. The objective: locate and successfully identify 3 creatures, where identify succeeds when the drone is within 5 units of a target.\ndemo_3x.mov\nGotta catch 'em all?\nI gave 7 frontier LLMs a simple task: pilot a drone through a 3D voxel world and find 3 creatures.\nOnly one could do it.\nIs this a rigorous benchmark? No. However, it's a reasonably fair comparison - same prompt, same seeds, same iteration limits. I'm sure with enough refinement you could coax better results out of each model. But that's kind of the point: out of the box, with zero hand-holding, only one model figured out how to actually fly.\nWhy can't Claude look down?\nThe core differentiator wasn't intelligence - it was altitude control. Creatures sit on the ground. To identify them, you need to descend.\nGemini Flash: Actively adjusts altitude, descends to creature level, identifies\nGPT-5.2-chat: Gets close horizontally but never lowers\nClaude Opus: Attempts identification 160+ times, never succeeds - approaching at wrong angles\nOthers: Wander randomly or get stuck\nThis left me puzzled. Claude Opus is arguably the most capable model in the lineup. It knows it needs to identify creatures. It tries - aggressively. But it never adjusts its approach angle.\nThe two-creature anomaly\nRun 13 (seed 72) was the only run where any model found 2 creatures. Why? They happened to spawn near each other. Gemini Flash found one, turned around, and spotted the second.\nIn most other runs, Flash found one creature quickly but ran out of iterations searching for the others. The world is big. 50 iterations isn't a lot of time.\nBigger ≠ better\nThis was the most surprising finding. I expected:\nClaude Opus 4.5 (most expensive) to dominate\nGemini 3 Pro to outperform Gemini 3 Flash (same family, more capability)\nInstead, the cheapest model beat models costing 10x more.\nWhat's going on here? A few theories:\nSpatial reasoning doesn't scale with model size - at least not yet\nFlash was trained differently - maybe more robotics data, more embodied scenarios?\nSmaller models follow instructions more literally - \"go down\" means go down, not \"consider the optimal trajectory\"\nI genuinely don't know. But if you're building an LLM-powered agent that needs to navigate physical or virtual space, the most expensive model might not be your best choice.\nColor theory, maybe\nAnecdotally, creatures with higher contrast (gray sheep, pink pigs) seemed easier to spot than brown-ish creatures that blended into the terrain. A future version might normalize creature visibility. Or maybe that's the point - real-world object detection isn't normalized either.\nPrior work\nBefore this, I tried having LLMs pilot a real DJI Tello drone.\nResults: it flew straight up, hit the ceiling, and did donuts until I caught it. (I was using Haiku 4.5, which in hindsight explains a lot.)\nThe Tello is now broken. I've ordered a BetaFPV and might get another Tello since they're so easy to program. Now that I know Gemini Flash can actually navigate, a real-world follow-up might be worth revisiting.\nRough edges\nThis is half-serious research, half \"let's see what happens.\"\nThe simulation has rough edges (it's a side project, not a polished benchmark suite)\nOne blanket prompt is used for all models - model-specific tuning would likely improve results\nThe feedback loop is basic (position, screenshot, recent commands) - there's room to get creative with what information gets passed back\nIteration limits (50) may artificially cap models that are slower but would eventually succeed\nTry it yourself\nPrerequisites\nTool\nVersion\nInstall\nZig\n≥0.15.2\nziglang.org/download\nRust\nstable (2024 edition)\nrust-lang.org/tools/install\nPython\n≥3.11\npython.org\nuv\nlatest\ndocs.astral.sh/uv\nYou'll also need an OpenRouter API key.\nSetup\ngh repo clone kxzk/snapbench\ncd snapbench\n# set your API key\nexport OPENROUTER_API_KEY=\"sk-or-...\"\nRunning the simulation manually\n# terminal 1: start the simulation (with optional seed)\nzig build run -Doptimize=ReleaseFast -- 42\n# or\nmake sim\n# terminal 2: start the drone controller\ncargo run --release --manifest-path llm_drone/Cargo.toml -- --model google/gemini-3-flash-preview\n# or\nmake drone\nRunning the benchmark suite\n# runs all models defined in bench/models.toml\nuv run bench/bench_runner.py\n# or\nmake bench\nResults get saved to data/run_<id>.csv.\nWhere this could go\nModel-specific prompts: Tune instructions to each model's strengths\nRicher feedback: Pass more spatial context (distance readings, compass, minimap?)\nMulti-agent runs: What if you gave each model a drone and made them compete?\nExtended iterations: Let slow models run longer to isolate reasoning from speed\nReal drone benchmark: Gemini Flash vs. the BetaFPV\nPokémon assets: Found low-poly Pokémon models on Poly Pizza—leaning into the Pokémon Snap inspiration\nWorld improvements: Larger terrain, better visuals, performance optimizations\nAttribution\nDrone by NateGazzard CC-BY via Poly Pizza\nCube World Kit by Quaternius via Poly Pizza\nDonated to Poly Pizza to support the platform.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:54:33.275175Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 152,
      "num_comments": 83,
      "engagement_score": 318.0
    },
    {
      "link": "https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study",
      "author": "https://www.theguardian.com/profile/andrew-gregory",
      "title": "Google AI Overviews cite YouTube more than any medical site for health queries, study suggests | Google | The Guardian",
      "source": "hackernews",
      "content": "Google’s search feature AI Overviews cites YouTube more than any medical website when answering queries about health conditions, according to research that raises fresh questions about a tool seen by 2 billion people each month.The company has said its AI summaries, which appear at the top of search results and use generative AI to answer questions from users, are “reliable” and cite reputable medical sources such as the Centers for Disease Control and Prevention and the Mayo Clinic.However, a study that analysed responses to more than 50,000 health queries, captured using Google searches from Berlin, found the top cited source was YouTube. The video-sharing platform is the world’s second most visited website, after Google itself, and is owned by Google.Researchers at SE Ranking, a search engine optimisation platform, found YouTube made up 4.43% of all AI Overview citations. No hospital network, government health portal, medical association or academic institution came close to that number, they said.“This matters because YouTube is not a medical publisher,” the researchers wrote. “It is a general-purpose video platform. Anyone can upload content there (eg board-certified physicians, hospital channels, but also wellness influencers, life coaches, and creators with no medical training at all).”Google told the Guardian that AI Overviews was designed to surface high-quality content from reputable sources, regardless of format, and a variety of credible health authorities and licensed medical professionals created content on YouTube. The study’s findings could not be extrapolated to other regions as it was conducted using German-language queries in Germany, it said.The research comes after a Guardian investigation found people were being put at risk of harm by false and misleading health information in Google AI Overviews responses.In one case that experts said was “dangerous” and “alarming”, Google provided bogus information about crucial liver function tests that could have left people with serious liver disease wrongly thinking they were healthy. The company later removed AI Overviews for some but not all medical searches.The SE Ranking study analysed 50,807 healthcare-related prompts and keywords to see which sources AI Overviews relied on when generating answers.They chose Germany because its healthcare system is strictly regulated by a mix of German and EU directives, standards and safety regulations. “If AI systems rely heavily on non-medical or non-authoritative sources even in such an environment, it suggests the issue may extend beyond any single country,” they wrote.AI Overviews surfaced on more than 82% of health searches, the researchers said. When they looked at which sources AI Overviews relied on most often for health-related answers, one result stood out immediately, they said. The single most cited domain was YouTube with 20,621 citations out of a total of 465,823.Researchers at SE Ranking found YouTube made up 4.43% of all AI Overview citations.\nPhotograph: Adam Vaughan/EPAThe next most cited source was NDR.de, with 14,158 citations (3.04%). The German public broadcaster produces health-related content alongside news, documentaries and entertainment. In third place was a medical reference site, Msdmanuals.com with 9,711 citations (2.08%).The fourth most cited source was Germany’s largest consumer health portal, Netdoktor.de, with 7,519 citations (1.61%). The fifth most cited source was a career platform for doctors, Praktischarzt.de, with 7,145 citations (1.53%).The researchers acknowledged limitations to their study. It was conducted as a one-time snapshot in December 2025, using German-language queries that reflected how users in Germany typically search for health information.Results could vary over time, by region, and by the phrasing of questions. However, even with those caveats, the findings still prompted alarm.Hannah van Kolfschooten, a researcher specialising in AI, health and law at the University of Basel who was not involved with the research, said: “This study provides empirical evidence that the risks posed by AI Overviews for health are structural, not anecdotal. It becomes difficult for Google to argue that misleading or harmful health outputs are rare cases.“Instead, the findings show that these risks are embedded in the way AI Overviews are designed. In particular, the heavy reliance on YouTube rather than on public health authorities or medical institutions suggests that visibility and popularity, rather than medical reliability, is the central driver for health knowledge.”A Google spokesperson said: “The implication that AI Overviews provide unreliable information is refuted by the report’s own data, which shows that the most cited domains in AI Overviews are reputable websites. And from what we’ve seen in the published findings, AI Overviews cite expert YouTube content from hospitals and clinics.”Google said the study showed that of the 25 most cited YouTube videos, 96% were from medical channels. However, the researchers cautioned that these videos represented fewer than 1% of all the YouTube links cited by AI Overviews on health.“Most of them (24 out of 25) come from medical-related channels like hospitals, clinics and health organisations,” the researchers wrote. “On top of that, 21 of the 25 videos clearly note that the content was created by a licensed or trusted source.“So at first glance it looks pretty reassuring. But it’s important to remember that these 25 videos are just a tiny slice (less than 1% of all YouTube links AI Overviews actually cite). With the rest of the videos, the situation could be very different.”Quick GuideContact us about this storyShowThe best public interest journalism relies on first-hand accounts from people in the know.If you have something to share on this subject, you can contact us confidentially using the following methods.Secure Messaging in the Guardian appThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.If you don't already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’. SecureDrop, instant messengers, email, telephone and postIf you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our SecureDrop platform.Finally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each. Illustration: Guardian Design / Rich Cousins",
      "publish_datetime": "2026-01-24T17:00:44.000Z",
      "scraping_timestamp": "2026-01-27T05:54:36.554537Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 377,
      "num_comments": 199,
      "engagement_score": 775.0
    },
    {
      "link": "https://stratechery.com/2026/tsmc-risk/",
      "author": null,
      "title": "TSMC Risk - Stratechery by Ben Thompson",
      "source": "hackernews",
      "content": "You probably think, given this title, you know what this Article is about. The most advanced semiconductors are made by TSMC in Taiwan,1 and Taiwan is claimed by China, which has not and will not take reunification-by-force off of the table.\nRelatedly, AI obviously has significant national security implications; at Davos, Anthropic CEO Dario Amodei reiterated his objection to the U.S. allowing the sale of Nvidia chips to China. From Bloomberg:\nAnthropic Chief Executive Officer Dario Amodei said selling advanced artificial intelligence chips to China is a blunder with “incredible national security implications” as the US moves to allow Nvidia Corp. to sell its H200 processors to Beijing. “It would be a big mistake to ship these chips,” Amodei said in an interview with Bloomberg Editor-in-Chief John Micklethwait at the World Economic Forum in Davos, Switzerland. “I think this is crazy. It’s a bit like selling nuclear weapons to North Korea.”\nThe nuclear weapon analogy is an interesting one: a lot of game theory was developed to manage the risk of nuclear weapons, particularly once the U.S.S.R. gained/stole nuclear capability, ending the U.S.’s brief monopoly on the technology. Before that happened, however, the U.S. had a dominant military position, given we had nuclear weapons and no one else did. Perhaps Amodei believes the U.S. should have advanced AI and China should not, giving us a dominant military position?\nThe problem with that reality, however, is Taiwan, as I explained in AI Promise and Chip Precariousness. AI, in contrast to nuclear weapons, has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion; if we got to a situation where only the U.S. had the sort of AI that would give us an unassailable advantage militarily, then the optimal strategy for China would change to taking TSMC off of the board.\nGiven this dependency, my recommendations in the Article run counter to Amodei: I want China dependent on not just U.S. chips but also on TSMC directly, which is why I argued in favor of selling Nvidia chips to China, and further believe that Huawei and other Chinese companies ought to be able to source from TSMC (on the flip side, I would ban the sale of semiconductor manufacturing equipment to Chinese fabs). I think it’s a good thing the Trump administration moved on the first point, at least.\nHowever, this risk is not what this Article is about: there is another TSMC risk facing the entire AI industry in particular; moreover, it’s a risk the downside of which is already being realized.\nThe TSMC Brake\nThere was one refrain that was common across Big Tech earnings last quarter: demand for AI exceeds supply. Here was Amazon CEO Andy Jassy on the company’s earnings call:\nYou’re going to see us continue to be very aggressive investing in capacity because we see the demand. As fast as we’re adding capacity right now, we’re monetizing it.\nHere was Microsoft CFO Amy Hood on the company’s earnings call:\nAzure AI services revenue was generally in line with expectations, and this quarter, demand again exceeded supply across workloads, even as we brought more capacity online.\nHere was Google CFO Anat Ashkenazi on the company’s earnings call:\nIn GCP, we see strong demand for enterprise AI infrastructure, including TPUs and GPUs, enterprise AI solutions driven by demand for Gemini 2.5 and our other AI models, and core GCP infrastructure and other services such as cybersecurity and data analytics. As I’ve mentioned on previous earnings calls, while we have been working hard to increase capacity and have improved the pace of server deployments and data center construction, we still expect to remain in a tight demand-supply environment in Q4 and 2026.\nHere was Meta CEO Mark Zuckerberg on the company’s earnings call:\nTo date, we keep on seeing this pattern where we build some amount of infrastructure to what we think is an aggressive assumption. And then we keep on having more demand to be able to use more compute, especially in the core business in ways that we think would be quite profitable than we end up having compute for.\nEarlier this month, TSMC CEO C.C. Wei admitted that the shortage was a lack of chips, not power; from the company’s earnings call:\nTalking about to build a lot of AI data center all over the world, I use one of my customers’ customers’ answer. I asked the same question. They told me that they planned this one, 5-6 years ago already. So, as I said, those cloud service providers are smart, very smart. So, they say that they work on the power supply 5-6 years ago. So, today, their message to me is: silicon from TSMC is a bottleneck, and asked me not to pay attention to all others, because they have to solve the silicon bottleneck first. But indeed, we do get the power supply, all over the world, especially in the US. Not only that, but we also look at, who support those kind of a power supply, like a turbine, like, what, nuclear power plant, the plan or those kinds of things. We also look at the supply of the rack. We also look at the supply of the cooling system. Everything, so far, so good. So we have to work hard to narrow the gap between the demand and supply from TSMC.\nThe cause of that gap is obvious if you look at TSMC’s financials, specifically the company’s annual capital expenditures:\nAfter a big increase in CapEx in 2021, driven by the COVID shortages and a belief in 5G, TSMC’s annual CapEx in the following years was basically flat — it actually declined on a year-over-year basis in both 2023 and 2024. Note those dates! ChatGPT was released in November 2022; that kicked off a massive increase in CapEx amongst the hyperscalers in particular, but it sure seems like TSMC didn’t buy the hype.\nThat lack of increased investment earlier this decade is why there is a shortage today, and is why TSMC has been a de facto brake on the AI buildout/bubble; I wrote last quarter:\nTo put it another way, if Altman and OpenAI are the ones pushing to accelerate the AI infrastructure buildout, it’s Wei and TSMC that are the brakes. The extent to which all of Altman’s deals actually materialize is dependent on how much TSMC invests in capacity now, and while they haven’t shown their hand yet, the company is saying all of the right things about AI being a huge trend without having yet committed to a commensurate level of investment, at least relative to OpenAI’s goals.\nThat Update was about the future, but it’s important to note that the TSMC brake has — if all of those CEO and CFO comments above are to be believed — already cost the biggest tech companies a lot of money. That’s the implication of not having enough supply to satisfy demand: there was revenue to be made that wasn’t, because TSMC didn’t buy the AI hype at the same time everyone else did.\nTSMC’s CapEx Plans\nTSMC is, finally, starting to invest more. Last year’s CapEx increased 37% to $41 billion, and there’s another increase in store for this year to $52–$56 billion; if we take the midpoint, that represents an increase of 32%, a bit less than last year:\nMake no mistake, $54 billion is a big number, one that Wei admitted made him nervous:\nYou essentially try to ask whether the AI demand is real or not. I’m also very nervous about it. Yeah, you bet, because we have to invest about USD52 billion to USD56 billion for the CapEx, right? If we did not do it carefully, that will be a big disaster to TSMC for sure. So, of course, I spent a lot of time in the last three-four months talking to my customers and then customers’ customers. I want to make sure that my customers’ demands are real.\nWei made clear that he was worried about the market several years down the line:\nIf you build a new fab, it takes two and three year, two to three years to build a new fab. So even we start to spend $52 billion to $56 billion, the contribution to this year is almost none, and 2027, a little bit. So we actually, we are looking for 2028-2029 supply, and we hope it’s a time that the gap will be narrow…So 2026-2027 for the short-term, we are looking to improve our productivity. 2028 to 2029, yes, we start to increase our capacity significantly. And it will continue this way if the AI demand megatrend as we expected.\nFirst off, this delayed impact explains why TSMC’s lack of CapEx increase a few years ago is resulting in supply-demand imbalance today. Secondly, notice how this year’s planned increase — which again, won’t really have an impact until 2028 — pales in comparison to the CapEx growth of the hyperscalers (2025 numbers are estimates; note that Amazon’s CapEx includes Amazon.com):\nRemember, a significant portion of this CapEx growth is for chips that are supported by TSMC’s stagnant CapEx growth from a few years ago. It’s notable, then, that TSMC’s current and projected CapEx growth is still less than the hyperscalers: how much less is it going to be than the hyperscalers’ growth in 2028, when the fabs being built today start actually producing chips?\nIn short, the TSMC brake isn’t going anywhere — if anything, it’s being pressed harder than ever.\nTSMC Risk\nTSMC is, to be clear, being extremely rational. CapEx is inherently risky: you are spending money now in anticipation of demand that may or may not materialize. Moreover, the risk for a foundry is higher than basically any other business model: nearly all of a foundry’s costs are CapEx, which means that if demand fails to materialize, costs — in the form of depreciation — don’t go down as they might with a business model with a higher percentage of marginal costs. This is exacerbated by the huge dollar figures entailed in building fabs: $52–$56 billion may drive revenues with big margins, but those big margins can easily flip to being huge losses and years of diminished pricing power thanks to excess capacity. Therefore, it’s understandable that TSMC is trying to manage its risks. Sure, the company may be foregoing some upside in 2028, but what is top of Wei’s mind is avoiding “a big disaster.”\nWhat is important to note, however, is that the risk TSMC is managing doesn’t simply go away: rather, it’s being offloaded to the hyperscalers in particular. Specifically, if we get to 2028, and TSMC still isn’t producing enough chips to satisfy demand, then that means the hyperscalers will be forgoing billions of dollars in revenue — even more than they are already forgoing today. Yes, that risk is harder to see than the risk TSMC is avoiding, because the hyperscalers aren’t going to be bankrupt for a lack of chips to satisfy demand. Still, the potential money not made — particularly when the number is potentially in the hundreds of billions of dollars — is very much a risk that the hyperscalers are incurring because of TSMC’s conservatism.\nWhat the hyperscalers need to understand is that simply begging TSMC to make more isn’t going to fix this problem, because begging TSMC to make more is to basically ask TSMC to take back the risk TSMC is offloading to the hyperscalers — they already declined! Rather, the only thing that will truly motivate TSMC to take on more risk is competition. If TSMC were worried about not just forgoing its own extra revenue, but actually losing business to a competitor, then the company would invest more. Moreover, that extra investment would be stacked on top of the investment made by said competitor, which means the world would suddenly have dramatically more fab capacity.\nIf You Want a Bubble\nIn short, the only way to truly get an AI bubble, with all of the potential benefits that entails, or, in the optimistic case, to actually meet demand in 2028 and beyond, is to have competition in the foundry space. That, by extension, means Samsung or Intel — or both — actually being viable options.\nRemember, however, the number one challenge facing those foundries: a lack of demand from the exact companies whom TSMC has deputized to take on their risk. I wrote in U.S. Intel:\nOur mythical startup, however, doesn’t exist in a vacuum: it exists in the same world as TSMC, the company who has defined the modern pure play foundry. TSMC has put in the years, and they’ve put in the money; TSMC has the unparalleled customer service approach that created the entire fabless chip industry; and, critically, TSMC, just as they did in the mobile era, is aggressively investing to meet the AI moment. If you’re an Nvidia, or an Apple in smartphones, or an AMD or a Qualcomm, why would you take the chance of fabricating your chips anywhere else? Sure, TSMC is raising prices in the face of massive demand, but the overall cost of a chip in a system is still quite small; is it worth risking your entire business to save a few dollars for worse performance with a worse customer experience that costs you time to market and potentially catastrophic product failures?\nWe know our mythical startup would face these challenges because they are the exact challenges Intel faces. Intel may need “a meaningful external customer to drive acceptable returns on [its] deployed capital”, but Intel’s needs do not drive the decision-making of those external customers, despite the fact that Intel, while not fully caught up to TSMC, is at least in the ballpark, something no startup could hope to achieve for decades.\nBecoming a meaningful customer of Samsung or Intel is very risky: it takes years to get a chip working on a new process, which hardly seems worth it if that process might not be as good, and if the company offering the process definitely isn’t as customer service-centric as TSMC. I understand why everyone sticks with TSMC.\nThe reality that hyperscalers and fabless chip companies need to wake up to, however, is that avoiding the risk of working with someone other than TSMC incurs new risks that are both harder to see and also much more substantial. Except again, we can see the harms already: foregone revenue today as demand outstrips supply. Today’s shortages, however, may prove to be peanuts: if AI has the potential these companies claim it does, future foregone revenue at the end of the decade is going to cost exponentially more — surely a lot more than whatever expense is necessary to make Samsung and/or Intel into viable competitors for TSMC.\nThis, incidentally, is how the geographic risk issue will be fixed, if it ever is. It’s hard to get companies to pay for insurance for geopolitical risks that may never materialize. What is much more likely is that TSMC’s customers realize that their biggest risk isn’t that TSMC gets blown up by China, but that TSMC’s monopoly and reasonable reluctance to risk a rate of investment that matches the rest of the industry means that the rest of the industry fails to fully capture the value of AI.",
      "publish_datetime": "2026-01-26T11:00:00+00:00",
      "scraping_timestamp": "2026-01-27T05:54:56.571050Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 138,
      "num_comments": 114,
      "engagement_score": 366.0
    },
    {
      "link": "https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/",
      "author": "Simon Willison",
      "title": "the browser is the sandbox",
      "source": "hackernews",
      "content": "the browser is the sandbox. Paul Kinlan is a web platform developer advocate at Google and recently turned his attention to coding agents. He quickly identified the importance of a robust sandbox for agents to operate in and put together these detailed notes on how the web browser can help:\nThis got me thinking about the browser. Over the last 30 years, we have built a sandbox specifically designed to run incredibly hostile, untrusted code from anywhere on the web, the instant a user taps a URL. [...]\nCould you build something like Cowork in the browser? Maybe. To find out, I built a demo called Co-do that tests this hypothesis. In this post I want to discuss the research I've done to see how far we can get, and determine if the browser's ability to run untrusted code is useful (and good enough) for enabling software to do more for us directly on our computer.\nPaul then describes how the three key aspects of a sandbox - filesystem, network access and safe code execution - can be handled by browser technologies: the File System Access API (still Chrome-only as far as I can tell), CSP headers with <iframe sandbox> and WebAssembly in Web Workers.\nCo-do is a very interesting demo that illustrates all of these ideas in a single application:\nYou select a folder full of files and configure an LLM provider and set an API key, Co-do then uses CSP-approved API calls to interact with that provider and provides a chat interface with tools for interacting with those files. It does indeed feel similar to Claude Cowork but without running a multi-GB local container to provide the sandbox.\nMy biggest complaint about <iframe sandbox> remains how thinly documented it is, especially across different browsers. Paul's post has all sorts of useful details on that which I've not encountered elsewhere, including a complex double-iframe technique to help apply network rules to the inner of the two frames.\nThanks to this post I also learned about the <input type=\"file\" webkitdirectory> tag which turns out to work on Firefox, Safari and Chrome and allows a browser read-only access to a full directory of files at once. I had Claude knock up a webkitdirectory demo to try it out and I'll certainly be using it for projects in the future.",
      "publish_datetime": "1769385092",
      "scraping_timestamp": "2026-01-27T05:55:39.993604Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 334,
      "num_comments": 175,
      "engagement_score": 684.0
    },
    {
      "link": "https://workdaycase.com",
      "author": null,
      "title": "Workday Case",
      "source": "hackernews",
      "content": "NOTICE OF COLLECTIVE ACTION LAWSUIT AGAINST WORKDAY, INC.TO: All individuals 40 years of age or older who, at any time from September 24, 2020 to the present, applied for job opportunities using Workday, Inc.’s job application platform.Re: Mobley v. Workday, Inc. Case No. 3:23-0770-RFL, Northern District of California.Read:INTRODUCTIONWHAT IS THIS LAWSUIT ABOUTHOW TO JOIN THIS LAWSUITHOW WILL YOUR CLAIM BE HANDLED AND PROVEN?LEGAL EFFECT OF OPTING IN TO JOIN THIS CASENO RETALIATION PERMITTEDYOUR LEGAL REPRESENTATION IF YOU JOINOPT-IN CONSENT TO JOIN FORMThe Age Discrimination in Employment Act of 1967By submitting this form, I am consenting to join the collective action Mobley v. Workday, Inc., case number 3:23-cv-0770-RFL (“Lawsuit”), pending in the Northern District of California Federal Court, which includes claims under the Age Discrimination in Employment Act of 1967 (“ADEA”), and to be bound by the judgment of the Court.By submitting this form, I confirm that on or after September 24, 2020, I applied for at least one employment opportunity using Workday’s application platform while I was 40 or more years old.I am checking the box below as my signature for the Opt-In Consent To Join and as my authorization for it to be filed in the Northern District of California Federal Court on my behalf.(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[7]='DOB';ftypes[7]='text';fnames[3]='MMERGE3';ftypes[3]='address';fnames[0]='EMAIL';ftypes[0]='email';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';fnames[6]='MMERGE6';ftypes[6]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true); // SMS Phone Multi-Country Functionality if(!window.MC) { window.MC = {}; } window.MC.smsPhoneData = { defaultCountryCode: 'US', programs: [], smsProgramDataCountryNames: [] }; function getCountryUnicodeFlag(countryCode) { return countryCode.toUpperCase().replace(/./g, (char) =>I. INTRODUCTION This notice and its contents have been authorized by the United States District Court for the Northern District of California. The Court has taken no position in this case regarding the merits of Plaintiffs’ claims or of Workday’s defenses.The purpose of this notice is to provide notification of your following rights under the Age Discrimination in Employment Act (“ADEA”):You have the right to join this lawsuit if you applied for employment opportunities using Workday’s application platform since September 24, 2020, while you were 40 or more years old.To join this lawsuit you must fill out and electronically sign and send in the Opt-In Consent To Join Form by clicking submit on or before March 7, 2026. How to do that is explained more fully in this Section III of this notice.You have the right to have the Plaintiffs’ attorneys represent you in connection with this case, and to contact them for advice or assistance regarding the Opt-In Form or regarding the case, at this link or via the email addresses and phone numbers listed below in Section V of this NoticeII. WHAT IS THIS LAWSUIT ABOUT? Derek Mobley (“Plaintiff”) filed this action against Workday, Inc. in the Northern District of California, Case No. 3:23-0770-RFL (“Lawsuit”), on behalf of himself and all others similarly situated. His allegations include that Workday, Inc., through its use of certain Artificial Intelligence (“AI”) features on its job application platform, violated the Age Discrimination in Employment Act (“ADEA”). Workday denies these allegations. The Court has not made any findings about whether Plaintiff’s claims or Workday’s denial of liability have any merit.The Court has provisionally certified an ADEA collective, which includes: “All individuals aged 40 and over who, from September 24, 2020, through the present, applied for job opportunities using Workday, Inc.’s job application platform and were denied employment recommendations.” In this context, being “denied” an “employment recommendation” means that (i) the individual’s application was scored, sorted, ranked, or screened by Workday’s AI; (ii) the result of the AI scoring, sorting, ranking, or screening was not a recommendation to hire; and (iii) that result was communicated to the prospective employer, or the result was an automatic rejection by Workday.Once you submit your Opt-In Consent to Join Form, the Court will subsequently need to determine that you meet the requirements described in the above paragraph in order for you to have the right to stay in the Lawsuit. However, all you need to know right now to opt-in to this case is that, on or after September 24, 2020, you applied for employment opportunities using Workday’s application platform while you were 40 or more years old.III. HOW TO JOIN THIS LAWSUIT To join this case, please complete, sign and return the Opt-In Consent To Join Form. If you have questions, you may contact the Plaintiffs’ attorneys by clicking submit or at the addresses and phone numbers provided below in Section V of this Notice.If you are unable to sign and return the Opt-In Consent To Join Form electronically through this notice, you may obtain a paper copy of such forms to sign and return by mail, email, text or other delivery on or before March 7 2026. A paper copy can be downloaded and printed from this link or you can contact Plaintiffs’ attorneys to request such Forms by calling, emailing, texting or mailing them at the addresses or phone numbers provided below in Section V of this Notice. To join this case you must submit your Opt-In Consent To Join Form on or before March 7, 2026.The Opt-In Consent To Join Form requires that you sign page one and that you list your contact information and birthday on page two. Only page one will be publicly filed with the Court. Your contact information and birthday will not be filed with the Court as part of your initial opt-in to the case.IV. HOW WILL YOUR CLAIM BE HANDLED AND PROVEN? After you submit your Opt-In Consent To Join Form to the Plaintiffs’ attorneys for filing, and unless and until you decide otherwise, they will continue to represent you as your counsel in this matter. As your legal representative, Plaintiffs’ attorneys can help you obtain the documents, testimony, and other evidence that you may need to show that you meet all the requirements necessary to litigate your claim as a party to this Lawsuit. If you choose to join this case, you may be required, with the help of Plaintiffs’ attorneys, to answer written questions, produce documents related to your job application process, attend a deposition, and/or testify in court. Workday may also be required to answer written questions or produce documents related to its products to you. The Court has procedures that will protect your confidential information from public disclosure, including your birthday.V. LEGAL EFFECT OF OPTING IN TO JOIN THIS CASE If you opt in to the Lawsuit, you will be bound by a favorable or unfavorable judgment on Plaintiffs’ ADEA claim. If you do not opt in, you will be precluded from future participation in the ADEA collective, including participation in any settlement related to the ADEA claim, but you may still bring your own separate lawsuit, depending on your situation. No ADEA collective has been finally certified, and you may still be dismissed from the case.VI. NO RETALIATION PERMITTED Federal law prohibits Workday from retaliating against you for opting in to this Lawsuit or otherwise exercising your rights.VII. YOUR LEGAL REPRESENTATION IF YOU JOIN When you join this Lawsuit by electronically completing and sending the Opt-In Consent To Join Form, unless you decide otherwise, the lawyers representing you will be:You will not be responsible for paying any lawyers’ fees. Plaintiffs’ attorneys are being paid on contingency, which means that if Plaintiffs do not receive a recovery in this case, no lawyer fees will be owed. If Plaintiffs obtain a recovery, either by settlement or judgment, Plaintiffs’ attorneys may file a request with the Court to receive part of the recovery as compensation for their services.If you want to join the Lawsuit, but prefer to retain your own attorney to represent you, you have the right to do so. You also have the right to represent yourself. To join this case you must submit the Opt-In Consent To Join Form with the Court on or before March 7, 2026.PLEASE DO NOT CONTACT THE COURT WITH QUESTIONS ABOUT THIS LAWSUIT.Almost there! Check your inbox and click the confirmation link to complete your submission. Don’t see the email? Check spam or promotions or contact [email protected].PLEASE DO NOT CONTACT THE COURT WITH QUESTIONS ABOUT THIS LAWSUIT.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:55:42.178160Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 110,
      "num_comments": 36,
      "engagement_score": 182.0
    },
    {
      "link": "https://danielsada.tech/blog/ai-lazyslop-and-personal-responsibility/",
      "author": "Daniel Sada",
      "title": "Daniel Sada Caraveo - AI Lazyslop, and Personal Responsibility - Software, Notes & Culture",
      "source": "hackernews",
      "content": "Once upon a time, I got a PR from a coworker, I’ll call him Mike.\nMike sent me a 1600 line pull-request with no tests, entirely written by AI, and expected me to approve it immediately as to not to block him on his deployment schedule.\nWhen asking for tests, I’d get pushback on “why do I need tests? It works already”. Then, I’d get a ping from his manager asking on why am I blocking the review.\nAfter I “Requested changes” he’d get frustrated that I’d do that, and put all his changes in an already approved PR and sneak merge it in another PR.\nI don’t blame Mike, I blame the system that forced him to do this.\nBut this is the love letter I’d wish I could have written to him.\nAI and personal responsibility.\nDear Mike,\nI know you wrote your PR entirely using AI but you didn’t review it at all.\nI’m not opposed to you using AI, but what I want to know is:\nWhat was your thought process using AI?\nShare your prompts! Share your process! It helps me understand your rationale.\nDid you personally review the code?\nOne tip I personally do is to add comments to my own PR to show the team my process.\nDid you use any AI reviewing tools? What things did you decide to fix and ignore?\nI care more about what you think.\nHow do you have confidence that this works?\nThe cost of tests is cheaper with AI, can you write a test for it?\nBut also don’t lazy-slop-tests that only test language fundamentals. (Classic test testing dictionaries getters and setters.)\nOthers in the industry\nAs we find ourselves in this new world, there is still some shame in using AI. But we shouldn’t! Ghostty is asking people to disclose the use of AI upfront and I think changes like this are positive.\nLinus Torvalds recently mentioned playing and vibe-coding with AI for a language he didn’t master. We are experiencing a cultural shift in how this tool integrates in our daily lives. Even if you, personally, don’t use AI; There is a high probability that a coworker or collaborator will, and then, you have to decide the rules of engagement.\nGiven that, I want to define AI Lazyslop.\nAI Lazyslop: AI generation that was not read by the creator, which generates a burden on the reader to review.\nThe anti-AI Lazyslop manifesto.\nI will own my code, and the outputs of the parts of the LLM I decide to accept. I will disclose use of AI for my code.\nI attest that:\nI have read and tested all my code.\nI have included relevant plans/prompts in my PR.\nI have reviewed my code with some AI assistance, and this is the summary of what I decided to fix:\nI can explain the logic and design decisions in this code without referring back to AI.\nWhat happened to Mike?\nWell, I’d like to say there was a happy story, but Mike evolved to the semi-lazy-slop mode, in which he relayed all of the comments reviewers had to his PR to the LLM. Is that better? Is that worse? I’m not sure, but I’m guessing this is happening in a lot of places.\nAI Disclosure\nI used claude to help me “Help me review my blog post for style and grammar”. It caught the following:\nLine 18: Missing space after \"immediately\"\nCurrent: immediately not to block him\nBetter: immediately so as not to block him or immediately to avoid blocking him\nLine 18: Question punctuation\nCurrent: \"why do I need tests, it works already\"\nShould be: \"why do I need tests? It works already\" (separate sentences or use semicolon)\nLine 20: Unnecessary quotes around \"Requesting changes\"\nCurrent: After me \"Requesting changes\"\nBetter: After I requested changes or After my \"Requesting changes\" review\nLine 46: Typo\nCurrent: rules of engeagement\nShould be: rules of engagement\nLine 46: Comma splice\nCurrent: Even if you, personally, don't use AI; There is a high probability\nShould be: Even if you, personally, don't use AI, there is a high probability (lowercase \"there\" after semicolon, or use period)\nLine 51: The sentence is a bit long. Consider: I will own my code and the LLM outputs I decide to accept. I will disclose my use of AI.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:55:52.008210Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 59,
      "num_comments": 67,
      "engagement_score": 193.0
    },
    {
      "link": "https://www.theregister.com/2026/01/26/cursor_opinion/",
      "author": null,
      "title": "Cursor is better at marketing than coding • The Register",
      "source": "hackernews",
      "content": "Opinion AI-integrated development environment (IDE) company Cursor recently implied it had built a working web browser almost entirely with its AI agents. I won't say they lied, but CEO Michael Truell certainly tweeted: \"We built a browser with GPT-5.2 in Cursor.\"\nHe followed up with: \"It's 3M+ lines of code across thousands of files. The rendering engine is from-scratch in Rust with HTML parsing, CSS cascade, layout, text shaping, paint, and a custom JS VM.\"\nThat sounds impressive, doesn't it? He also added: \"It *kind of* works,\" which is not the most ringing endorsement. Still, numerous news sources and social media chatterboxes ran with the news that AI built a web browser in a week.\nToo bad it wasn't true. If you actually looked at Cursor engineer Wilson Lin's blog post about FastRender, the AI-created web browser, you won't see much boasting about a working web browser. Instead, there's a video of a web browser sort of working, and a much less positive note that \"building a browser from scratch is extremely difficult.\"\nThe thing about making such a software announcement on GitHub is that while the headlines are proclaiming another AI victory, developers have this nasty trick. They actually git the code and try it out.\nDevelopers quickly discovered the \"browser\" barely compiles, often does not run, and was heavily misrepresented in marketing.\nAs a techie, the actual blog post about how they tried and didn't really succeed was much more interesting. Of course, that Cursor sicced hundreds of GPT-5.2-style agents which ran for a week to produce three million lines of new code, to produce, at best, a semi-functional web browser from scratch, doesn't make for a good headline.\nAccording to Perplexity, my AI chatbot of choice, this week‑long autonomous browser experiment consumed in the order of 10-20 trillion tokens and would have cost several million dollars at then‑current list prices for frontier models.\nI'd just cloned a copy of Chromium myself, and for all that time and money, independent developers who cloned the repo reported that the codebase is very far from a functional browser. Recent commits do not compile cleanly, GitHub Actions runs on main are failing, and reviewers could not find a single recent commit that was built without errors.\nWhere builds succeeded after manual patching, performance was abysmal, with reports of pages taking around a minute to load and a heavy reliance on existing projects like Servo, a Rust-based web rendering engine, and QuickJS, a JavaScript engine, despite \"from scratch\" claims.\nLin defended the project on Y Combinator, saying, for instance: \"The JS engine used a custom JS VM being developed in vendor/ecma-rs as part of the browser, which is a copy of my personal JS parser project vendored to make it easier to commit to.\" If it's derived from his personal JavaScript parser, that's not really from scratch, is it? Nor is it, from the sound of the argument, written by AI.\nGregory Terzian, a Servo maintainer, responded: \"The actual code is worse; I can only describe it as a tangle of spaghetti... I can't make much, if anything, out of it.\" He then gave the backhanded compliment: \"So I agree this isn't just wiring up of dependencies, and neither is it copied from existing implementations: it's a uniquely bad design that could never support anything resembling a real-world web engine.\" Now that's a burn.\nFrom where I sit, what makes the Cursor case more dangerous than just a failed hack‑week project is that the hype is baked into its methodology. The \"experiment\" wasn't presented as what it really was: an interesting, but messy, internal learning exercise. No, it was rolled out as a milestone that conveniently confirmed the company's long‑running autonomous agent advertising. Missing from the story were basics any senior engineer would demand: passing Continuous Integration (CI), reproducible builds, and real benchmarks that show the browser doing more than limping through a hello-world page.\nZoom out, and CEOs are still predicting that AI will write 90 percent of code in a year, while most enterprise AI pilots still fail to deliver meaningful return on investment.\nWe're now in a kind of AI uncanny valley for developers. Sure, tools like Cursor can be genuinely helpful as glorified autocomplete and refactoring assistants, but marketing keeps insisting junior engineers can take whole projects from spec to shipping. When you start believing your own sizzle reel, you stop doing the tedious validation work that separates a demo from a deliverable.\nEnough already. The hype has grown cold. Sarah Friar, OpenAI's CFO, recently blogged that in 2026, its focus would be on \"practical adoption.\" Let's see real-world practical results first, and then we can talk about practical AI adoption. ®",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:55:58.605495Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 204,
      "num_comments": 125,
      "engagement_score": 454.0
    },
    {
      "link": "https://ntdotdev.wordpress.com/2026/01/25/state-of-the-windows-what-is-going-on-with-windows-11/",
      "author": "NTDEV",
      "title": "State of the Windows: What is going on with Windows 11? - NTDEV",
      "source": "hackernews",
      "content": "Hi! Long time no see, huh? 🙂\nIt’s been three years since my last State of the Windows article, which was about the inconsistencies in the Windows 11 user experience. Since then, Microsoft (and the world as a whole to be quite honest) has gotten through a lot of changes, especially since the introduction of OpenAI’s ChatGPT, and in our case, most importantly, Microsoft Copilot.\nUntil a few years ago, Windows was Microsoft’s crown jewel (or as they said in the Windows 7 commercial, the heartbeat of Microsoft), an impressive operating system that had the purpose to be a common platform for all devices. However, since the launch of Windows 11, which at first had the purpose to somewhat modernize the look and feel of the OS, it seems that priorities have changed quite a bit.\nToday we are going to talk about the perceived drop of quality in Windows, from fundamental issues like critical bugs and incidents from the last 3 years to how everything’s become a Copilot upsell funnel mechanism.\nSo, without further ado, let’s go!\nFirst, let’s talk about the current, show-stopping errors that appeared with the latest January 2026 update, and how to fix them.\n1. PCs that wouldn’t shutdown – January 2026.\nThis is one of the bugs that I’ve actually encountered a few days ago at work – people would come in saying that their PCs would either act unresponsive after shutting down (as in, they wouldn’t start up again the next day), or it would reboot immediately after shutting down.\nThis issue was introduced with the January 2026 update, KB5073455 and it is more prominent on newer platforms, especially Intel’s Meteor Lake and Arrow Lake.\nReportedly this issue is caused by the System Guard Secure Launch, a virtualization-based security component which, as the name implies, protects the boot process by using DRTM, or Dynamic Root of Trust for Measurement.\nMore information about this technology can be found here.\nSince last Patch Tuesday, Microsoft released a patch for this issue, KB5077797, which seems to solve the issues.\nAs a temporary workaround, users can shutdown their PCs using the Command Prompt, by entering shutdown /s /t 0. But this begs the question, why does the shutdown command work through this command which supposedly does the same thing, but not through the classic shutdown button?\n2. Outlook “classic” can’t open pst files – january 2026\nOnce again, this month’s Patch Tuesday brought another major issue, which has the potential to affect one’s productivity in a big way.\nIf the KB5074109 is installed, applications can become unresponsive when accessing PST files stored on cloud storage. To put it simply, if you have your mail archive saved on OneDrive or Dropbox and are still using the classic Win32 Outlook (and not the WebView based one) it’s possible that you can’t access the file, since the application would become unresponsive.\nAlso, users with Outlook POP account profiles and profiles with PST files report that Outlook hangs and does not exit properly.\nUp until yesterday, the only fixes were to either uninstall the update or to “use webmail”.\nTheir words, not mine.\nHowever, as of January 25th, Microsoft released the KB5078132 update, which should fix the issue.\nSome other issues with the latest update include:\nApps that wouldn’t load (including system ones like Notepad), crashing with the error 0x803f8001.\nUnbootable volumes – the infamous 0x7f BSOD which means that it can’t mount the disk partition to continue booting.\nThese are the most glaring issues that were introduced with the latest Windows update.\nbut as a whole, windows is a mess.\nSince the introduction of 24H2, Microsoft had a scandal related to Windows almost every month.\nSome of the most critical ones include:\nWinRE wouldn’t recognize keyboard and mouse input after the October 2025 update\nTask Manager wouldn’t close completely if KB5067036 was installed, meaning that whenever one opened a new taskmgr window a new instance would be launched.\nRDP failures with 24H2/25H2 – once again, this is actually a critical issue that I’ve also encountered at work, and because of it we had to rollback a few PCs to 23H2 in order to have stable RDP connections.\nVarious devices like DACs or webcams not working after installing updates.\nDRM video issues with the September 2025 update\nUnfortunately, these are only some of the more prominent issues that occurred in the last year.\nBut wait, that’s not all!\nAnother ever-growing issue is the fact that Windows is bloated. And I’m not talking about the number of apps that are included in the OS or something that any script or custom OS could fix, but the fact that critical components are becoming so heavy that Microsoft has to develop workarounds in order to make them feel faster.\nThe prime example is Windows Explorer, which has become so sluggish that it has to be preloaded in order to make it faster, but even then it’s actually slower than previous versions of Windows!\nHow can one of the core UI elements of the OS become so heavy while not offering any noticeable quality-of-life improvement (apart from the introduction of tabs in Windows 11 22H2)?\nEven Windows updates have become insanely big. Just take a look at the latest January 2026 update for Windows 11 23H2 and the one for 25H2.\nMore than 4 times bigger! And you probably know why. More on that later.\nLast, but not least, the technical debt of Windows has become almost unbearable. 30+ years of Windows NT certainly adds up. And the fact that Microsoft can’t focus on a visual language or a software platform for once is daunting, especially since given the nature of Windows you have to support them all.\nHowever, the biggest problem is that Windows is not only buggy. Sure, a piece of software of this magnitude can and will have its fair share of issues. Our use cases have gotten more complex and the software that we expect to fulfill those use cases have become more complex, more secure, more powerful, you name it. That being said…\nwindows is getting annoying.\nLet’s tackle the elephant in the room.\nLLMs (and AI as a whole) have the potential to be an outright revolution, changing the fabric of society and the way of our lives. Microsoft Copilot as well could be a brilliant idea if it is implemented correctly and has the power to completely transform Microsoft into an even bigger juggernaut, unlocking even more of that sweet shareholder value… if it’s done properly, that is.\nIntroduced in 2023, Microsoft Copilot is Microsoft’s chatbot based on OpenAI’s models after a $10 billion investment. Introduced in Windows 11 with the December 2023 Patch Tuesday, at first it was just a benign WebView application that could be easily removed. However, since 2024, Copilot and artificial intelligence have taken a critical role in Microsoft’s overall strategy, especially in areas like programming.\nThis shift also showed up in subsequent versions of Windows.\nIn Windows 11 24H2, the main highlight of the new update was the introduction of a suite of AI features, with the most important being Windows Recall.\nThe flagship feature of 24H2, Recall was intended to a be a “photographic memory” of sorts for your PC. This meant that, as you might have guessed, Windows Recall would take a snapshot of your screen every few seconds. Then, the information from those screenshots would be processed using local AI to make it searchable.\nHowever, soon after it was introduced it became clear that it was a security nightmare. Early security researchers discovered that the data was stored in a largely unencrypted SQLite database, making it a goldmine for info-stealing malware. Not only that, but the early builds of Recall had no option to disable it. The backlash was so severe that Microsoft was forced to pull the feature just days before the Copilot+ PC launch in 2024, leaving a new generation of PCs without their flagship feature.\nApplications like Signal and Brave implemented anti-Recall features that would prevent the app from taking screenshots.\nWhile Microsoft would mitigate the main security concerns related to Recall, by making it uninstallable, encrypting its database and making it usable only if Windows Hello was enabled, the damage was already done.\nAlso, Windows Recall is one of the main reasons why Windows updates are so big these days, as each Windows update also introduces updates to the AI models included with the OS, even though you may not even have a Copilot+ PC that supports these features.\nApart from Recall, Windows has also suffered from an acute Copilot-ification.\nThe preinstalled browser (Edge) has Copilot.\nThe main text editor (Notepad) has Copilot.\nThere is also a dedicated Copilot app which is preinstalled and non-removable (in most cases)\nThe main photo viewer has a Copilot button which just opens the main app\nThe Settings application has Copilot in its search function (which doesn’t even work properly).\nThe Search application has Copilot (which only opens up the Copilot app) and needs a COMPLETE overhaul in my opinion.\nPaint has Copilot\nOffice has Copilot (which is one of the few instances where Copilot and AI as a whole is genuinely useful if used correctly)\nAlso, judging by the latest Windows 11 builds, even Windows Explorer, which is already quite heavy in itself as we discussed earlier is getting Copilot!\nWith the Copilot epidemic also came the complete death of the “offline” Windows.\nPerhaps the most nagging change since 2023 is the slow death of the local account. Microsoft has spent the last three years systematically closing every “backdoor” that allowed users to set up Windows without a Microsoft Account.\nMethods like OOBE.exe /bypassNRO or a@a.com as the email for the account have been “fixed”, meaning that it becomes harder and harder to use a local account with each subsequent update.\na vision with a crumbling foundation\nLooking back at the last three years, the “State of Windows” is one of extreme ambition built on a crumbling foundation. Microsoft is trying to build a futuristic AI skyscraper, but they are building it on top of a basement filled with 30-year-old technical debt and a ground floor that can’t even handle a shutdown command properly.\nUnfortunately, the issue that plagued Windows since the dawn of time has only aggravated recently. Windows 11 is a mixture of old and new technologies that are glued together, with decades of legacy code that simply refuses to die (because if it did a lot of corporate costumers would complain, and whether we like it or not they are paying big cash for support to Microsoft).\nAlso, it tries to have a “modern” UI that unfortunately not only is inconsistent, but also it’s too heavy for its own good, being just a lipstick on a bloated old pig.\nLast, but certainly not least, it is full of AI features that most people didn’t ask for, some are even actively feared (see Recall) and are also quite lacking in polish and usefulness.\nI feel like a good analogy of the current situation with Windows is this picture which was shown in an internal presentation regarding the performance and reliability of pre-reset Longhorn\nUntil Microsoft stops treating Windows as an “AI innovation platform” of sorts and starts treating it as the stable, reliable tool it was always meant to be, the user experience will continue to feel like a battle between the person sitting at the desk and the company that built the desk.\nThank you for you attention.\nDid you find this article interesting? Writing these deep dives (once every few years 😂) takes a lot of time (and caffeine).☕Also it would be nice if I would get a proper domain for this page.\n[Support my work!]",
      "publish_datetime": "2026-01-25T21:08:44+00:00",
      "scraping_timestamp": "2026-01-27T05:56:16.176143Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 115,
      "num_comments": 167,
      "engagement_score": 449.0
    },
    {
      "link": "https://github.com/clawdbot/clawdbot",
      "author": "clawdbot",
      "title": "GitHub - clawdbot/clawdbot: Your own personal AI assistant. Any OS. Any Platform. The lobster way. 🦞",
      "source": "hackernews",
      "content": "🦞 Clawdbot — Personal AI Assistant\nEXFOLIATE! EXFOLIATE!\nClawdbot is a personal AI assistant you run on your own devices.\nIt answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane — the product is the assistant.\nIf you want a personal, single-user assistant that feels local, fast, and always-on, this is it.\nWebsite · Docs · Getting Started · Updating · Showcase · FAQ · Wizard · Nix · Docker · Discord\nPreferred setup: run the onboarding wizard (clawdbot onboard). It walks through gateway, workspace, channels, and skills. The CLI wizard is the recommended path and works on macOS, Linux, and Windows (via WSL2; strongly recommended).\nWorks with npm, pnpm, or bun.\nNew install? Start here: Getting started\nSubscriptions (OAuth):\nAnthropic (Claude Pro/Max)\nOpenAI (ChatGPT/Codex)\nModel note: while any model is supported, I strongly recommend Anthropic Pro/Max (100/200) + Opus 4.5 for long‑context strength and better prompt‑injection resistance. See Onboarding.\nModels (selection + auth)\nModels config + CLI: Models\nAuth profile rotation (OAuth vs API keys) + fallbacks: Model failover\nInstall (recommended)\nRuntime: Node ≥22.\nnpm install -g clawdbot@latest\n# or: pnpm add -g clawdbot@latest\nclawdbot onboard --install-daemon\nThe wizard installs the Gateway daemon (launchd/systemd user service) so it stays running.\nQuick start (TL;DR)\nRuntime: Node ≥22.\nFull beginner guide (auth, pairing, channels): Getting started\nclawdbot onboard --install-daemon\nclawdbot gateway --port 18789 --verbose\n# Send a message\nclawdbot message send --to +1234567890 --message \"Hello from Clawdbot\"\n# Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)\nclawdbot agent --message \"Ship checklist\" --thinking high\nUpgrading? Updating guide (and run clawdbot doctor).\nDevelopment channels\nstable: tagged releases (vYYYY.M.D or vYYYY.M.D-<patch>), npm dist-tag latest.\nbeta: prerelease tags (vYYYY.M.D-beta.N), npm dist-tag beta (macOS app may be missing).\ndev: moving head of main, npm dist-tag dev (when published).\nSwitch channels (git + npm): clawdbot update --channel stable|beta|dev.\nDetails: Development channels.\nFrom source (development)\nPrefer pnpm for builds from source. Bun is optional for running TypeScript directly.\ngit clone https://github.com/clawdbot/clawdbot.git\ncd clawdbot\npnpm install\npnpm ui:build # auto-installs UI deps on first run\npnpm build\npnpm clawdbot onboard --install-daemon\n# Dev loop (auto-reload on TS changes)\npnpm gateway:watch\nNote: pnpm clawdbot ... runs TypeScript directly (via tsx). pnpm build produces dist/ for running via Node / the packaged clawdbot binary.\nSecurity defaults (DM access)\nClawdbot connects to real messaging surfaces. Treat inbound DMs as untrusted input.\nFull security guide: Security\nDefault behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack:\nDM pairing (dmPolicy=\"pairing\" / channels.discord.dm.policy=\"pairing\" / channels.slack.dm.policy=\"pairing\"): unknown senders receive a short pairing code and the bot does not process their message.\nApprove with: clawdbot pairing approve <channel> <code> (then the sender is added to a local allowlist store).\nPublic inbound DMs require an explicit opt-in: set dmPolicy=\"open\" and include \"*\" in the channel allowlist (allowFrom / channels.discord.dm.allowFrom / channels.slack.dm.allowFrom).\nRun clawdbot doctor to surface risky/misconfigured DM policies.\nHighlights\nLocal-first Gateway — single control plane for sessions, channels, tools, and events.\nMulti-channel inbox — WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, BlueBubbles, Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android.\nMulti-agent routing — route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions).\nVoice Wake + Talk Mode — always-on speech for macOS/iOS/Android with ElevenLabs.\nLive Canvas — agent-driven visual workspace with A2UI.\nFirst-class tools — browser, canvas, nodes, cron, sessions, and Discord/Slack actions.\nCompanion apps — macOS menu bar app + iOS/Android nodes.\nOnboarding + skills — wizard-driven setup with bundled/managed/workspace skills.\nStar History\nEverything we built so far\nCore platform\nGateway WS control plane with sessions, presence, config, cron, webhooks, Control UI, and Canvas host.\nCLI surface: gateway, agent, send, wizard, and doctor.\nPi agent runtime in RPC mode with tool streaming and block streaming.\nSession model: main for direct chats, group isolation, activation modes, queue modes, reply-back. Group rules: Groups.\nMedia pipeline: images/audio/video, transcription hooks, size caps, temp file lifecycle. Audio details: Audio.\nChannels\nChannels: WhatsApp (Baileys), Telegram (grammY), Slack (Bolt), Discord (discord.js), Google Chat (Chat API), Signal (signal-cli), iMessage (imsg), BlueBubbles (extension), Microsoft Teams (extension), Matrix (extension), Zalo (extension), Zalo Personal (extension), WebChat.\nGroup routing: mention gating, reply tags, per-channel chunking and routing. Channel rules: Channels.\nApps + nodes\nmacOS app: menu bar control plane, Voice Wake/PTT, Talk Mode overlay, WebChat, debug tools, remote gateway control.\niOS node: Canvas, Voice Wake, Talk Mode, camera, screen recording, Bonjour pairing.\nAndroid node: Canvas, Talk Mode, camera, screen recording, optional SMS.\nmacOS node mode: system.run/notify + canvas/camera exposure.\nTools + automation\nBrowser control: dedicated clawd Chrome/Chromium, snapshots, actions, uploads, profiles.\nCanvas: A2UI push/reset, eval, snapshot.\nNodes: camera snap/clip, screen record, location.get, notifications.\nCron + wakeups; webhooks; Gmail Pub/Sub.\nSkills platform: bundled, managed, and workspace skills with install gating + UI.\nRuntime + safety\nChannel routing, retry policy, and streaming/chunking.\nPresence, typing indicators, and usage tracking.\nModels, model failover, and session pruning.\nSecurity and troubleshooting.\nOps + packaging\nControl UI + WebChat served directly from the Gateway.\nTailscale Serve/Funnel or SSH tunnels with token/password auth.\nNix mode for declarative config; Docker-based installs.\nDoctor migrations, logging.\nHow it works (short)\nWhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat\n│\n▼\n┌───────────────────────────────┐\n│\nGateway\n│\n│\n(control plane)\n│\n│\nws://127.0.0.1:18789\n│\n└──────────────┬────────────────┘\n│\n├─ Pi agent (RPC)\n├─ CLI (clawdbot …)\n├─ WebChat UI\n├─ macOS app\n└─ iOS / Android nodes\nKey subsystems\nGateway WebSocket network — single WS control plane for clients, tools, and events (plus ops: Gateway runbook).\nTailscale exposure — Serve/Funnel for the Gateway dashboard + WS (remote access: Remote).\nBrowser control — clawd‑managed Chrome/Chromium with CDP control.\nCanvas + A2UI — agent‑driven visual workspace (A2UI host: Canvas/A2UI).\nVoice Wake + Talk Mode — always‑on speech and continuous conversation.\nNodes — Canvas, camera snap/clip, screen record, location.get, notifications, plus macOS‑only system.run/system.notify.\nTailscale access (Gateway dashboard)\nClawdbot can auto-configure Tailscale Serve (tailnet-only) or Funnel (public) while the Gateway stays bound to loopback. Configure gateway.tailscale.mode:\noff: no Tailscale automation (default).\nserve: tailnet-only HTTPS via tailscale serve (uses Tailscale identity headers by default).\nfunnel: public HTTPS via tailscale funnel (requires shared password auth).\nNotes:\ngateway.bind must stay loopback when Serve/Funnel is enabled (Clawdbot enforces this).\nServe can be forced to require a password by setting gateway.auth.mode: \"password\" or gateway.auth.allowTailscale: false.\nFunnel refuses to start unless gateway.auth.mode: \"password\" is set.\nOptional: gateway.tailscale.resetOnExit to undo Serve/Funnel on shutdown.\nDetails: Tailscale guide · Web surfaces\nRemote Gateway (Linux is great)\nIt’s perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over Tailscale Serve/Funnel or SSH tunnels, and you can still pair device nodes (macOS/iOS/Android) to execute device‑local actions when needed.\nGateway host runs the exec tool and channel connections by default.\nDevice nodes run device‑local actions (system.run, camera, screen recording, notifications) via node.invoke.\nIn short: exec runs where the Gateway lives; device actions run where the device lives.\nDetails: Remote access · Nodes · Security\nmacOS permissions via the Gateway protocol\nThe macOS app can run in node mode and advertises its capabilities + permission map over the Gateway WebSocket (node.list / node.describe). Clients can then execute local actions via node.invoke:\nsystem.run runs a local command and returns stdout/stderr/exit code; set needsScreenRecording: true to require screen-recording permission (otherwise you’ll get PERMISSION_MISSING).\nsystem.notify posts a user notification and fails if notifications are denied.\ncanvas.*, camera.*, screen.record, and location.get are also routed via node.invoke and follow TCC permission status.\nElevated bash (host permissions) is separate from macOS TCC:\nUse /elevated on|off to toggle per‑session elevated access when enabled + allowlisted.\nGateway persists the per‑session toggle via sessions.patch (WS method) alongside thinkingLevel, verboseLevel, model, sendPolicy, and groupActivation.\nDetails: Nodes · macOS app · Gateway protocol\nAgent to Agent (sessions_* tools)\nUse these to coordinate work across sessions without jumping between chat surfaces.\nsessions_list — discover active sessions (agents) and their metadata.\nsessions_history — fetch transcript logs for a session.\nsessions_send — message another session; optional reply‑back ping‑pong + announce step (REPLY_SKIP, ANNOUNCE_SKIP).\nDetails: Session tools\nSkills registry (ClawdHub)\nClawdHub is a minimal skill registry. With ClawdHub enabled, the agent can search for skills automatically and pull in new ones as needed.\nClawdHub\nChat commands\nSend these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only):\n/status — compact session status (model + tokens, cost when available)\n/new or /reset — reset the session\n/compact — compact session context (summary)\n/think <level> — off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only)\n/verbose on|off\n/usage off|tokens|full — per-response usage footer\n/restart — restart the gateway (owner-only in groups)\n/activation mention|always — group activation toggle (groups only)\nApps (optional)\nThe Gateway alone delivers a great experience. All apps are optional and add extra features.\nIf you plan to build/run companion apps, follow the platform runbooks below.\nmacOS (Clawdbot.app) (optional)\nMenu bar control for the Gateway and health.\nVoice Wake + push-to-talk overlay.\nWebChat + debug tools.\nRemote gateway control over SSH.\nNote: signed builds required for macOS permissions to stick across rebuilds (see docs/mac/permissions.md).\niOS node (optional)\nPairs as a node via the Bridge.\nVoice trigger forwarding + Canvas surface.\nControlled via clawdbot nodes ….\nRunbook: iOS connect.\nAndroid node (optional)\nPairs via the same Bridge + pairing flow as iOS.\nExposes Canvas, Camera, and Screen capture commands.\nRunbook: Android connect.\nAgent workspace + skills\nWorkspace root: ~/clawd (configurable via agents.defaults.workspace).\nInjected prompt files: AGENTS.md, SOUL.md, TOOLS.md.\nSkills: ~/clawd/skills/<skill>/SKILL.md.\nConfiguration\nMinimal ~/.clawdbot/clawdbot.json (model + defaults):\n{\nagent: {\nmodel: \"anthropic/claude-opus-4-5\"\n}\n}\nFull configuration reference (all keys + examples).\nSecurity model (important)\nDefault: tools run on the host for the main session, so the agent has full access when it’s just you.\nGroup/channel safety: set agents.defaults.sandbox.mode: \"non-main\" to run non‑main sessions (groups/channels) inside per‑session Docker sandboxes; bash then runs in Docker for those sessions.\nSandbox defaults: allowlist bash, process, read, write, edit, sessions_list, sessions_history, sessions_send, sessions_spawn; denylist browser, canvas, nodes, cron, discord, gateway.\nDetails: Security guide · Docker + sandboxing · Sandbox config\nWhatsApp\nLink the device: pnpm clawdbot channels login (stores creds in ~/.clawdbot/credentials).\nAllowlist who can talk to the assistant via channels.whatsapp.allowFrom.\nIf channels.whatsapp.groups is set, it becomes a group allowlist; include \"*\" to allow all.\nTelegram\nSet TELEGRAM_BOT_TOKEN or channels.telegram.botToken (env wins).\nOptional: set channels.telegram.groups (with channels.telegram.groups.\"*\".requireMention); when set, it is a group allowlist (include \"*\" to allow all). Also channels.telegram.allowFrom or channels.telegram.webhookUrl as needed.\n{\nchannels: {\ntelegram: {\nbotToken: \"123456:ABCDEF\"\n}\n}\n}\nSlack\nSet SLACK_BOT_TOKEN + SLACK_APP_TOKEN (or channels.slack.botToken + channels.slack.appToken).\nDiscord\nSet DISCORD_BOT_TOKEN or channels.discord.token (env wins).\nOptional: set commands.native, commands.text, or commands.useAccessGroups, plus channels.discord.dm.allowFrom, channels.discord.guilds, or channels.discord.mediaMaxMb as needed.\n{\nchannels: {\ndiscord: {\ntoken: \"1234abcd\"\n}\n}\n}\nSignal\nRequires signal-cli and a channels.signal config section.\niMessage\nmacOS only; Messages must be signed in.\nIf channels.imessage.groups is set, it becomes a group allowlist; include \"*\" to allow all.\nMicrosoft Teams\nConfigure a Teams app + Bot Framework, then add a msteams config section.\nAllowlist who can talk via msteams.allowFrom; group access via msteams.groupAllowFrom or msteams.groupPolicy: \"open\".\nWebChat\nUses the Gateway WebSocket; no separate WebChat port/config.\nBrowser control (optional):\n{\nbrowser: {\nenabled: true,\ncolor: \"#FF4500\"\n}\n}\nDocs\nUse these when you’re past the onboarding flow and want the deeper reference.\nStart with the docs index for navigation and “what’s where.”\nRead the architecture overview for the gateway + protocol model.\nUse the full configuration reference when you need every key and example.\nRun the Gateway by the book with the operational runbook.\nLearn how the Control UI/Web surfaces work and how to expose them safely.\nUnderstand remote access over SSH tunnels or tailnets.\nFollow the onboarding wizard flow for a guided setup.\nWire external triggers via the webhook surface.\nSet up Gmail Pub/Sub triggers.\nLearn the macOS menu bar companion details.\nPlatform guides: Windows (WSL2), Linux, macOS, iOS, Android\nDebug common failures with the troubleshooting guide.\nReview security guidance before exposing anything.\nAdvanced docs (discovery + control)\nDiscovery + transports\nBonjour/mDNS\nGateway pairing\nRemote gateway README\nControl UI\nDashboard\nOperations & troubleshooting\nHealth checks\nGateway lock\nBackground process\nBrowser troubleshooting (Linux)\nLogging\nDeep dives\nAgent loop\nPresence\nTypeBox schemas\nRPC adapters\nQueue\nWorkspace & skills\nSkills config\nDefault AGENTS\nTemplates: AGENTS\nTemplates: BOOTSTRAP\nTemplates: IDENTITY\nTemplates: SOUL\nTemplates: TOOLS\nTemplates: USER\nPlatform internals\nmacOS dev setup\nmacOS menu bar\nmacOS voice wake\niOS node\nAndroid node\nWindows (WSL2)\nLinux app\nEmail hooks (Gmail)\ndocs.clawd.bot/gmail-pubsub\nClawd\nClawdbot was built for Clawd, a space lobster AI assistant. 🦞\nby Peter Steinberger and the community.\nclawd.me\nsoul.md\nsteipete.me\nCommunity\nSee CONTRIBUTING.md for guidelines, maintainers, and how to submit PRs.\nAI/vibe-coded PRs welcome! 🤖\nSpecial thanks to Mario Zechner for his support and for\npi-mono.\nThanks to all clawtributors:",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-27T05:56:30.395180Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 366,
      "num_comments": 232,
      "engagement_score": 830.0
    },
    {
      "link": "https://www.theguardian.com/technology/2026/jan/26/georgia-datacenters-ai-ban",
      "author": "https://www.theguardian.com/profile/timothy-pratt",
      "title": "Georgia leads push to ban datacenters used to power America’s AI boom | AI (artificial intelligence) | The Guardian",
      "source": "hackernews",
      "content": "Lawmakers in several states are exploring passing laws that would put statewide bans in place on building new datacenters as the issue of the power-hungry facilities has moved to the center of economic and environmental concerns in the US.In Georgia a state lawmaker has introduced a bill proposing what could become the first statewide moratorium on new datacenters in America. The bill is one of at least three statewide moratoriums on datacenters introduced in state legislatures in the last week as Maryland and Oklahoma lawmakers are also considering similar measures.But it is Georgia that is quickly becoming ground zero in the fight against untrammelled growth of datacenters – which are notorious for using huge amounts of energy and water – as they power the emerging industry of artificial intelligence.The Georgia bill seeks to halt all such projects until March of next year “to allow state, county and municipal-level officials time to set necessary policies for regulating datacenters … which permanently alter the landscape of our state”, said bill sponsor state Democratic legislator Ruwa Romman.It comes at a time when Georgia’s public service commission – the agency that oversees utility company Georgia Power – just last month approved a plan to provide 10 additional gigawatts of energy in the coming years. It was the largest amount of electricity sought for a multi-year plan in the commission’s history, was driven by datacenters and will mostly be supplied by fossil fuels.The 10-gigawatt plan – enough to power about 8.3m homes – in turn comes as the Atlanta metro area led the nation in datacenter construction in 2024.This accelerated growth has already led at least 10 Georgia municipalities to pass their own moratoriums on datacenter construction, with Atlanta suburb Roswell becoming the most recent earlier this month. Municipalities in at least 14 states have done the same, according to Tech Policy Press.Bernie Sanders, the Vermont independent democratic socialist senator, proposed a national moratorium last month.“What we’re seeing is, as communities are learning more about this aggressive industry’s presence … [they] want to have time to thoroughly investigate all potential harms,” said Seth Gladstone, spokesperson for Food and Water Watch.The rampant development of datacenters to power AI raises several concerns for residents and activists alike. One is their impact on the cost of electricity. “In the public’s mind, datacenters and utility bills are inextricably linked,” said Charles Hua, founder and executive director of PowerLines, an organization that works on lowering utility bills and involving communities in decisions about energy.Hua noted that the relationship between the two varies, depending on each state’s market and regulatory system. In Georgia, he said, the Georgia Power utility company makes profit off new capital investments – so it has incentive to keep building new power plants. This approach has led Georgia’s rates to go up by a third in the last several years alone. Meanwhile, he said, the power company doesn’t have incentive to make the electrical grid more efficient – which “could actually lower prices”, Hua said.But datacenter concerns in Georgia also include water use and lost tax revenue. Republicans in the state legislature have introduced bills this year to protect consumers from increases in their utility bills and to end tax breaks for the centers. A Democrat has proposed that datacenters make public how much energy and water they use each year.Romman is also running for governor. If elected, she would become the first Palestinian American elected to statewide office in Georgia and break the near quarter-century hold Republicans have on the office.Her bill, HB 1012, has a Republican co-sponsor in state representative Jordan Ridley, who said he signed onto the measure because he wanted to give local governments time to develop zoning regulations on datacenters, since “it seems like they’re being built across the state”.“Every local government has zoning codes and … they need public input. That takes time,” Ridley said. At the same time, Ridley added, “datacenters … provide tax revenue and high-paying jobs. I’m not against datacenters.”Romman’s bill is not just a policy proposal; it’s also a political one. In a statement, she wrote that the moratorium “would provide time for Georgians to vote on the majority of the Public Service Commission seats who make final decisions on energy-related projects”.Georgia is one of 10 states that elect their utility regulators. Voters in the state elected progressive Democrats Alicia Johnson and Peter Hubbard to the five-member commission in November, leading the agency to lose its all-Republican makeup for the first time in nearly two decades. Another seat is up for a vote this November.The calculus: if the commission becomes majority-Democratic, it will no longer give a rubber stamp to electricity demands from Georgia Power driven by tech companies seeking to build datacenters.Hubbard, now in his new position, recently wrote an editorial asserting that Georgia voters “see data centers receiving tax breaks as their power bills go up. They see local communities struggle with competition for water supplies and high voltage transmission lines that reduce property values. And they see how the PSC approved every request placed before it by the monopoly electric utility.“This is why opposition to data centers is growing in Georgia; because Georgians oppose being treated as collateral damage by the unregulated growth of data centers that will push their power bills even higher.”There’s another political implication to Romman’s bill. Paul Glaze, spokesperson for Georgia Conservation Voters, said if the bill crosses from the House to the Senate, “it may be a preview of the potential general election” later this year.“The question is, in communities where datacenters are coming, who are voters going to trust to have their back?” Glaze said. “Anyone serious about statewide office should have a clear position on this.”",
      "publish_datetime": "2026-01-26T16:07:27.000Z",
      "scraping_timestamp": "2026-01-27T05:56:36.112139Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 49,
      "num_comments": 22,
      "engagement_score": 93.0
    }
  ]
}