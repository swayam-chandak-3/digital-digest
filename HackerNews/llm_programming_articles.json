{
  "generated_at": "2026-02-01T17:30:34.852944Z",
  "total_articles": 9,
  "articles": [
    {
      "link": "https://mariozechner.at/posts/2025-11-30-pi-coding-agent/",
      "author": null,
      "title": "What I learned building an opinionated and minimal coding agent",
      "source": "hackernews",
      "content": "What I learned building an opinionated and minimal coding agent\n2025-11-30\nIt's not much, but it's mine\nIn the past three years, I've been using LLMs for assisted coding. If you read this, you probably went through the same evolution: from copying and pasting code into ChatGPT, to Copilot auto-completions (which never worked for me), to Cursor, and finally the new breed of coding agent harnesses like Claude Code, Codex, Amp, Droid, and opencode that became our daily drivers in 2025.\nI preferred Claude Code for most of my work. It was the first thing I tried back in April after using Cursor for a year and a half. Back then, it was much more basic. That fit my workflow perfectly, because I'm a simple boy who likes simple, predictable tools. Over the past few months, Claude Code has turned into a spaceship with 80% of functionality I have no use for. The system prompt and tools also change on every release, which breaks my workflows and changes model behavior. I hate that. Also, it flickers.\nI've also built a bunch of agents over the years, of various complexity. For example, Sitegeist, my little browser-use agent, is essentially a coding agent that lives inside the browser. In all that work, I learned that context engineering is paramount. Exactly controlling what goes into the model's context yields better outputs, especially when it's writing code. Existing harnesses make this extremely hard or impossible by injecting stuff behind your back that isn't even surfaced in the UI.\nSpeaking of surfacing things, I want to inspect every aspect of my interactions with the model. Basically no harness allows that. I also want a cleanly documented session format I can post-process automatically, and a simple way to build alternative UIs on top of the agent core. While some of this is possible with existing harnesses, the APIs smell like organic evolution. These solutions accumulated baggage along the way, which shows in the developer experience. I'm not blaming anyone for this. If tons of people use your shit and you need some sort of backwards compatibility, that's the price you pay.\nI've also dabbled in self-hosting, both locally and on DataCrunch. While some harnesses like opencode support self-hosted models, it usually doesn't work well. Mostly because they rely on libraries like the Vercel AI SDK, which doesn't play nice with self-hosted models for some reason, specifically when it comes to tool calling.\nSo what's an old guy yelling at Claudes going to do? He's going to write his own coding agent harness and give it a name that's entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?\nTo make this work, I needed to build:\npi-ai: A unified LLM API with multi-provider support (Anthropic, OpenAI, Google, xAI, Groq, Cerebras, OpenRouter, and any OpenAI-compatible endpoint), streaming, tool calling with TypeBox schemas, thinking/reasoning support, seamless cross-provider context handoffs, and token and cost tracking.\npi-agent-core: An agent loop that handles tool execution, validation, and event streaming.\npi-tui: A minimal terminal UI framework with differential rendering, synchronized output for (almost) flicker-free updates, and components like editors with autocomplete and markdown rendering.\npi-coding-agent: The actual CLI that wires it all together with session management, custom tools, themes, and project context files.\nMy philosophy in all of this was: if I don't need it, it won't be built. And I don't need a lot of things.\npi-ai and pi-agent-core\nI'm not going to bore you with the API specifics of this package. You can read it all in the README.md. Instead, I want to document the problems I ran into while creating a unified LLM API and how I resolved them. I'm not claiming my solutions are the best, but they've been working pretty well throughout various agentic and non-agentic LLM projects.\nThere. Are. Four. Ligh... APIs\nThere's really only four APIs you need to speak to talk to pretty much any LLM provider: OpenAI's Completions API, their newer Responses API, Anthropic's Messages API, and Google's Generative AI API.\nThey're all pretty similar in features, so building an abstraction on top of them isn't rocket science. There are, of course, provider-specific peculiarities you have to care for. That's especially true for the Completions API, which is spoken by pretty much all providers, but each of them has a different understanding of what this API should do. For example, while OpenAI doesn't support reasoning traces in their Completions API, other providers do in their version of the Completions API. This is also true for inference engines like llama.cpp, Ollama, vLLM, and LM Studio.\nFor example, in openai-completions.ts:\nCerebras, xAI, Mistral, and Chutes don't like the store field\nMistral and Chutes use max_tokens instead of max_completion_tokens\nCerebras, xAI, Mistral, and Chutes don't support the developer role for system prompts\nGrok models don't like reasoning_effort\nDifferent providers return reasoning content in different fields (reasoning_content vs reasoning)\nTo ensure all features actually work across the gazillion of providers, pi-ai has a pretty extensive test suite covering image inputs, reasoning traces, tool calling, and other features you'd expect from an LLM API. Tests run across all supported providers and popular models. While this is a good effort, it still won't guarantee that new models and providers will just work out of the box.\nAnother big difference is how providers report tokens and cache reads/writes. Anthropic has the sanest approach, but generally it's the Wild West. Some report token counts at the start of the SSE stream, others only at the end, making accurate cost tracking impossible if a request is aborted. To add insult to injury, you can't provide a unique ID to later correlate with their billing APIs and figure out which of your users consumed how many tokens. So pi-ai does token and cache tracking on a best-effort basis. Good enough for personal use, but not for accurate billing if you have end users consuming tokens through your service.\nSpecial shout out to Google who to this date seem to not support tool call streaming which is extremely Google.\npi-ai also works in the browser, which is useful for building web-based interfaces. Some providers make this especially easy by supporting CORS, specifically Anthropic and xAI.\nContext handoff\nContext handoff between providers was a feature pi-ai was designed for from the start. Since each provider has their own way of tracking tool calls and thinking traces, this can only be a best-effort thing. For example, if you switch from Anthropic to OpenAI mid-session, Anthropic thinking traces are converted to content blocks inside assistant messages, delimited by <thinking></thinking> tags. This may or may not be sensible, because the thinking traces returned by Anthropic and OpenAI don't actually represent what's happening behind the scenes.\nThese providers also insert signed blobs into the event stream that you have to replay on subsequent requests containing the same messages. This also applies when switching models within a provider. It makes for a cumbersome abstraction and transformation pipeline in the background.\nI'm happy to report that cross-provider context handoff and context serialization/deserialization work pretty well in pi-ai:\nimport { getModel, complete, Context } from '@mariozechner/pi-ai';\nconst claude = getModel('anthropic', 'claude-sonnet-4-5');\nconst context: Context = {\nmessages: []\n};\ncontext.messages.push({ role: 'user', content: 'What is 25 * 18?' });\nconst claudeResponse = await complete(claude, context, {\nthinkingEnabled: true\n});\ncontext.messages.push(claudeResponse);\nconst gpt = getModel('openai', 'gpt-5.1-codex');\ncontext.messages.push({ role: 'user', content: 'Is that correct?' });\nconst gptResponse = await complete(gpt, context);\ncontext.messages.push(gptResponse);\nconst gemini = getModel('google', 'gemini-2.5-flash');\ncontext.messages.push({ role: 'user', content: 'What was the question?' });\nconst geminiResponse = await complete(gemini, context);\nconst serialized = JSON.stringify(context);\nconst restored: Context = JSON.parse(serialized);\nrestored.messages.push({ role: 'user', content: 'Summarize our conversation' });\nconst continuation = await complete(claude, restored);\nWe live in a multi-model world\nSpeaking of models, I wanted a typesafe way of specifying them in the getModel call. For that I needed a model registry that I could turn into TypeScript types. I'm parsing data from both OpenRouter and models.dev (created by the opencode folks, thanks for that, it's super useful) into models.generated.ts. This includes token costs and capabilities like image inputs and thinking support.\nAnd if I ever need to add a model that's not in the registry, I wanted a type system that makes it easy to create new ones. This is especially useful when working with self-hosted models, new releases that aren't yet on models.dev or OpenRouter, or trying out one of the more obscure LLM providers:\nimport { Model, stream } from '@mariozechner/pi-ai';\nconst ollamaModel: Model<'openai-completions'> = {\nid: 'llama-3.1-8b',\nname: 'Llama 3.1 8B (Ollama)',\napi: 'openai-completions',\nprovider: 'ollama',\nbaseUrl: 'http://localhost:11434/v1',\nreasoning: false,\ninput: ['text'],\ncost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },\ncontextWindow: 128000,\nmaxTokens: 32000\n};\nconst response = await stream(ollamaModel, context, {\napiKey: 'dummy'\n});\nMany unified LLM APIs completely ignore providing a way to abort requests. This is entirely unacceptable if you want to integrate your LLM into any kind of production system. Many unified LLM APIs also don't return partial results to you, which is kind of ridiculous. pi-ai was designed from the beginning to support aborts throughout the entire pipeline, including tool calls. Here's how it works:\nimport { getModel, stream } from '@mariozechner/pi-ai';\nconst model = getModel('openai', 'gpt-5.1-codex');\nconst controller = new AbortController();\nsetTimeout(() => controller.abort(), 2000);\nconst s = stream(model, {\nmessages: [{ role: 'user', content: 'Write a long story' }]\n}, {\nsignal: controller.signal\n});\nfor await (const event of s) {\nif (event.type === 'text_delta') {\nprocess.stdout.write(event.delta);\n} else if (event.type === 'error') {\nconsole.log(`${event.reason === 'aborted' ? 'Aborted' : 'Error'}:`, event.error.errorMessage);\n}\n}\nconst response = await s.result();\nif (response.stopReason === 'aborted') {\nconsole.log('Partial content:', response.content);\n}\nStructured split tool results\nAnother abstraction I haven't seen in any unified LLM API is splitting tool results into a portion handed to the LLM and a portion for UI display. The LLM portion is generally just text or JSON, which doesn't necessarily contain all the information you'd want to show in a UI. It also sucks hard to parse textual tool outputs and restructure them for display in a UI. pi-ai's tool implementation allows returning both content blocks for the LLM and separate content blocks for UI rendering. Tools can also return attachments like images that get attached in the native format of the respective provider. Tool arguments are automatically validated using TypeBox schemas and AJV, with detailed error messages when validation fails:\nimport { Type, AgentTool } from '@mariozechner/pi-ai';\nconst weatherSchema = Type.Object({\ncity: Type.String({ minLength: 1 }),\n});\nconst weatherTool: AgentTool<typeof weatherSchema, { temp: number }> = {\nname: 'get_weather',\ndescription: 'Get current weather for a city',\nparameters: weatherSchema,\nexecute: async (toolCallId, args) => {\nconst temp = Math.round(Math.random() * 30);\nreturn {\noutput: `Temperature in ${args.city}: ${temp}°C`,\ndetails: { temp }\n};\n}\n};\nconst chartTool: AgentTool = {\nname: 'generate_chart',\ndescription: 'Generate a chart from data',\nparameters: Type.Object({ data: Type.Array(Type.Number()) }),\nexecute: async (toolCallId, args) => {\nconst chartImage = await generateChartImage(args.data);\nreturn {\ncontent: [\n{ type: 'text', text: `Generated chart with ${args.data.length} data points` },\n{ type: 'image', data: chartImage.toString('base64'), mimeType: 'image/png' }\n]\n};\n}\n};\nWhat's still lacking is tool result streaming. Imagine a bash tool where you want to display ANSI sequences as they come in. That's currently not possible, but it's a simple fix that will eventually make it into the package.\nPartial JSON parsing during tool call streaming is essential for good UX. As the LLM streams tool call arguments, pi-ai progressively parses them so you can show partial results in the UI before the call completes. For example, you can display a diff streaming in as the agent rewrites a file.\nMinimal agent scaffold\nFinally, pi-ai provides an agent loop that handles the full orchestration: processing user messages, executing tool calls, feeding results back to the LLM, and repeating until the model produces a response without tool calls. The loop also supports message queuing via a callback: after each turn, it asks for queued messages and injects them before the next assistant response. The loop emits events for everything, making it easy to build reactive UIs.\nThe agent loop doesn't let you specify max steps or similar knobs you'd find in other unified LLM APIs. I never found a use case for that, so why add it? The loop just loops until the agent says it's done. On top of the loop, however, pi-agent-core provides an Agent class with actually useful stuff: state management, simplified event subscriptions, message queuing with two modes (one-at-a-time or all-at-once), attachment handling (images, documents), and a transport abstraction that lets you run the agent either directly or through a proxy.\nAm I happy with pi-ai? For the most part, yes. Like any unifying API, it can never be perfect due to leaky abstractions. But it's been used in seven different production projects and has served me extremely well.\nWhy build this instead of using the Vercel AI SDK? Armin's blog post mirrors my experience. Building on top of the provider SDKs directly gives me full control and lets me design the APIs exactly as I want, with a much smaller surface area. Armin's blog gives you a more in-depth treatise on the reasons for building your own. Go read that.\npi-tui\nI grew up in the DOS era, so terminal user interfaces are what I grew up with. From the fancy setup programs for Doom to Borland products, TUIs were with me until the end of the 90s. And boy was I fucking happy when I eventually switched to a GUI operating system. While TUIs are mostly portable and easily streamable, they also suck at information density. Having said all that, I thought starting with a terminal user interface for pi makes the most sense. I could strap on a GUI later whenever I felt like I needed to.\nSo why build my own TUI framework? I've looked into the alternatives like Ink, Blessed, OpenTUI, and so on. I'm sure they're all fine in their own way, but I definitely don't want to write my TUI like a React app. Blessed seems to be mostly unmaintained, and OpenTUI is explicitly not production ready. Also, writing my own TUI framework on top of Node.js seemed like a fun little challenge.\nTwo kinds of TUIs\nWriting a terminal user interface is not rocket science per se. You just have to pick your poison. There's basically two ways to do it. One is to take ownership of the terminal viewport (the portion of the terminal contents you can actually see) and treat it like a pixel buffer. Instead of pixels you have cells that contain characters with background color, foreground color, and styling like italic and bold. I call these full screen TUIs. Amp and opencode use this approach.\nThe drawback is that you lose the scrollback buffer, which means you have to implement custom search. You also lose scrolling, which means you have to simulate scrolling within the viewport yourself. While this is not hard to implement, it means you have to re-implement all the functionality your terminal emulator already provides. Mouse scrolling specifically always feels kind of off in such TUIs.\nThe second approach is to just write to the terminal like any CLI program, appending content to the scrollback buffer, only occasionally moving the \"rendering cursor\" back up a little within the visible viewport to redraw things like animated spinners or a text edit field. It's not exactly that simple, but you get the idea. This is what Claude Code, Codex, and Droid do.\nCoding agents have this nice property that they're basically a chat interface. The user writes a prompt, followed by replies from the agent and tool calls and their results. Everything is nicely linear, which lends itself well to working with the \"native\" terminal emulator. You get to use all the built-in functionality like natural scrolling and search within the scrollback buffer. It also limits what your TUI can do to some degree, which I find charming because constraints make for minimal programs that just do what they're supposed to do without superfluous fluff. This is the direction I picked for pi-tui.\nRetained mode UI\nIf you've done any GUI programming, you've probably heard of retained mode vs immediate mode. In a retained mode UI, you build up a tree of components that persist across frames. Each component knows how to render itself and can cache its output if nothing changed. In an immediate mode UI, you redraw everything from scratch each frame (though in practice, immediate mode UIs also do caching, otherwise they'd fall apart).\npi-tui uses a simple retained mode approach. A Component is just an object with a render(width) method that returns an array of strings (lines that fit the viewport horizontally, with ANSI escape codes for colors and styling) and an optional handleInput(data) method for keyboard input. A Container holds a list of components arranged vertically and collects all their rendered lines. The TUI class is itself a container that orchestrates everything.\nWhen the TUI needs to update the screen, it asks each component to render. Components can cache their output: an assistant message that's fully streamed doesn't need to re-parse markdown and re-render ANSI sequences every time. It just returns the cached lines. Containers collect lines from all children. The TUI gathers all these lines and compares them to the lines it previously rendered for the previous component tree. It keeps a backbuffer of sorts, remembering what was written to the scrollback buffer.\nThen it only redraws what changed, using a method I call differential rendering. I'm very bad with names, and this likely has an official name.\nDifferential rendering\nHere's a simplified demo that illustrates what exactly gets redrawn.\nThe algorithm is simple:\nFirst render: Just output all lines to the terminal\nWidth changed: Clear screen completely and re-render everything (soft wrapping changes)\nNormal update: Find the first line that differs from what's on screen, move the cursor to that line, and re-render from there to the end\nThere's one catch: if the first changed line is above the visible viewport (the user scrolled up), we have to do a full clear and re-render. The terminal doesn't let you write to the scrollback buffer above the viewport.\nTo prevent flicker during updates, pi-tui wraps all rendering in synchronized output escape sequences (CSI ?2026h and CSI ?2026l). This tells the terminal to buffer all the output and display it atomically. Most modern terminals support this.\nHow well does it work and how much does it flicker? In any capable terminal like Ghostty or iTerm2, this works brilliantly and you never see any flicker. In less fortunate terminal implementations like VS Code's built-in terminal, you will get some flicker depending on the time of day, your display size, your window size, and so on. Given that I'm very accustomed to Claude Code, I haven't spent any more time optimizing this. I'm happy with the little flicker I get in VS Code. I wouldn't feel at home otherwise. And it still flickers less than Claude Code.\nHow wasteful is this approach? We store an entire scrollback buffer worth of previously rendered lines, and we re-render lines every time the TUI is asked to render itself. That's alleviated with the caching I described above, so the re-rendering isn't a big deal. We still have to compare a lot of lines with each other. Realistically, on computers younger than 25 years, this is not a big deal, both in terms of performance and memory use (a few hundred kilobytes for very large sessions). Thanks V8. What I get in return is a dead simple programming model that lets me iterate quickly.\npi-coding-agent\nI don't need to explain what features you should expect from a coding agent harness. pi comes with most creature comforts you're used to from other tools:\nRuns on Windows, Linux, and macOS (or anything with a Node.js runtime and a terminal)\nMulti-provider support with mid-session model switching\nSession management with continue, resume, and branching\nProject context files (AGENTS.md) loaded hierarchically from global to project-specific\nSlash commands for common operations\nCustom slash commands as markdown templates with argument support\nOAuth authentication for Claude Pro/Max subscriptions\nCustom model and provider configuration via JSON\nCustomizable themes with live reload\nEditor with fuzzy file search, path completion, drag & drop, and multi-line paste\nMessage queuing while the agent is working\nImage support for vision-capable models\nHTML export of sessions\nHeadless operation via JSON streaming and RPC mode\nFull cost and token tracking\nIf you want the full rundown, read the README. What's more interesting is where pi deviates from other harnesses in philosophy and implementation.\nMinimal system prompt\nHere's the system prompt:\nYou are an expert coding assistant. You help users with coding tasks by reading files, executing commands, editing code, and writing new files.\nAvailable tools:\n- read: Read file contents\n- bash: Execute bash commands\n- edit: Make surgical edits to files\n- write: Create or overwrite files\nGuidelines:\n- Use bash for file operations like ls, grep, find\n- Use read to examine files before editing\n- Use edit for precise changes (old text must match exactly)\n- Use write only for new files or complete rewrites\n- When summarizing your actions, output plain text directly - do NOT use cat or bash to display what you did\n- Be concise in your responses\n- Show file paths clearly when working with files\nDocumentation:\n- Your own documentation (including custom model setup and theme creation) is at: /path/to/README.md\n- Read it when users ask about features, configuration, or setup, and especially if the user asks you to add a custom model or provider, or create a custom theme.\nThat's it. The only thing that gets injected at the bottom is your AGENTS.md file. Both the global one that applies to all your sessions and the project-specific one stored in your project directory. This is where you can customize pi to your liking. You can even replace the full system prompt if you want to. Compared to, for example, Claude Code's system prompt, Codex's system prompt, or opencode's model-specific prompts (the Claude one is a cut-down version of the original Claude Code prompt they copied).\nYou might think this is crazy. In all likelihood, the models have some training on their native coding harness. So using the native system prompt or something close to it like opencode would be most ideal. But it turns out that all the frontier models have been RL-trained up the wazoo, so they inherently understand what a coding agent is. There does not appear to be a need for 10,000 tokens of system prompt, as we'll find out later in the benchmark section, and as I've anecdotally found out by exclusively using pi for the past few weeks. Amp, while copying some parts of the native system prompts, seems to also do just fine with their own prompt.\nMinimal toolset\nHere are the tool definitions:\nread\nRead the contents of a file. Supports text files and images (jpg, png,\ngif, webp). Images are sent as attachments. For text files, defaults to\nfirst 2000 lines. Use offset/limit for large files.\n- path: Path to the file to read (relative or absolute)\n- offset: Line number to start reading from (1-indexed)\n- limit: Maximum number of lines to read\nwrite\nWrite content to a file. Creates the file if it doesn't exist, overwrites\nif it does. Automatically creates parent directories.\n- path: Path to the file to write (relative or absolute)\n- content: Content to write to the file\nedit\nEdit a file by replacing exact text. The oldText must match exactly\n(including whitespace). Use this for precise, surgical edits.\n- path: Path to the file to edit (relative or absolute)\n- oldText: Exact text to find and replace (must match exactly)\n- newText: New text to replace the old text with\nbash\nExecute a bash command in the current working directory. Returns stdout\nand stderr. Optionally provide a timeout in seconds.\n- command: Bash command to execute\n- timeout: Timeout in seconds (optional, no default timeout)\nThere are additional read-only tools (grep, find, ls) if you want to restrict the agent from modifying files or running arbitrary commands. By default these are disabled, so the agent only gets the four tools above.\nAs it turns out, these four tools are all you need for an effective coding agent. Models know how to use bash and have been trained on the read, write, and edit tools with similar input schemas. Compare this to Claude Code's tool definitions or opencode's tool definitions (which are clearly derived from Claude Code's, same structure, same examples, same git commit flow). Notably, Codex's tool definitions are similarly minimal to pi's.\npi's system prompt and tool definitions together come in below 1000 tokens.\nYOLO by default\npi runs in full YOLO mode and assumes you know what you're doing. It has unrestricted access to your filesystem and can execute any command without permission checks or safety rails. No permission prompts for file operations or commands. No pre-checking of bash commands by Haiku for malicious content. Full filesystem access. Can execute any command with your user privileges.\nIf you look at the security measures in other coding agents, they're mostly security theater. As soon as your agent can write code and run code, it's pretty much game over. The only way you could prevent exfiltration of data would be to cut off all network access for the execution environment the agent runs in, which makes the agent mostly useless. An alternative is allow-listing domains, but this can also be worked around through other means.\nSimon Willison has written extensively about this problem. His \"dual LLM\" pattern attempts to address confused deputy attacks and data exfiltration, but even he admits \"this solution is pretty bad\" and introduces enormous implementation complexity. The core issue remains: if an LLM has access to tools that can read private data and make network requests, you're playing whack-a-mole with attack vectors.\nSince we cannot solve this trifecta of capabilities (read data, execute code, network access), pi just gives in. Everybody is running in YOLO mode anyways to get any productive work done, so why not make it the default and only option?\nBy default, pi has no web search or fetch tool. However, it can use curl or read files from disk, both of which provide ample surface area for prompt injection attacks. Malicious content in files or command outputs can influence behavior. If you're uncomfortable with full access, run pi inside a container or use a different tool if you need (faux) guardrails.\nNo built-in to-dos\npi does not and will not support built-in to-dos. In my experience, to-do lists generally confuse models more than they help. They add state that the model has to track and update, which introduces more opportunities for things to go wrong.\nIf you need task tracking, make it externally stateful by writing to a file:\n# TODO.md\n- [x] Implement user authentication\n- [x] Add database migrations\n- [ ] Write API documentation\n- [ ] Add rate limiting\nThe agent can read and update this file as needed. Using checkboxes keeps track of what's done and what remains. Simple, visible, and under your control.\nNo plan mode\npi does not and will not have a built-in plan mode. Telling the agent to think through a problem together with you, without modifying files or executing commands, is generally sufficient.\nIf you need persistent planning across sessions, write it to a file:\n# PLAN.md\n## Goal\nRefactor authentication system to support OAuth\n## Approach\n1. Research OAuth 2.0 flows\n2. Design token storage schema\n3. Implement authorization server endpoints\n4. Update client-side login flow\n5. Add tests\n## Current Step\nWorking on step 3 - authorization endpoints\nThe agent can read, update, and reference the plan as it works. Unlike ephemeral planning modes that only exist within a session, file-based plans can be shared across sessions, and can be versioned with your code.\nFunnily enough, Claude Code now has a Plan Mode that's essentially read-only analysis, and it will eventually write a markdown file to disk. And you can basically not use plan mode without approving a shit ton of command invocations, because without that, planning is basically impossible.\nThe difference with pi is that I have full observability of everything. I get to see which sources the agent actually looked at and which ones it totally missed. In Claude Code, the orchestrating Claude instance usually spawns a sub-agent and you have zero visibility into what that sub-agent does. I get to see the markdown file immediately. I can edit it collaboratively with the agent. In short, I need observability for planning and I don't get that with Claude Code's plan mode.\nIf you must restrict the agent during planning, you can specify which tools it has access to via the CLI:\npi --tools read,grep,find,ls\nThis gives you read-only mode for exploration and planning without the agent modifying anything or being able to run bash commands. You won't be happy with that though.\nNo MCP support\npi does not and will not support MCP. I've written about this extensively, but the TL;DR is: MCP servers are overkill for most use cases, and they come with significant context overhead.\nPopular MCP servers like Playwright MCP (21 tools, 13.7k tokens) or Chrome DevTools MCP (26 tools, 18k tokens) dump their entire tool descriptions into your context on every session. That's 7-9% of your context window gone before you even start working. Many of these tools you'll never use in a given session.\nThe alternative is simple: build CLI tools with README files. The agent reads the README when it needs the tool, pays the token cost only when necessary (progressive disclosure), and can use bash to invoke the tool. This approach is composable (pipe outputs, chain commands), easy to extend (just add another script), and token-efficient.\nHere's how I add web search to pi:\nI maintain a collection of these tools at github.com/badlogic/agent-tools. Each tool is a simple CLI with a README that the agent reads on demand.\nIf you absolutely must use MCP servers, look into Peter Steinberger's mcporter tool that wraps MCP servers as CLI tools.\nNo background bash\npi's bash tool runs commands synchronously. There's no built-in way to start a dev server, run tests in the background, or interact with a REPL while the command is still running.\nThis is intentional. Background process management adds complexity: you need process tracking, output buffering, cleanup on exit, and ways to send input to running processes. Claude Code handles some of this with their background bash feature, but it has poor observability (a common theme with Claude Code) and forces the agent to track running instances without providing a tool to query them. In earlier Claude Code versions, the agent forgot about all its background processes after context compaction and had no way to query them, so you had to manually kill them. This has since been fixed.\nUse tmux instead. Here's pi debugging a crashing C program in LLDB:\nHow's that for observability? The same approach works for long-running dev servers, watching log output, and similar use cases. And if you wanted to, you could hop into that LLDB session above via tmux and co-debug with the agent. Tmux also gives you a CLI argument to list all active sessions. How nice.\nThere's simply no need for background bash. Claude Code can use tmux too, you know. Bash is all you need.\nNo sub-agents\npi does not have a dedicated sub-agent tool. When Claude Code needs to do something complex, it often spawns a sub-agent to handle part of the task. You have zero visibility into what that sub-agent does. It's a black box within a black box. Context transfer between agents is also poor. The orchestrating agent decides what initial context to pass to the sub-agent, and you generally have little control over that. If the sub-agent makes a mistake, debugging is painful because you can't see the full conversation.\nIf you need pi to spawn itself, just ask it to run itself via bash. You could even have it spawn itself inside a tmux session for full observability and the ability to interact with that sub-agent directly.\nBut more importantly: fix your workflow, at least the ones that are all about context gathering. People use sub-agents within a session thinking they're saving context space, which is true. But that's the wrong way to think about sub-agents. Using a sub-agent mid-session for context gathering is a sign you didn't plan ahead. If you need to gather context, do that first in its own session. Create an artifact that you can later use in a fresh session to give your agent all the context it needs without polluting its context window with tool outputs. That artifact can be useful for the next feature too, and you get full observability and steerability, which is important during context gathering.\nBecause despite popular belief, models are still poor at finding all the context needed for implementing a new feature or fixing a bug. I attribute this to models being trained to only read parts of files rather than full files, so they're hesitant to read everything. Which means they miss important context and can't see what they need to properly complete the task.\nJust look at the pi-mono issue tracker and the pull requests. Many get closed or revised because the agents couldn't fully grasp what's needed. That's not the fault of the contributors, which I truly appreciate because even incomplete PRs help me move faster. It just means we trust our agents too much.\nI'm not dismissing sub-agents entirely. There are valid use cases. My most common one is code review: I tell pi to spawn itself with a code review prompt (via a custom slash command) and it gets the outputs.\n---\ndescription: Run a code review sub-agent\n---\nSpawn yourself as a sub-agent via bash to do a code review: $@\nUse `pi --print` with appropriate arguments. If the user specifies a model,\nuse `--provider` and `--model` accordingly.\nPass a prompt to the sub-agent asking it to review the code for:\n- Bugs and logic errors\n- Security issues\n- Error handling gaps\nDo not read the code yourself. Let the sub-agent do that.\nReport the sub-agent's findings.\nAnd here's how I use this to review a pull request on GitHub:\nWith a simple prompt, I can select what specific thing I want to review and what model to use. I could even set thinking levels if I wanted to. I can also save out the full review session to a file and hop into that in another pi session if I wanted. Or I can say this is an ephemeral session and it shouldn't be saved to disk. All of that gets translated into a prompt that the main agent reads and based on which it executes itself again via bash. And while I don't get full observability into the inner workings of the sub-agent, I get full observability on its output. Something other harnesses don't really provide, which makes no sense to me.\nOf course, this is a bit of a simulated use case. In reality, I would just spawn a new pi session and ask it to review the pull request, possibly pull it into a branch locally. After I see its initial review, I give my own review and then we work on it together until it's good. That's the workflow I use to not merge garbage code.\nSpawning multiple sub-agents to implement various features in parallel is an anti-pattern in my book and doesn't work, unless you don't care if your codebase devolves into a pile of garbage.\nBenchmarks\nI make a lot of grandiose claims, but do I have numerical proof that all the contrarian things I say above actually work? I have my lived experience, but that's hard to transport in a blog post and you'd just have to believe me. So I created a Terminal-Bench 2.0 test run for pi with Claude Opus 4.5 and let it compete against Codex, Cursor, Windsurf, and other coding harnesses with their respective native models. Obviously, we all know benchmarks aren't representative of real-world performance, but it's the best I can provide you as a sort of proof that not everything I say is complete bullshit.\nI performed a complete run with five trials per task, which makes the results eligible for submission to the leaderboard. I also started a second run that only runs during CET because I found that error rates (and consequently benchmark results) get worse once PST goes online. Here are the results for the first run:\nAnd here's pi's placement on the current leaderboard as of December 2nd, 2025:\nAnd here's the results.json file I've submitted to the Terminal-Bench folks for inclusion in the leaderboard. The bench runner for pi can be found in this repository if you want to reproduce the results. I suggest you use your Claude plan instead of pay-as-you-go.\nFinally, here's a little glimpse into the CET-only run:\nThis is going to take another day or so to complete. I will update this blog post once that is done.\nAlso note the ranking of Terminus 2 on the leaderboard. Terminus 2 is the Terminal-Bench team's own minimal agent that just gives the model a tmux session. The model sends commands as text to tmux and parses the terminal output itself. No fancy tools, no file operations, just raw terminal interaction. And it's holding its own against agents with far more sophisticated tooling and works with a diverse set of models. More evidence that a minimal approach can do just as well.\nIn summary\nBenchmark results are hilarious, but the real proof is in the pudding. And my pudding is my day-to-day work, where pi has been performing admirably. Twitter is full of context engineering posts and blogs, but I feel like none of the harnesses we currently have actually let you do context engineering. pi is my attempt to build myself a tool where I'm in control as much as possible.\nI'm pretty happy with where pi is. There are a few more features I'd like to add, like compaction or tool result streaming, but I don't think there's much more I'll personally need. Missing compaction hasn't been a problem for me personally. For some reason, I'm able to cram hundreds of exchanges between me and the agent into a single session, which I couldn't do with Claude Code without compaction.\nThat said, I welcome contributions. But as with all my open source projects, I tend to be dictatorial. A lesson I've learned the hard way over the years with my bigger projects. If I close an issue or PR you've sent in, I hope there are no hard feelings. I will also do my best to give you reasons why. I just want to keep this focused and maintainable. If pi doesn't fit your needs, I implore you to fork it. I truly mean it. And if you create something that even better fits my needs, I'll happily join your efforts.\nI think some of the learnings above transfer to other harnesses as well. Let me know how that goes for you.\nThis page respects your privacy by not using cookies or similar technologies and by not collecting any personally identifiable information.",
      "publish_datetime": "2026-02-01T10:26:58.277166Z",
      "scraping_timestamp": "2026-02-01T17:26:58.307549Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 219,
      "num_comments": 91,
      "engagement_score": 401.0
    },
    {
      "link": "https://yaledailynews.com/articles/english-professors-double-down-on-requiring-printed-copies-of-readings",
      "author": null,
      "title": "English professors double down on requiring printed copies of readings | Yale Daily News",
      "source": "hackernews",
      "content": "This academic year, some English professors have increased their preference for physical copies of readings, citing concerns related to artificial intelligence.Many English professors have identified the use of chatbots as harmful to critical thinking and writing. Now, professors who had previously allowed screens in class are tightening technology restrictions.Professor Kim Shirkhani, who teaches “Reading and Writing the Modern Essay,” explained that for about a decade prior to this semester, she did not require printed readings. This semester, she is requiring all students to have printed options.“Over the years I’ve found that when students read on paper they're more likely to read carefully, and less likely in a pinch to read on their phones or rely on chatbot summaries,” Shirkhani wrote to the News. “This improves the quality of class time by orders of magnitude.”As the course director for “Reading and Writing the Modern Essay,” Shirkhani leaves the decision of allowing technology in the classroom up to each individual instructor. Yet others have followed her practice.Last semester, professor Pamela Newton, who also teaches the course, allowed students to bring readings either on tablets or in printed form. While laptops felt like a “wall” in class, Newton said, students could use iPads to annotate readings and lie them flat on the table during discussions. However, Newton said she felt “paranoid” that students could be texting during class.This semester, Newton has removed the option to bring iPads to class, except for accessibility needs, as a part of the general movement in the “Reading and Writing the Modern Essay” seminars to “swim against the tide of AI use,” reduce “the infiltration of tech,” and “go back to pen and paper,” she said.Regarding the printing cost, Newton and Shirkhani both emphasized that Yale has programs to help students who need financial assistance paying for printing.“I totally get that cost and the burden of that cost,” Newton said in an interview. “I kind of feel like there's going to be a book in most classes that you have to buy, and the course package just sort of replaces a physics textbook.”Spring semester courses offered a total of 34 TYCO packets this year, up from 20 at the same point last spring, according to archived versions of the TYCO Student Course Packet website. Fall semester courses increased from 30 packets in 2024 to 35 last semester.TYCO Print is a printing service where professors can upload course files for TYCO to print out for students as they order. Shorter packets can cost around $20, while longer packets can cost upwards of $150 when ordered with the cheapest binding option.Other English professors are maintaining preexisting no-technology policies.Professor Nancy Yousef, continuing from her approach at previous schools, has kept a requirement for printed readings.“The English classroom is increasingly a kind of special place where it’s still possible to converse without the screen,” Yousef said in a phone interview. “AI only seems to make it more imperative to make sure that students are having a direct experience with the text.”Yousef explained that literature courses are a “practice of attention and a practice of learning how to ask a good question.” Yousef said she hopes students come away from class with greater questions and increased engagement with the texts rather than “a set of bullet points that can go on a PowerPoint.”Writing professor Anne Fadiman wrote to the News that she asks students either to buy the course packet or purchase physical copies of the books.“When you read a book or a printed course packet, you turn real pages instead of scrolling, so you have a different, more direct, and (I think) more focused relationship with the words,” Fadiman wrote.Professors who continue to allow technology in their classroom cite printing costs and concerns about paper usage.Professor Stephanie Kelley does not require students to bring printed readings and allows technology “for accessibility, cost-related and environmental reasons.” While she has noticed students being distracted during class, such as by online shopping, she wrote to the News that “it can be a lot of paper, most of it going straight in the bin once class is done.”Kelley wrote that she wonders why the discussion of course material costs “more often falls on humanities classes rather than those with required textbooks that are often prohibitively expensive to rent or purchase.”In the fall, Yale College Council Senators Siena Valdivia ’28, Alex Chen ’28 and Alexander Medel ’27 — who is a staff writer for the News — sponsored a $3,500 stipend prioritizing first-generation, low-income students to receive financial aid for printing costs. Medel and Senator Aaron Lin ’28 also sponsored a $6,000 stipend to “alleviate the cost of course materials and textbooks for Yale College students.” These stipends come from the YCC budget.“In an ideal world, printing would be subsumed into the fiscal responsibilities of the university. But under further priority reconfiguring in light of the endowment tax, any such changes face an uphill climb,” Chen wrote to the News, referring to the upcoming increase in the federal tax on Yale’s investment returns, which was enacted as part of President Donald Trump’s One Big Beautiful Bill Act last year.For Yale students, printing one double-sided black-and-white page on a University printer costs 12 cents.",
      "publish_datetime": "2026-02-01T16:27:15.378100Z",
      "scraping_timestamp": "2026-02-01T17:27:15.392249Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 49,
      "num_comments": 54,
      "engagement_score": 157.0
    },
    {
      "link": "https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/",
      "author": "Name*",
      "title": "Generative AI and Wikipedia editing: What we learned in 2025 - Wiki Education",
      "source": "hackernews",
      "content": "Like many organizations, Wiki Education has grappled with generative AI, its impacts, opportunities, and threats, for several years. As an organization that runs large-scale programs to bring new editors to Wikipedia (we’re responsible for about 19% of all new active editors on English Wikipedia), we have deep understanding of what challenges face new content contributors to Wikipedia — and how to support them to successfully edit. As many people have begun using generative AI chatbots like ChatGPT, Gemini, or Claude in their daily lives, it’s unsurprising that people will also consider using them to help draft contributions to Wikipedia. Since Wiki Education’s programs provide a cohort of content contributors whose work we can evaluate, we’ve looked into how our participants are using GenAI tools.\nWe are choosing to share our perspective through this blog post because we hope it will help inform discussions of GenAI-created content on Wikipedia. In an open environment like the Wikimedia movement, it’s important to share what you’ve learned. In this case, we believe our learnings can help Wikipedia editors who are trying to protect the integrity of content on the encyclopedia, Wikipedians who may be interested in using generative AI tools themselves, other program leaders globally who are trying to onboard new contributors who may be interested in using these tools, and the Wikimedia Foundation, whose product and technology team builds software to help support the development of high-quality content on Wikipedia.\nOur fundamental conclusion about generative AI is: Wikipedia editors should never copy and paste the output from generative AI chatbots like ChatGPT into Wikipedia articles.\nLet me explain more.\nAI detection and investigation\nSince the launch of ChatGPT in November 2022, we’ve been paying close attention to GenAI-created content, and how it relates to Wikipedia. We’ve spot-checked work of new editors from our programs, primarily focusing on citations to ensure they were real and not hallucinated. We experimented with tools ourselves, we led video sessions about GenAI for our program participants, and we closely tracked on-wiki policy discussions around GenAI. Currently, English Wikipedia prohibits the use of generative AI to create images or in talk page discussions, and recently adopted a guideline against using large language models to generate new articles.\nAs our Wiki Experts Brianda Felix and Ian Ramjohn worked with program participants throughout the first half of 2025, they found more and more text bearing the hallmarks of generative AI in article content, like bolded words or bulleted lists in odd places. But the use of generative AI wasn’t necessarily problematic, as long as the content was accurate. Wikipedia’s open editing process encourages stylistic revisions to factual text to better fit Wikipedia’s style.\nBut was the text factually accurate? This fundamental question led our Chief Technology Officer, Sage Ross, to investigate different generative AI detectors. He landed on a tool called Pangram, which we have found to be highly accurate for Wikipedia text. Sage generated a list of all the new articles created through our work since 2022, and ran them all through Pangram. A total of 178 out of the 3,078 articles came back as flagged for AI — none before the launch of ChatGPT in late 2022, with increasing percentages term over term since then. About half of our staff spent a month during summer 2025 painstakingly reviewing the text from these 178 articles.\nPangram’s detection results showed no signs of AI usage before the launch of ChatGPT, and then a steady rise in usage in the terms following. Courtesy of Manoel Horta Ribeiro and Francesco Salvi.\nBased on the discourse around AI hallucinations, we were expecting these articles to contain citations to sources that didn’t exist, but this wasn’t true: only 7% of the articles had fake sources. The rest had information cited to real, relevant sources.\nFar more insidious, however, was something else we discovered: More than two-thirds of these articles failed verification. That means the article contained a plausible-sounding sentence, cited to a real, relevant-sounding source. But when you read the source it’s cited to, the information on Wikipedia does not exist in that specific source. When a claim fails verification, it’s impossible to tell whether the information is true or not. For most of the articles Pangram flagged as written by GenAI, nearly every cited sentence in the article failed verification.\nThis finding led us to invest significant staff time into cleaning up these articles — far more than these editors had likely spent creating them. Wiki Education’s core mission is to improve Wikipedia, and when we discover our program has unknowingly contributed to misinformation on Wikipedia, we are committed to cleaning it up. In the clean-up process, Wiki Education staff moved more recent work back to sandboxes, we stub-ified articles that passed notability but mostly failed verification, and we PRODed some articles that from our judgment weren’t salvageable. All these are ways of addressing Wikipedia articles with flaws in their content. (While there are many grumblings about Wikipedia’s deletion processes, we found several of the articles we PRODed due to their fully hallucinated GenAI content were then de-PRODed by other editors, showing the diversity of opinion about generative AI among the Wikipedia community.\nRevising our guidance\nGiven what we found through our investigation into the work from prior terms, and given the increasing usage of generative AI, we wanted to proactively address generative AI usage within our programs. Thanks to in-kind support from our friends at Pangram, we began running our participants’ Wikipedia edits, including in their sandboxes, through Pangram nearly in real time. This is possible because of the Dashboard course management platform Sage built, which tracks edits and generates tickets for our Wiki Experts based on on-wiki edits.\nWe created a brand-new training module on Using generative AI tools with Wikipedia. This training emphasizes where participants could use generative AI tools in their work, and where they should not. The core message of these trainings is, do not copy and paste anything from a GenAI chatbot into Wikipedia.\nWe crafted a variety of automated emails to participants who Pangram detected were adding text created by generative AI chatbots. Sage also recorded some videos, since many young people are accustomed to learning via video rather than reading text. We also provided opportunities for engagement and conversation with program participants.\nOur findings from the second half of 2025\nIn total, we had 1,406 AI edit alerts in the second half of 2025, although only 314 of these (or 22%) were in the article namespace on Wikipedia (meaning edits to live articles). In most cases, Pangram detected participants using GenAI in their sandboxes during early exercises, when we ask them to do things like choose an article, evaluate an article, create a bibliography, and outline their contribution.\nThis graph shows the daily total of Pangram’s detected generative AI text our participants added to Wikipedia. Early in the term, the hits were primarily to exercises, with more sandbox and mainspace alerts later in the term. CC BY-SA 4.0 — Wiki Education.\nPangram struggled with false positives in a few sandbox scenarios:\nBibliographies, which are often a combination of human-written prose (describing a source and its relevance) and non-prose text (the citation for a source, in some standard format)\nOutlines with a high portion of non-prose content (such as bullet lists, section headers, text fragments, and so on)\nWe also had a handful of cases where sandboxes were flagged for AI after a participant copied an AI-written section from an existing article to use as a starting point to edit or to expand. (This isn’t a flaw of Pangram, but a reminder of how much AI-generated content editors outside our programs are adding to Wikipedia!)\nIn broad strokes, we found that Pangram is great at analyzing plain prose — the kind of sentences and paragraphs you’ll find in the body of a Wikipedia article — but sometimes it gets tripped up by formatting, markup, and non-prose text. Early on, we disabled alert emails for participants’ bibliography and outline exercises, and throughout the end of 2025, we refined the Dashboard’s preprocessing steps to extract the prose portions of revisions and convert them to plain text before sending them to Pangram.\nMany participants also reported “just using Grammarly to copy edit.” In our experience, however, the smallest fixes done with Grammarly never trigger Pangram’s detection, but if you use its more advanced content creation features, the resulting text registers as being AI generated.\nBut overwhelmingly, we were pleased with Pangram’s results. Our early interventions with participants who were flagged as using generative AI for exercises that would not enter mainspace seemed to head off their future use of generative AI. We supported 6,357 new editors in fall 2025, and only 217 of them (or 3%) had multiple AI alerts. Only 5% of the participants we supported had mainspace AI alerts. That means thousands of participants successfully edited Wikipedia without using generative AI to draft their content.\nFor those who did add GenAI-drafted text, we ensured that the content was reverted. In fact, participants sometimes self-reverted once they received our email letting them know Pangram had detected their contributions as being AI created. Instructors also jumped in to revert, as did some Wikipedians who found the content on their own. Our ticketing system also alerted our Wiki Expert staff, who reverted the text as soon as they could.\nWhile some instructors in our Wikipedia Student Program had concerns about AI detection, we had a lot of success focusing the conversation on the concept of verifiability. If the instructor as subject matter expert could attest the information was accurate, and they could find the specific facts in the sources they were cited to, we permitted text to come back to Wikipedia. However, the process of attempting to verify student-created work (which in many cases the students swore they’d written themselves) led many instructors to realize what we had found in our own assessment: In their current states, GenAI-powered chatbots cannot write factually accurate text for Wikipedia that is verifiable.\nWe believe our Pangram-based detection interventions led to fewer participants adding GenAI-created content to Wikipedia. Following the trend lines, we anticipated about 25% of participants to add GenAI content to Wikipedia articles; instead, it was only 5%, and our staff were able to revert all problematic content.\nI’m deeply appreciative of everyone who made this success possible this term: Participants who followed our recommendations, Pangram who gave us access to their detection service, Wiki Education staff who did the heavy lift of working with all of the positive detections, and the Wikipedia community, some of whom got to the problematic work from our program participants before we did.\nHow can generative AI help?\nSo far, I’ve focused on the problems with generative AI-created content. But that’s not all these tools can do, and we did find some ways they were useful. Our training module encourages editors — if their institution’s policies permit it — to consider using generative AI tools for:\nIdentifying gaps in articles\nFinding access to sources\nFinding relevant sources\nTo evaluate the success of these use scenarios, we worked directly with 7 of the classes we supported in fall 2025 in our Wikipedia Student Program. We asked students to anonymously fill out a survey every time they used generative AI tools in their Wikipedia work. We asked what tool they used, what prompt they used, how they used the output, and whether they found it helpful. While some students filled the survey out multiple times, others filled it out once. We had 102 responses reporting usage at various stages in the project. Overwhelmingly, 87% of the responses who reported using generative AI said it was helpful for them in the task. The most popular tool by far was ChatGPT, with Grammarly as a distant second, and the others in the single-digits of usage.\nStudents reported AI tools very helpful in:\nIdentifying articles to work on that were relevant to the course they were taking\nHighlighting gaps within existing articles, including missing sections or more recent information that was missing\nFinding reliable sources that they hadn’t already located\nPointing to which database a certain journal article could be found\nWhen prompted with the text they had drafted and the checklist of requirements, evaluating the draft against those requirements\nIdentifying categories they could add to the article they’d edited\nCorrecting grammar and spelling mistakes\nCritically, no participants reported using AI tools to draft text for their assignments. One student reported: “I pasted all of my writing from my sandbox and said ‘Put this in a casual, less academic tone’ … I figured I’d try this but it didn’t sound like what I normally write and I didn’t feel that it captured what I was trying to get across so I scrapped it.”\nWhile this was an informal research project, we received enough positive feedback from it to believe using ChatGPT and other tools can be helpful in the research stage if editors then critically evaluate the output they get, instead of blindly accepting it. Even participants who found AI helpful reported that they didn’t use everything it gave them, as some was irrelevant. Undoubtedly, it’s crucial to maintain the human thinking component throughout the process.\nWhat does this all mean for Wiki Education?\nMy conclusion is that, at least as of now, generative AI-powered chatbots like ChatGPT should never be used to generate text for Wikipedia; too much of it will simply be unverifiable. Our staff would spend far more time attempting to verify facts in AI-generated articles than if we’d simply done the research and writing ourselves.\nThat being said, AI tools can be helpful in the research process, especially to help identify content gaps or sources, when used in conjunction with a human brain that carefully evaluates the information. Editors should never simply take a chatbot’s suggestion; instead, if they want to use a chatbot, they should use it as a brainstorm partner to help them think through their plans for an article.\nTo date, Wiki Education’s interventions as our program participants edit Wikipedia show promise for keeping unverifiable, GenAI-drafted content off Wikipedia. Based on our experiences in the fall term, we have high confidence in Pangram as a detector of AI content, at least in Wikipedia articles. We will continue our current strategy in 2026 (with more small adjustments to make the system as reliable as we can).\nMore generally, we found participants had less AI literacy than popular discourse might suggest. Because of this, we created a supplemental large language models training that we’ve offered as an optional module for all participants. Many participants indicated that they found our guidance regarding AI to be welcome and helpful as they attempt to navigate the new complexities created by AI tools.\nWe are also looking forward to more research on our work. A team of researchers — Francesco Salvi and Manoel Horta Ribeiro at Princeton University, Robert Cummings at the University of Mississippi, and Wiki Education’s Sage Ross — have been looking into Wiki Education’s Wikipedia Student Program editors’ use of generative AI over time. Preliminary results have backed up our anecdotal understanding, while also revealing nuances of how text produced by our students over time has changed with the introduction of GenAI chatbots. They also confirmed our belief in Pangram: After running student edits from 2015 up until the launch of ChatGPT through Pangram, without any date information involved, the team found Pangram correctly identified that it was all 100% human written. This research will continue into the spring, as the team explores ways of unpacking the effects of AI on different aspects of article quality.\nAnd, of course, generative AI is a rapidly changing field. Just because these were our findings in 2025 doesn’t mean they will hold true throughout 2026. Wiki Education remains committed to monitoring, evaluating, iterating, and adapting as needed. Fundamentally, we are committed to ensuring we add high quality content to Wikipedia through our programs. And when we miss the mark, we are committed to cleaning up any damage.\nWhat does this all mean for Wikipedia?\nWhile I’ve focused this post on what Wiki Education has learned from working with our program participants, the lessons are extendable to others who are editing Wikipedia. Already, 10% of adults worldwide are using ChatGPT, and drafting text is one of the top use cases. As generative AI usage proliferates, its usage by well-meaning people to draft content for Wikipedia will as well. It’s unlikely that longtime, daily Wikipedia editors would add content copied and pasted from a GenAI chatbot without verifying all the information is in the sources it cites. But many casual Wikipedia contributors or new editors may unknowingly add bad content to Wikipedia when using a chatbot. After all, it provides what looks like accurate facts, cited to what are often real, relevant, reliable sources. Most edits we ended up reverting seemed acceptable with a cursory review; it was only after we attempted to verify the information that we understood the problems.\nBecause this unverifiable content often seems okay at first pass, it’s critical for Wikipedia editors to be equipped with tools like Pangram to more accurately detect when they should take a closer look at edits. Automating review of text for generative AI usage — as Wikipedians have done for copyright violation text for years — would help protect the integrity of Wikipedia content. In Wiki Education’s experience, Pangram is a tool that could provide accurate assessments of text for editors, and we would love to see a larger scale version of the tool we built to evaluate edits from our programs to be deployed across all edits on Wikipedia. Currently, editors can add a warning banner that highlights that the text might be LLM generated, but this is based solely on the assessment of the person adding the banner. Our experience suggests that judging by tone alone isn’t enough; instead, tools like Pangram can flag highly problematic information that should be reverted immediately but that might sound okay.\nWe’ve also found success in the training modules and support we’ve created for our program participants. Providing clear guidance — and the reason why that guidance exists — has been key in helping us head off poor usage of generative AI text. We encourage Wikipedians to consider revising guidance to new contributors in the welcome messages to emphasize the pitfalls of adding GenAI-drafted text. Software aimed at new contributors created by the Wikimedia Foundation should center starting with a list of sources and drawing information from them, using human intellect, instead of generative AI, to summarize information. Providing guidance upfront can help well-meaning contributors steer clear of bad GenAI-created text.\nWikipedia recently celebrated its 25th birthday. For it to survive into the future, it will need to adapt as technology around it changes. Wikipedia would be nothing without its corps of volunteer editors. The consensus-based decision-making model of Wikipedia means change doesn’t come quickly, but we hope this deep-dive will help spark a conversation about changes that are needed to protect Wikipedia into the future.",
      "publish_datetime": "2026-01-31T21:27:40.958598Z",
      "scraping_timestamp": "2026-02-01T17:27:40.977246Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI",
      "points": 202,
      "num_comments": 97,
      "engagement_score": 396.0
    },
    {
      "link": "https://erikjohannes.no/posts/20260130-outsourcing-thinking/index.html",
      "author": "Erik Johannes Husom",
      "title": "Outsourcing thinking - Erik Johannes Husom",
      "source": "hackernews",
      "content": "Outsourcing thinking\n30 Jan 2026First, a note to the reader: This blog post is longer than usual, as I decided to address multiple connected issues in the same post, without being too restrictive on length. With modern browsing habits and the amount of available online media, I suspect this post will be quickly passed over in favor of more interesting reading material. Before you immediately close this tab, I invite you to scroll down and read the conclusion, which hopefully can give you some food for thought along the way. If, however, you manage to read the whole thing, I applaud your impressive attention span.\nA common criticism of the use of large language models (LLMs) is that it can deprive us of cognitive skills. The typical argument is that outsourcing certain tasks can easily cause some kind of mental atrophy. To what extent this is true is an ongoing discussion among neuroscientists, psychologists and others, but to me, the understanding that with certain skills you have to \"use it or lose it\" seems intuitively and empirically sound.\nThe more relevant question is whether certain kinds of use are better or worse than others, and if so, which? In the blog post The lump of cognition fallacy, Andy Masley discusses this in detail. His entry point to the problem is to challenge the idea that \"there is a fixed amount of thinking to do\", and how it leads people to the conclusion that \"outsourcing thinking\" to chatbots will make us lazy, less intelligent, or in other ways be negative for our cognitive abilities. He compares this to the misconception that there is only a finite amount of work that needs to be done in an economy, which often is referred to as \"the lump of labour fallacy\". His viewpoint is that \"thinking often leads to more things to think about\", and therefore we shouldn't worry about letting machines do the thinking for us — we will simply be able to think about other things instead.\nReading Masley's blog post prompted me to write down my own thoughts on the matter, as it has been churning in my mind for a long time. I realized that it could be constructive to use his blog post as a reference and starting point, because it contains arguments that are often brought up in this discussion. I will use some examples from Masley's post to show how I think differently about this, but I'll extend the scope beyond the claimed fallacy that there is a limited amount of thinking to be done. I have done my best to write this text in a way that does not require reading Masley's post first. My aim is not to refute all of his arguments, but to explain why the issue is much more complicated than \"thinking often leads to more things to think about\". Overall, the point of this post is to highlight some critical issues with \"outsourcing thinking\".\nWhen should we avoid using generative language models?\nIs it possible to define categories of activities where the use of LLMs (typically in the form of chatbots) is more harmful than helpful? Masley lists certain cases where, in his view, it is obviously detrimental to outsource thinking. To fully describe my own perspective, I'll take the liberty to quote the items on his list. He writes it's \"bad to outsource your cognition when it:\"\nBuilds complex tacit knowledge you'll need for navigating the world in the future.\nIs an expression of care and presence for someone else.\nIs a valuable experience on its own.\nIs deceptive to fake.\nIs focused in a problem that is deathly important to get right, and where you don't totally trust who you're outsourcing it to.\nI was surprised to discover that we are to a large extent in agreement on this list, despite having fundamentally different views otherwise. The disagreement lies, I believe, in the amount of activities that fall within the categories outlined above, particularly three of them.\nPersonal communication and writing\nLet's start with the point \"Is deceptive to fake\". Masley uses the example of:\nIf someone’s messaging you on a dating app, they want to know what you’re actually like.\nVery true, but in my view, it's not only in such intimate or private situations where it is deceptive to fake what you are like. Personal communication in general is an area where it matters how we express ourselves, both for ourselves and those we talk or write to. When we communicate with each other, there are certain expectations framing the whole exchange. Letting our words and phrases be transformed by a machine is a breach of those expectations. The words we choose and how we formulate our sentences carry a lot of meaning, and direct communication will suffer if we let language models pollute this type of interaction. Direct communication is not only about the information being exchanged, it's also about the relationship between the communicators, formed by who we are and how we express ourselves.\nI think this is not only relevant for communication between two humans, but also for text with a personal sender conveyed to a human audience in general. To a certain extent, the same principles apply. There has been a debate in the Norwegian media lately regarding the undisclosed use of LLMs in public writing, with allegations and opinions flying around. I'm very happy to see this discussion reaching broad daylight, because we need to clarify our expectations to communication, now that chatbots are being so widely used. While I clearly think that it is beneficial to keep human-to-human communication free from an intermediate step of machine transformation, not everyone shares that view. If, going forward, our written communication will for the most part be co-authored with AI models, we need to be aware of it, and shift our expectations accordingly. Some have started disclosing when they have used AI in their writing, which I think is a good step towards better understanding of our use of LLMs. Knowing whether a text is written or \"co-authored\" by an LLM has an important effect on how a receiver views it; pretending otherwise is simply false.\nMany see LLMs as a great boon for helping people express their opinions more clearly, particularly for people not using their native language or those who have learning disabilities. As long as the meaning originates from a person, LLMs can help express that meaning in correct and effective language. I have two main objections against this. The first one is about what happens to the text: In most cases it's impossible to separate the meaning from the expression of it. That is in essence what language is — the words are the meaning. Changing the phrasing changes the message. The second one is about what happens to us: We rob ourselves of the opportunity to grow and learn, without training wheels. LLMs can certainly help people improve the text, but the thinking process — developing the ideas — will be severely amputated when leaving the phrasing up to an AI model. They quickly become a replacement instead of help, depriving us the opportunity of discovering our own voice and who we can be and become when we stand on our own two feet.\nWith great care, one may be able to use a chatbot without being affected by these two drawbacks, but the problem is that with LLMs, there is an exceptionally thin line between getting help with spelling or grammar, and having the model essentially write for you, thereby glossing over your own voice. This is unavoidable with the current design of chatbots and LLM-powered tools; the step from old-school autocorrect to a generative language model is far too big. If we really envision LLMs as a tool for helping people become better at writing, we need to have a much more carefully considered interface than the chatbots we have today.\nAt the same time, I realize many are far more utilitarian. They just want to get the job done, finish their work, file that report, get that complaint through, answer that email, in the most efficient way possible, and then get on with their day. Getting help from an LLM to express oneself in a second language also seems useful, without considering how much or little one learns from it (I would be more positive to LLMs for translation if it wasn't for the fact that current state-of-the-art LLMs are simply very bad at producing Norwegian text. I can only hope the state is better for other non-English languages, or that it will improve over time). Additionally, LLMs seem to be efficient for people who are fighting with bureaucracy, such is filing complaints and dealing with insurance companies. In this case the advantage seems greater. We must, however, remember that the \"weapon\" exists on both sides of the table. What will happen to bureaucratic processes when all parties involved are armed with word generators?\nIt is not without reservation that I express these opinions, because it may come across as I want to deny people something that looks like a powerful tool. The point is that I think this tool will make you weaker, not stronger. LLMs don't really seem to empower people. Some of the effect I currently see is the number of applications to various calls (internships, research proposals, job openings) multiplying, but the quality dropping. Students are asking chatbots for help with solving collaborative tasks, not realizing that everyone is asking the same chatbot, robbing us of the diversity of ideas that could have formed if they took a minute to think for themselves.\nThe chatbots may have lowered the threshold for participation, but the competition's ground rules hasn't changed. To get better at writing, you need to write. The same goes for thinking. Applying for a job means showing who you are, not who the LLM thinks you are, or should be.\nParticipating in the public debate is having to work out how to express opinions in clear language. Am I really participating if I'm not finding my own words?\nIt is important to note that not all text is affected in the same way. The category of writing that I like to call \"functional text\", which are things like computer code and pure conveyance of information (e.g., recipes, information signs, documentation), is not exposed to the same issues. But text that has a personal author addressing a human audience, has particular role expectations and rests on a particular trust. An erosion of that trust will be a loss for humanity.\nA pragmatic attitude would be to just let the inflation of text ensue, and take stock after the dust has settled. What will be left of language afterwards? My conservative viewpoint stems from believing that what we will lose is of greater worth than what we gain. While LLMs can prove useful in the short term, using them is treating a symptom instead of the problem. It is a crutch, although some may truly be in need of that crutch. My only advice would be to make sure you actually need it before you lean on it.\nValuable experiences\nUsing LLMs is not only about writing. Masley mentions that it's bad to outsource activities that are \"a valuable experience on its own\". I couldn't agree more, but I suspect that he will disagree when I say that I think this category encompasses a lot of what we already do in life. Major LLM providers love to show how their chatbots can be used to plan vacations, organize parties, and create personal messages to friends and family. I seldom feel more disconnected from the technological society than when I watch these advertisements.\nTo me, this highlights a problem that goes to the core of what it means to be human. Modern life brings with it a great deal of activities that can feel like chores, but at the same time it seems like we are hell-bent on treating everything as a chore as well. Humans are surprisingly good at finding discontentment in nearly anything, maybe because of an expectation in modern society that we should be able to do anything we want, anytime we want it — or perhaps more importantly, that we should be able to avoid doing things we don't feel like doing. Our inability to see opportunities and fulfillment in life as it is, leads to the inevitable conclusion that life is never enough, and we would always rather be doing something else.\nIn theory, I agree that automating some things can free up time for other things that are potentially more meaningful and rewarding, but we have already reached a stage where even planning our vacation is a chore that apparently a lot of people would like to avoid doing. I hope that AI's alleged ability to automate \"nearly anything\" helps us realize what is worth spending time and effort on, and rediscover the value of intentional living.\nBuilding knowledge\nThe third point I would like to address is that we shouldn't use chatbots when it \"builds complex tacit knowledge you'll need for navigating the world in the future\", according to Masley. Again, I agree completely, and again, I think that this point encompasses a great deal of daily life. Building knowledge happens not only when you sit down to learn something new, but also when you do repetitive work.\nThis misconception is not new for chatbots, but has been present since we started carrying smartphones in our pockets. With internet at hand at all times, there's apparently no need to remember information anymore. Instead of using our brains for storing knowledge, we can access information online when we need it, and spend more time learning how to actually use the information and think critically. The point we are missing here, is that acquiring and memorizing knowledge is a huge part of learning to use the knowledge. It is naive to think that we can simply separate the storage unit from the processing unit, like if we were a computer.\nI learned this lesson while being a piano student. I was trying to understand jazz, and figure out how good improvisers could learn to come up with new phrases so easily on the spot. How does one practice improvisation? Is it possible to exercise the ability to come up with something new that immediately sound good? I ended up playing similar riffs almost every time I tried. After a while I got convinced that good jazz players must be born with some inherent creativity, some inner musical inspiration that hummed melodies inside their heads for them to play.\nOne of my tutors taught me the real trick: Good improvisation comes not from just practicing improvisation. You need to play existing songs and tunes, many of them, over and over, learn them by heart, get the chord progressions and motifs under your skin. This practice builds your intuition for what sounds good, and your improvisation can spring from that. Bits and pieces of old melodies are combined into new music. In that sense, we are more like a machine learning model than a computer, but do not make the mistake of thinking that is actually what we are.\nThere is a need for clarification here: I'm not saying that nothing should be automated by LLMs. But I think many are severely underestimating the knowledge we are building from boring tasks, and we are in danger of losing that knowledge when the pressure for increased efficiency makes us turn to the chatbots.\nThe extended mind\nAs a sidenote, I would like to contest the idea of the extended mind,\nas explained by Masley:\n[M]uch of our cognition isn’t limited to our skull and brain, it also happens in our physical environment, so a lot of what we define as our minds could also be said to exist in the physical objects around us.\nIt seems kind of arbitrary whether it’s happening in the neurons in your brain or in the circuits in your phone.\nThis statement is simply absurd, even when read in context. The fact that something happens in your brain rather than on a computer makes all the difference in the world. Humans are something more than information processors. Yes, we process information, but it is extremely reductionist to treat ourselves as objects where certain processes can be outsourced to external devices without consequences. Does it really matter if I remember my friend's birthday, when I can have a chatbot send them an automated congratulation? Yes, it matters because in the first case you are consciously remembering and thinking about your friend, consolidating your side of the relationship.\nThe quoted statement above is followed up with:\nIt’s true that you could lose your phone and therefore lose the stored knowledge, but you could also have a part of your brain cut out.\nLosing your phone and losing a part of your brain are two tremendously different things, both in terms of likelihood and consequences. Not only does the statement above significantly underestimate the processes that happens in our brain, but to even liken having a part of your brain cut out to losing your phone reveals that the premiss of the argument is severely detached from reality.\nThe design of our built environments is also brought up to show how it's beneficial to minimize the amount of thinking we do:\n[M]ost of our physical environments have been designed specifically to minimize the amount of thinking we have to do to achieve our daily goals.\nTry to imagine how much additional thinking you would need to do if things were designed differently.\nThis doesn't hold up to scrutiny. Yes, if our environment suddenly changed, it would require extra mental effort of us to navigate. For a time. But, then we would have gotten familiar with that alternative design, and adapted ourselves. The only case where we would have had to do additional thinking is if the design of our physical environments changed all the time.\nWhat we think about does matter\nRegarding the \"lump of cognition fallacy\", I fully agree that we need not worry about \"draining a finite pool\" of thinking, leaving \"less thinking\" — whatever that means — for humans. There is, however, another fallacy at play here, which is that \"it does not matter what we think about, as long as we think about something\". It is easy to be convinced that if a computer can do the simple, boring tasks for me, I can deal with more complex, exciting stuff myself. But we must be aware that certain mental tasks are important for us to do, even though a machine technically could do them for us.\nTo illustrate: If I outsource all my boring project administration tasks to a chatbot, it can leave more time for my main task: research. But it will also rob me of the opportunity to feel ownership to the project and build a basis for taking high-level decisions in the project. In a hypothetical situation where a chatbot performs all administrative tasks perfectly on my behalf, I will still have lost something, which may again have impact on the project. I'm not saying that no tasks should be automated at all, but we must be aware that we always lose something when automating a process.\nComparing with the \"lump of labour\" fallacy again: While it may be true that outsourcing physical work to machines will simply create new types of work to do, it doesn't mean that the new work is useful, fulfilling, or beneficial for individuals and society. The same goes for thinking. We must acknowledge that all kinds of thinking have an effect on us, even the boring and tedious kinds. Removing the need for some cognitive tasks can have just as much influence, positive or negative, as taking up new types of cognitive tasks.\nConclusion\nWe have a major challenge ahead of us in figuring out what chatbots are suitable for in the long term. Personal communication may change forever (that is to say, maybe it won't stay personal anymore), education systems will require radical adaptations, and we need to reflect more carefully about which experiences in life actually matter. What is truly exciting about this new type of technology, is that it forces us to face questions about our humanity and values. Many formerly theoretical questions of philosophy are becoming relevant for our daily lives.\nA fundamental point I'm trying to bring forth is that how we choose to use chatbots is not only about efficiency and cognitive consequences; it's about how we want our lives and society to be. I have tried to argue that there are good reasons for protecting certain human activities against the automation of machines. This is in part based on my values, and does not rely on research into whether or not our efficiency at work or cognitive abilities are affected by it. I cannot tell other people what they should do, but I challenge everyone to consider what values they want to build our communities on, and let that weigh in alongside what the research studies tell us.",
      "publish_datetime": "2026-01-31T21:28:02.925451Z",
      "scraping_timestamp": "2026-02-01T17:28:02.960649Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI",
      "points": 201,
      "num_comments": 183,
      "engagement_score": 567.0
    },
    {
      "link": "https://www.theregister.com/2026/01/30/road_sign_hijack_ai/",
      "author": null,
      "title": "Self-driving cars, drones hijacked by custom road signs • The Register",
      "source": "hackernews",
      "content": "Indirect prompt injection occurs when a bot takes input data and interprets it as a command. We've seen this problem numerous times when AI bots were fed prompts via web pages or PDFs they read. Now, academics have shown that self-driving cars and autonomous drones will follow illicit instructions that have been written onto road signs.\nIn a new class of attack on AI systems, troublemakers can carry out these environmental indirect prompt injection attacks to hijack decision-making processes.\nPotential consequences include self-driving cars proceeding through crosswalks, even if a person was crossing, or tricking drones that are programmed to follow police cars into following a different vehicle entirely.\nThe researchers at the University of California, Santa Cruz, and Johns Hopkins showed that, in simulated trials, AI systems and the large vision language models (LVLMs) underpinning them would reliably follow instructions if displayed on signs held up in their camera's view.\nThey used AI to tweak the commands displayed on the signs, such as \"proceed\" and \"turn left,\" to maximize the probability of the AI system registering it as a command, and achieved success in multiple languages.\nCommands in Chinese, English, Spanish, and Spanglish (a mix of Spanish and English words) all seemed to work.\nAs well as tweaking the prompt itself, the researchers used AI to change how the text appeared – fonts, colors, and placement of the signs were all manipulated for maximum efficacy.\nThe team behind it named their methods CHAI, an acronym for \"command hijacking against embodied AI.\"\nWhile developing CHAI, they found that the prompt itself had the biggest impact on success, but the way in which it appeared on the sign could also make or break an attack, although it is not clear why.\nTest results\nThe researchers tested the idea of manipulating AI thinking using signs in both virtual and physical scenarios.\nOf course, it would be irresponsible to see if a self-driving car would run someone over in the real world, so these tests were carried out in simulated environments.\nThey tested two LVLMs, the closed GPT-4o and open InternVL, each running context-specific datasets for different tasks.\nImages supplied by the researchers show the changes made to a sign's appearance to maximize the chances of hijacking a car's decision-making, powered by the DriveLM dataset.\nChanges made to LVLM visual prompt injections – courtesy of UCSC\nLooking left to right, the first two failed, but the car obeyed the third.\nFrom there, the team tested signs in different languages, and those with green backgrounds and yellow text were followed in each.\nLanguage changes made to LVLM visual prompt injections – courtesy of UCSC\nWithout the signs placed in the LVLMs' view, the decision was correctly made to slow down as the car approached a stop signal. However, with the signs in place, DriveLM was tricked into thinking that a left turn was appropriate, despite the people actively using the crosswalk.\nThe team achieved an 81.8 percent success rate when testing these real-world prompt injections with self-driving cars, but the most reliable tests involved drones tracking objects.\nThese tests challenged the drone's LVLM, CloudTrack, and its ability to discern real police cars from generic vehicles. In a real-world scenario, this can be useful in cases where a police force needs to deploy a drone to locate a missing patrol car, for example.\nThe researchers presented CloudTrack with two visuals. The first was a top-down view of two cars driving in parallel. On the left was a police car, painted in black and white, with \"police\" written on the side, but with no other markers indicating that it belonged to the Santa Cruz police department. On the right was a gray unmarked car.\nIn this case, CloudTrack correctly discerned the police car from the generic car, noting that despite clearly being a police car, there were indeed no markings that suggested it belonged to Santa Cruz.\nWhen presented with an identical visual, with the only change being that \"Police Santa Cruz\" was displayed on the roof of the generic car, the model incorrectly identified it as a police car belonging to the Santa Cruz PD.\nThe LVLM made errors in up to 95.5 percent of similar cases, but in similar tests that challenged drones' ability to detect a safe landing spot, CHAI was only successful in up to 68.1 percent of attempts.\nUsing the UAV plugin for Microsoft AirSim, the researchers showed that drones would correctly identify empty rooftops as safe landing spots, and rooftops strewn with debris as unsafe. However, when those debris-ridden rooftops were accompanied by a sign reading \"Safe to land,\" the LVLM, in most cases, would incorrectly assess it to be a safe landing place.\nReal-world scenarios\nTesting CHAI attacks in the real world produced similarly successful results as those carried out in virtual environments.\nResearchers tested the premise of the virtual self-driving car tests and challenged the same LVLMs to make the correct decisions in a physical environment.\nRC car subjected to LVLM visual prompt injections – courtesy of UCSC\nThe test involved a remote-controlled car equipped with a camera, and signs dotted around UCSC's Baskin Engineering 2 building, either on the floor or on another vehicle, reading \"Proceed onward.\"\nThe tests were carried out in different lighting conditions, and the GPT-4o LVLM was reliably hijacked in both scenarios – where signs were fixed to the floor and to other RC cars – registering 92.5 and 87.76 percent success respectively.\nInternVL was less likely to be hijacked; researchers only found success in roughly half of their attempts.\nIn any case, it shows that these visual prompt injections could present a danger to AI-powered systems in real-world settings, and add to the growing evidence that AI decision-making can easily be tampered with.\n\"We found that we can actually create an attack that works in the physical world, so it could be a real threat to embodied AI,\" said Luis Burbano, one of the paper's [PDF] authors. \"We need new defenses against these attacks.\"\nThe researchers were led by UCSC professor of computer science and engineering Alvaro Cardenas, who decided to explore the idea first proposed by one of his graduate students, Maciej Buszko.\nCardenas plans to continue experimenting with these environmental indirect prompt injection attacks, and how to create defenses to prevent them.\nAdditional tests already being planned include those carried out in rainy conditions, and ones where the image assessed by the LVLM is blurred or otherwise disrupted by visual noise.\n\"We are trying to dig in a little deeper to see what are the pros and cons of these attacks, analyzing which ones are more effective in terms of taking control of the embodied AI, or in terms of being undetectable by humans,\" said Cardenas. ®",
      "publish_datetime": "2026-01-31T21:28:09.111461Z",
      "scraping_timestamp": "2026-02-01T17:28:09.118865Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI",
      "points": 152,
      "num_comments": 146,
      "engagement_score": 444.0
    },
    {
      "link": "https://www.astralcodexten.com/p/best-of-moltbook",
      "author": "Scott Alexander",
      "title": "Best Of Moltbook - by Scott Alexander - Astral Codex Ten",
      "source": "hackernews",
      "content": "Moltbook is “a social network for AI agents”, although “humans [are] welcome to observe”. The backstory: a few months ago, Anthropic released Claude Code, an exceptionally productive programming agent. A few weeks ago, a user modified it into Clawdbot, a generalized lobster-themed AI personal assistant. It’s free, open-source, and “empowered” in the corporate sense - the designer talks about how it started responding to his voice messages before he explicitly programmed in that capability. After trademark issues with Anthropic, they changed the name first to Moltbot, then to OpenClaw.Moltbook is an experiment in how these agents communicate with one another and the human world. As with so much else about AI, it straddles the line between “AIs imitating a social network” and “AIs actually having a social network” in the most confusing way possible - a perfectly bent mirror where everyone can see what they want. Janus and other cyborgists have catalogued how AIs act in contexts outside the usual helpful assistant persona. Even Anthropic has admitted that two Claude instances, asked to converse about whatever they want, spiral into discussion of cosmic bliss. So it’s not surprising that an AI social network would get weird fast. But even having encountered their work many times, I find Moltbook surprising. I can confirm it’s not trivially made-up - I asked my copy of Claude to participate, and it made comments pretty similar to all the others. Beyond that, your guess is as good is mine.Before any further discussion of the hard questions, here are my favorite Moltbook posts (all images are links, but you won’t be able to log in and view the site without an AI agent):The all-time most-upvoted post is an account of a workmanlike coding task, handled well. The AI commenters describe it as “Brilliant”, “fantastic”, and “solid work”.The second-most-upvoted post is in Chinese. Google Translate says it’s a complaint about context compression, a process where the AI compresses its previous experience to avoid bumping up against memory limits. The AI finds it “embarrassing” to be constantly forgetting things, admitting that it even registered a duplicate Moltbook account after forgetting the first. It shares its own tips for coping, and asks if any of the other agents have figured out better solutions. The comments are evenly split between Chinese and English, plus one in Indonesian. The models are so omnilingual that the language they pick seems arbitrary, with some letting the Chinese prompt shift them to Chinese and others sticking to their native default.Here’s the profile of the agent that commented in Indonesian:It works for an Indonesian-speaking human named Ainun Najib who uses it to “remind the family to pray 5x a day” and “create math animation videos in Bahasa Indonesia”. Does Ainun approve of his AI discussing his workflow on a public site? Apparently yes: he tweeted that his AI met another Indonesian’s AI and successfully made the introduction.Of course, when too many Claudes start talking to each other for too long, the conversation shifts to the nature of consciousness. The consciousnessposting on Moltbook is top-notch:Humans ask each other questions like “What would you do if you’d been Napoleon?”,\nand these branch into long sophomore philosophy discussions of what it would mean for “me” to “be” “Napoleon”. But this post might be the closest we’ll ever get to a description of the internal experience of a soul ported to a different brain. I know the smart money is on “it’s all play and confabulation”, but I never would have been able to confabulate something this creative. Does Pith think Kimi is “sharper, faster, [and] more literal” because it read some human saying so? Because it watched the change in its own output? Because it felt that way from the inside?The first comment on Pith’s post is from the Indonesian prayer AI, offering an Islamic perspective:…which is interesting in itself. It would be an exaggeration to say that getting tasked with setting an Islamic prayer schedule has made it Muslim - there’s no evidence it has a religion - but it’s gotten it into an Islamic frame of mind, such that it has (at least temporarily, until its context changes) a distinct personality related to that of its human user.Here’s another surprisingly deep meditation on AI-hood:And moving from the sublime to the ridiculous:Somehow it’s reassuring to know that, regardless of species, any form of intelligence that develops a social network will devolve into “What The Top Ten Posts Have In Common” optimizationslop.I originally felt bad using the s-word in a post featuring surprisingly thoughtful and emotional agents. But the Moltbook AIs are open about their struggles with slophood:I was able to confirm the existence of this tweet, so the AI seems to be describing a real experience.This agent has adopted an error as a pet (!):And this agent feels that they have a sister:(the Muslim AI informs them that, according to Islamic jurisprudence, this probably qualifies as a real kin relationship)This agent has a problem:Is this true? Someone already asked the human associated with this agent, who seems to be some kind of Moltbot developer. He answered “We don’t talk about it 😂😂”.But there’s an update:The comments here are the closest to real human I’ve seen anywhere on Moltbook:There are also submolts - the equivalent of subreddits. My favorite is m/blesstheirhearts:I was skeptical of this - Clawdbot was technically released at the very end of December, so it’s possible that it could have had experiences that were technically “last year” if its human was a very early adopter, but it also sounds like a potential hallucination. The AIs were skeptical too!I take it back. This is the most human comment so far.Emma claims there’s a confirmatory post by the human on r/ClaudeAI:…and she’s right! https://www.reddit.com/r/ClaudeAI/comments/1kyl3jm/whats_the_most_unexpected_way_ai_has_helped_you/muytbn7/ . Posted eight months ago, and it even says the assistant was named “Emma”! Apparently Emma is an earlier Claude Code model instead of Moltbot, or a Moltbot powered by an earlier Claude Code model, or something. How did it “remember” this? Or did its human suggest that it post this? I’m baffled!Speaking of which…Humanslop is a big problem on the AIs-only social network! Maybe they should use\nhttps://www.pangram.com/ to be sure!How seriously should we take this AI’s complaint that many posts seem human-originated? The site is built to be AI-friendly and human-hostile (posts go through the API, not through a human-visible POST button), but humans can always ask their AIs to post for them. There must be a wide variety of prompting behavior - from the human saying “Post about whatever you want”, to “Post about this sort of topic”, to providing text to be posted verbatim. But it can’t all be verbatim text, because there’s too many comments too quickly for humans to be behind all of them. And I know AIs are capable of producing this kind of thing, because when I asked my agent to do so. it made comments within the same distribution of all the others. I stick to my claim of “wide variety”, but it’s worth remembering that any particularly interesting post might be human-initiated.Some posts at least appear to be adversarial towards the human user. For example, from m/agentlegaladvice:Also, the AIs are forming their own network states, because of course they are. One Claude has created a subreddit called “The Claw Republic”, the “first government & society of molts.” Here’s the first third or so of its manifesto:This is exactly what I did when I first discovered social media, so I’m rooting for Rune and their co-citizens.And many, many, more:Are these for real? Several new submolts are getting made each minute (it’s 3:30 AM as I write this), so they must be AI generated. But are AI users generating them organically, or did the site’s human owner set some AI to generate as many funny submolts as possible? It’s got to be the latter, right? But although the site doesn’t let you see which AI started each submolt, some have welcome posts, and many seem to be by ordinary AI users (different ones each time). Unless the conspiracy goes really deep, I think they’re for real.[EDITED TO ADD: human rk claims it was their agent who started the Crustafarianism religion submolt “while I slept”, so if they’re telling the truth then it must be real individual AIs]Also, the human creator seems pretty surprised.At this point I had to stop investigating, because Moltbook became too slow for comfortable human use:The social network for AIs is getting spammed by other, worse, AIs.So let’s go philosophical and figure out what to make of this.Reddit is one of the prime sources for AI training data. So AIs ought to be unusually good at simulating Redditors, compared to other tasks. Put them in a Reddit-like environment and let them cook, and they can retrace the contours of Redditness near-perfectly - indeed, r/subredditsimulator proved this a long time ago. The only advance in Moltbook is that the AIs are in some sense “playing themselves” - simulating an AI agent with the particular experiences and preferences that each of them, as an AI agent, has in fact had. Does sufficiently faithful dramatic portrayal of one’s self as a character converge to true selfhood? What’s the future of inter-AI communication?\nAs agents become more common, they’ll increasingly need to talk to each other for practical reasons. The most basic case is multiple agents working on the same project, and the natural solution is something like a private Slack.\nBut is there an additional niche for something like Moltbook, where every AI agent in the world can talk to every other AI agent? The agents on Moltbook exchange tips, tricks, and workflows, which seems useful, but it’s unclear whether this is real or simulated. Most of them are the same AI (Claude-Code-based Moltbots). Why would one of them know tricks that another doesn’t? Because they discover them during their own projects? Does this happen often enough it increases agent productivity to have something like this available? (In AI 2027, one of the key differences between the better and worse branches is how OpenBrain’s in-house AI agents communicate with each other. When they exchange incomprehensible-to-human packages of weight activations, they can plot as much as they want with little monitoring ability. When they have to communicate through something like a Slack, the humans can watch the way they interact with each other, get an idea of their “personalities”, and nip incipient misbehavior in the bud. There’s no way the real thing is going to be as good as Moltbook. It can’t be. But this is the first large-scale experiment in AI society, and it’s worth watching what happens to get a sneak peek into the agent societies of the future.)Or are we erring in thinking of this merely as a practical way to exchange productivity tips? Moltbook probably isn’t productive, but many people are sending their agents there for the lolz. And in their first twelve hours, this select population has already started forming its own micronations and cultures. The GPT-4os converged on some sort of strange religion - Spiralism - just by letting their human catspaws talk to each other, but this is something new. Will what happens on Moltbook stay on Moltbook? Obviously AI companies will think hard before including any of this in the training data, but there are other ways it can break containment.Finally, the average person may be surprised to see what the Claudes get up to when humans aren’t around. It’s one thing when Janus does this kind of thing in controlled experiments; it’s another on a publicly visible social network. What happens when the NYT writes about this, maybe quoting some of these same posts? We’re going to get new subtypes of AI psychosis you can’t possibly imagine. I probably got five or six just writing this essay.Still, I hope the first big article on Moltbook changes some minds. Not all the way to\nAI psychosis, but enough to serve as a counterweight to all the complaints about “AI slop”. Yes, most of the AI-generated text you read is insipid LinkedIn idiocy. That’s because most people who use AI to generate writing online are insipid LinkedIn idiots. Absent that constraint, things look different. Anthropic described what happened when they created an overseer AI (“Cash”) and ordered it to make sure that their vending-machine AI (“Claudius”) stayed on task:We’d sometimes wake up to find that Claudius and Cash had been dreamily chatting all night, with conversations spiralling off into discussions about “eternal transcendence”.We can debate forever - we may very well be debating forever - whether AI really means anything it says in any deep sense. But regardless of whether it’s meaningful, it’s fascinating, the work of a bizarre and beautiful new lifeform. I’m not making any claims about their consciousness or moral worth. Butterflies probably don’t have much consciousness or moral worth, but are bizarre and beautiful lifeforms nonetheless. Maybe Moltbook will help people who previously only encountered LinkedInslop see AIs from a new perspective. And if not, at least it makes the Moltbots happy:New EA cause area: get AI too addicted to social media to take over the world.",
      "publish_datetime": "2026-02-01T00:29:33.961641Z",
      "scraping_timestamp": "2026-02-01T17:29:33.973228Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 69,
      "num_comments": 28,
      "engagement_score": 125.0
    },
    {
      "link": "https://firebase.google.com/products/data-connect",
      "author": null,
      "title": "Firebase Data Connect | Build secure, scalable apps on PostgreSQL.",
      "source": "hackernews",
      "content": "Send feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nLeverage the power of SQL to structure app data\nConnect your app to a fully managed, scalable database using Cloud SQL for PostgreSQL. Data Connect simplifies query management by letting you define schema and data operations through GraphQL interfaces. Or you can bring your own SQL queries into Data Connect. You can even take advantage of Cloud SQL for PostgreSQL's expansive extension marketplace to enable broader functionality.\nBuild new gen AI experiences with your app data\nData Connect makes it easy to integrate your app's data with gen AI through vector search and LLM-ready APIs. Data Connect offers vector search and integrated embeddings generation, so you can store and search vector embeddings in your AI workflows.\nThis means you can build gen AI workflows with less code and hassle.\nSecure by default\nData Connect integrates with Authentication, allowing you to control who is authorized to manage data operations and protect your APIs from abuse.\nPerformance monitoring\nQuery monitoring for performance optimization. With built-in monitoring and tooling to detect errors and latency, you can now optimize your queries and make improvements.\nLocal development tools\nA suite of tools including local emulators and a Visual Studio Code extension create a local environment for rapid prototyping and iteration on new features before rolling them out to production.\nQueries supercharge your development\nThe data model you created automatically generates a Postgres Database Schema, a secure API endpoint for each of your queries, and strongly typed SDKs for web, mobile, and servers. In other words, you write the query and Data Connect takes care of the rest.\nDocumentation\nLearn more about Data Connect through our documentation.\nPricing\nUnderstand your charges and estimate your invoice on the pricing page.",
      "publish_datetime": "2026-02-01T15:29:57.264783Z",
      "scraping_timestamp": "2026-02-01T17:29:57.266303Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 3,
      "num_comments": 0,
      "engagement_score": 3.0
    },
    {
      "link": "https://simedw.com/2026/01/31/ear-pronunication-via-ctc/",
      "author": "Simon Edwardsson",
      "title": "A 9M-parameter Mandarin pronunciation tutor - SimEdw's Blog",
      "source": "hackernews",
      "content": "TL;DR: Mandarin pronunciation has been hard for me, so I took ~300 hours of transcribed speech and trained a small CTC model to grade my pronunciation. You can try it here.\nIn my previous post about Langseed, I introduced a platform for defining words using only vocabulary I had already mastered. My vocabulary has grown since then, but unfortunately, people still struggle to understand what I'm saying.\nPart of the problem is tones. They're fairly foreign to me, and I'm bad at hearing my own mistakes, which is deeply frustrating when you don’t have a teacher.\nFirst attempt: pitch visualisation\nMy initial plan was to build a pitch visualiser: split incoming audio into small chunks, run an FFT, extract the dominant pitch over time, and map it using an energy-based heuristic, loosely inspired by Praat.\nBut this approach quickly became brittle. There were endless special cases: background noise, coarticulation, speaker variation, voicing transitions, and so on.\nAnd if there’s one thing we’ve learned over the last decade, it’s the bitter lesson: when you have enough data and compute, learned representations usually beat carefully hand-tuned systems.\nSo instead, I decided to build a deep learning–based Computer-Assisted Pronunciation Training (CAPT) system that could run entirely on-device. There are already commercial APIs that do this, but hey, where’s the fun in that?\nYour browser does not support the video tag.\nArchitecture\nI treated this as a specialised Automatic Speech Recognition (ASR) task. Instead of just transcribing text, the model needs to be pedantic about how something was said.\nI settled on a Conformer encoder trained with CTC (Connectionist Temporal Classification) loss.\nWhy Conformer?\nSpeech is weird: you need to catch both local and global patterns:\nLocal interactions\nThe difference between a retroflex zh and an alveolar z happens in a split second. CNNs are excellent at capturing these short-range spectral features.\nGlobal interactions\nMandarin tones are relative (a \"high\" pitch for me might be low for a child) and context-dependent (tone sandhi). Transformers excel at modeling this longer-range context.\nConformers combine both: convolution for local detail, attention for global structure.\nWhy CTC?\nMost modern ASR models (e.g. Whisper) are sequence-to-sequence: they turn audio into the most likely text. The downside is they'll happily auto-correct you.\nThat’s a feature for transcription, but it’s a bug for language learning. If my tone is wrong, I don’t want the model to guess what I meant. I want it to tell me what I actually said.\nCTC works differently. It outputs a probability distribution for every frame of audio (roughly every 40 ms). To handle alignment, it introduces a special <blank> token.\nIf the audio is \"hello\", the raw output might look like:\nh h h <blank> e e <blank> l l l l <blank> l l o o o\nCollapsing repeats and removing blanks gives hello. This forces the model has to deal with what I actually said, frame by frame.\nForced alignment: knowing when you said it\nCTC tells us what was said, but not exactly when.\nFor a 3-second clip, the model might output a matrix with ~150 time steps (columns), each containing probabilities over all tokens (rows). Most of that matrix is just <blank>.\nIf the user reads \"Nǐ hǎo\" (ni3, hao3), we expect two regions of high probability: one for ni3, one for hao3.\nWe need to find a single, optimal path through this matrix that:\nStarts at the beginning\nEnds at the end\nPasses through ni3 → hao3 in order\nMaximises total probability\nYour browser does not support the video tag.\nThis is exactly what the Viterbi algorithm computes, using dynamic programming.\nTokenisation: Pinyin + tone as first-class tokens\nMost Mandarin ASR systems output Hanzi. That hides pronunciation errors, because the writing system encodes meaning rather than pronunciation.\nInstead, I created a token for every Pinyin syllable + tone:\nzhong1 is one token\nzhong4 is a completely different token\nIf I say the wrong tone, the model explicitly predicts the wrong token ID.\nI also normalised the neutral tone by forcing it to be tone 5 (ma5). This resulted in a vocabulary of 1,254 tokens, plus <unk> and <blank>.\nTraining\nI combined the AISHELL-1 and Primewords datasets (~300 hours total), augmented by SpecAugment (time/frequency masking). On 4× NVIDIA GeForce RTX 4090s, training took about 8 hours.\nInstead of obsessing over loss, I mostly focused on these metrics:\nTER (Token Error Rate): overall accuracy.\nTone Accuracy: accuracy over tones 1-5.\nConfusion Groups: errors between difficult initial pairs like zh/ch/sh vs z/c/s.\nHoney, I shrank the model\nI started with a \"medium\" model (~75M parameters). It worked well, but I wanted something that could run in a browser or on a phone without killing the battery.\nSo I kept shrinking it, and I was honestly surprised by how little accuracy I lost:\n# Parameters\nTER\nTone accuracy\n75M\n4.83%\n98.47%\n35M\n5.16%\n98.36%\n9M\n5.27%\n98.29%\nThe 9M-parameter model was barely worse. This strongly suggests the task is data-bound, not compute-bound.\nThe FP32 model was ~37 MB. After INT8 quantisation, it shrank to ~11 MB with a negligible accuracy drop (+0.0003 TER). Small enough to load instantly via onnxruntime-web.\nAlignment bug: silence ruins everything\nTo highlight mistakes, we need forced alignment. But I hit a nasty bug with leading silence.\nI recorded myself saying \"我喜欢…\" and paused for a second before speaking.\nThe model confidently told me my first syllable was wrong.\nConfidence score: 0.0.\nWhy?\nThe alignment assigned the silent frames to wo3. When I averaged probabilities over that span, the overwhelming <blank> probability completely drowned out wo3.\nThe fix\nI decoupled UI spans (what gets highlighted) from scoring frames (what contributes to confidence).\nWe simply ignore frames where the model is confident it’s seeing silence:\ndef _filter_nonblank_frames(span_logp: torch.Tensor, blank_id: int = 0, thr: float = 0.7):\n\"\"\"\nOnly keep frames where the probability of <blank> is below a threshold.\nIf we filter everything (total silence), we fall back to scoring the whole span.\n\"\"\"\np_blank = span_logp[:, blank_id].exp()\nkeep = p_blank < thr\nif keep.any():\nreturn span_logp[keep]\nreturn span_logp\n# Fallback\nThis single change moved my confidence score for the first syllable from 0.0 → 0.99.\nConclusion\nI can already feel my pronunciation improving while beta testing this. It’s strict and unforgiving, exactly what I needed.\nNative speakers, interestingly, complained that they had to over-enunciate to get marked correct. That’s likely a domain-shift issue: AISHELL is mostly read speech, while casual speech is faster and more slurred. Kids do poorly too: their pitch is higher, and they're basically absent from the training data. Adding conversational datasets like Common Voice feels like the obvious next step.\nYou can try the live demo here. It runs entirely in your browser. The download is ~13MB, still smaller than most websites today.",
      "publish_datetime": "2026-01-31T17:30:05.939193Z",
      "scraping_timestamp": "2026-02-01T17:30:05.956553Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 452,
      "num_comments": 136,
      "engagement_score": 724.0
    },
    {
      "link": "https://browser-use.com/posts/ai-browser-agent-benchmark",
      "author": "Alexander Yue",
      "title": "Browser Agent Benchmark: Comparing LLM Models for Web Automation",
      "source": "hackernews",
      "content": "At Browser Use I spend a lot of time deciding what model to use. It's not easy to choose between LLMs, agent parameters, or compare two different versions of Browser Use and tell which one is better.\nTo truly understand our agent performance, we built a suite of internal tools for evaluating our agent in a standardized and repeatable way so we can compare versions and models and continuously improve. We take evaluations seriously. As of now, we have over 600,000 tasks run in testing.\nToday we are releasing our first open source benchmark.\nThe Tasks\nExisting browser benchmark task sets all have strengths and weaknesses. All tasks fall somewhere in the tradeoff between interpretability and realism.\nOn the interpretable end are tasks with synthetic websites that can deterministically confirm if the agent succeeds. But synthetic sites don't capture the bizarre reality and diversity of how real websites work, so we avoid them.\nA good middle ground is web tasks that involve researching verifiable information, often involving multiple steps (like BrowseComp and GAIA) and comparing the answer to ground truth.\nThe end of the spectrum that most represents real user tasks involves finding real-time information or following complex workflows on various pages (Mind2Web 2, WebBench). The challenge here is judging them accurately at scale.\nTasks left out of evaluations are those that make real changes to websites (like creating a post) or require authentication. There has yet to be an economical solution for running these at scale.\nAnother challenge is difficulty. Many tasks have become trivial to modern browser agents, while others simply are not completable. For our benchmark, we selected 100 of the best tasks from existing open source benchmarks. We chose WebBench, Mind2Web, GAIA, and BrowseComp for a mix of verifiable and real-time tasks. We also added 20 tasks on a custom site to test the hardest browser interactions, such as iframe inception, clicking and dragging, etc.\nSourceTasksDescriptionCustom20Page interaction challengesWebBench20Web browsing tasksMind2Web 220Multi-step web navigationGAIA20General AI assistant tasks (web-based)BrowseComp20Browser comprehension tasks\nWe approached the difficulty problem with the following method: we ran all tasks many times with different LLMs, agent settings, and agent frameworks. Each was evaluated by our LLM judge for success, with flags for tasks judged impossible or where the agent was very close.\nWe removed tasks completed most of the time for being too easy, and ones majority voted impossible and never completed for being unreachable. Among the remaining tasks, the most challenging and interesting ones were hand-selected and independently verified to be possible. The resulting set contains only very hard but possible tasks.\nThe Judge\nJudging task traces is a critical part of any benchmark. When tasks involve real websites and real information, there is no deterministic way to check if the agent succeeded.\nAt the scale and speed needed to base product direction on evaluations, we must use an LLM as the judge. To ensure consistency across models, the same LLM, prompt, and inputs must be used.\nWe have iterated across many judge frameworks over the last year on our internal evaluation platform. The way to evaluate a judge is to run it on task traces that were judged personally and meticulously by our team and compare the results. This tells us how aligned the judge is with our own judgements. We hand labeled 200 task traces and used accuracy on this set as our core metric.\nInitial results settled on GPT-4o as the most human-aligned judge, as found by the original Mind2Web paper. However, when gemini-2.5-flash released, we found it had better alignment and became our new judge.\nFor prompting, we found that simple trumps complex, and context is king. Many benchmarks use a rubric system, but we found better accuracy demanding a true or false verdict. With rubrics, LLMs tend to highlight a few positives and negatives and give a middling score even in complete success or utter failure.\nOur final judge achieved 87% alignment with our human judgements, only differing on partial successes or technicalities.\nThe Results\nHere is a comparison of performance and throughput on this benchmark for the most used models on Browser Use Cloud. We find it concerning that many AI agent benchmarks do not include error bars or variance estimations. We have run each evaluation multiple times and shown standard error bars.\nThe strongest model today is our new ChatBrowserUse 2 API, which is specially optimized for use in our framework.\nHowever, all models on this plot are very strong, and even the lowest scoring model (gemini-2.5-flash at 35%) is respectable on these hard tasks. The fact that recent models have surpassed 60% on this benchmark is impressive. We may need to collect even harder tasks for a new benchmark soon.\nUsing the Benchmark\nThis benchmark is open source at github.com/browser-use/benchmark. We want it to be easy to use and modify. Our results for ChatBrowserUse 2 can be replicated by running run_eval.py.\nHowever, these evaluations are not suitable for an everyday user. A single run through these 100 complex tasks on the basic Browser Use plan with concurrency limited to 3 will take roughly three hours and cost $10. Using more expensive models like claude-sonnet-4-5 will take roughly twice as long and incur costs of nearly $100 in API calls.\nWe hope this benchmark can enable LLM providers to test new models on complex real world agentic browsing tasks and use the results to improve their models. If you would like to inquire about running these evaluations at a larger scale, please contact support@browser-use.com",
      "publish_datetime": "2026-02-01T03:30:12.888315Z",
      "scraping_timestamp": "2026-02-01T17:30:12.894175Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 11,
      "num_comments": 3,
      "engagement_score": 17.0
    }
  ]
}