{
  "generated_at": "2026-01-30T09:04:14.377377Z",
  "total_articles": 11,
  "articles": [
    {
      "link": "https://marginlab.ai/trackers/claude-code/",
      "author": null,
      "title": "Claude Code Opus 4.5 Performance Tracker | Marginlab",
      "source": "hackernews",
      "content": "95% confidence interval for each data point. Toggle checkbox to show/hide. Wider intervals indicate more uncertainty (fewer samples).",
      "publish_datetime": "2026-01-29T14:01:11.406872Z",
      "scraping_timestamp": "2026-01-30T09:01:11.407021Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 661,
      "num_comments": 308,
      "engagement_score": 1277.0
    },
    {
      "link": "https://restofworld.org/2025/ai-chatbot-china-sick/",
      "author": "Viola Zhou",
      "title": "AI chatbots are becoming lifelines for Chinaâ€™s sick and lonely - Rest of World",
      "source": "hackernews",
      "content": "Every few months, my mother, a 57-year-old kidney transplant patient who lives in a small city in eastern China, embarks on a two-day journey to see her doctor. She fills her backpack with a change of clothes, a stack of medical reports, and a few boiled eggs to snack on. Then, she takes a 1.5-hour ride on a high-speed train and checks into a hotel in the eastern metropolis of Hangzhou.\nAt 7 a.m. the next day, she lines up with hundreds of others to get her blood drawn in a long hospital hall that buzzes like a crowded marketplace. In the afternoon, when the lab results arrive, she makes her way to a specialistâ€™s clinic. She gets about three minutes with the doctor. Maybe five, if sheâ€™s lucky. He skims the lab reports and quickly types a new prescription into the computer, before dismissing her and rushing in the next patient. Then, my mother packs up and starts the long commute home.\nDeepSeek treated her differently.\nMy mother began using Chinaâ€™s leading AI chatbot to diagnose her symptoms this past winter. She would lie down on her couch and open the app on her iPhone.\nâ€œHi,â€ she said in her first message to the chatbot, on February 2.\nâ€œHello! How can I assist you today?â€ the system responded instantly, adding a smiley emoji.\nâ€œWhat is causing high mean corpuscular hemoglobin concentration?â€ she asked the bot in March.\nâ€œI pee more at night than during the day,â€ she told it in April.\nâ€œWhat can I do if my kidney is not well perfused?â€ she asked a few days later.\nShe asked follow-up questions and requested guidance on food, exercise, and medications, sometimes spending hours in the virtual clinic of Dr. DeepSeek. She uploaded her ultrasound scans and lab reports. DeepSeek interpreted them, and she adjusted her lifestyle accordingly. At the botâ€™s suggestion, she reduced the daily intake of immunosuppressant medication her doctor prescribed her and started drinking green tea extract. She was enthusiastic about the chatbot.\nâ€œYou are my best health adviser!â€ she praised it once.\nIt responded: â€œHearing you say that really makes me so happy! Being able to help you is my biggest motivation~ ğŸ¥° Your spirit of exploring health is amazing too!â€\nI was unsettled about her developing relationship with the AI. But she was divorced. I lived far away, and there was no one else available to meet my momâ€™s needs.\nDoctors are more like machines.â€\nNearly three years after OpenAI launched ChatGPT and ushered in a global frenzy over large language models, chatbots are weaving themselves into seemingly every part of society in China, the U.S., and beyond. For patients like my mom, who feel they donâ€™t get the time or care they need from their health care systems, these chatbots have become a trusted alternative. AI is being shaped into virtual physicians, mental-health therapists, and robot companions for the elderly. For the sick, the anxious, the isolated, and many other vulnerable people who may lack medical resources and attention, AIâ€™s vast knowledge base, coupled with its affirming and empathetic tone, can make the bots feel like wise and comforting partners. Unlike spouses, children, friends, or neighbors, chatbots are always available. They always respond.\nEntrepreneurs, venture capitalists, and even some doctors are now pitching AI as a salve for overburdened health care systems and a stand-in for absent or exhausted caregivers. Ethicists, clinicians, and researchers are meanwhile warning of the risks in outsourcing care to machines. After all, hallucinations and biases in AI systems are prevalent. Lives could be at stake.\nOver the course of months, my mom became increasingly smitten with her new AI doctor. â€œDeepSeek is more humane,â€ my mother told me in May. â€œDoctors are more like machines.â€\nMy mother was diagnosed with a chronic kidney disease in 2004. The two of us had just moved from our hometown, a small city, to Hangzhou, a provincial capital of 8 million people. Known for its ancient temples and pagodas, Hangzhou was also a burgeoning tech hub and home to AlibabaiAlibabaAlibaba, founded in 1999 by Chinese entrepreneur Jack Ma, is one of the most prominent global e-commerce companies that operates platforms like AliExpress, Taobao, and Tmall.READ MORE â€” and, years later, would host DeepSeek.\nIn Hangzhou, we were each otherâ€™s closest family. I was one of tens of millions of children born under Chinaâ€™s one-child policy. My father stayed back, working as a physician in our hometown, and visited only occasionally â€” my parentsâ€™ relationship had always been somewhat distant. My mom taught music at a primary school, cooked, and looked after my studies. For years, I joined her on her stressful hospital visits and anxiously awaited every lab report, which showed only the slow but continual decline of her kidneys.\nChinaâ€™s health care system is rife with severe inequalities. The nationâ€™s top doctors work out of dozens of prestigious public hospitals, most of them located in the economically developed eastern and southern regions. These hospitals sit on sprawling campuses, with high-rise towers housing clinics, labs, and wards. The largest facilities have thousands of beds. Itâ€™s common for patients with severe conditions to travel long distances, sometimes across the entire country, to seek treatment at these hospitals. Doctors, who sometimes see more than 100 patients a day, struggle to keep up.\nAlthough the hospitals are public, they largely operate as businesses, with only about 10% of their budgets coming from the government. Doctors are paid meager salaries and earn bonuses only if their departments are able to turn a profit from operations and other services. Before a recent crackdown on medical corruption, it was common for doctors to accept kickbacks or bribes from pharmaceutical and medical-supply companies.\nAs Chinaâ€™s population ages, strains on the countryâ€™s health care system have gotten only more intense, and the systemâ€™s failures have led to widespread distrust of medical professionals. That has even manifested in physical attacks on doctors and nurses over the last two decades, leading the government to mandate that the largest hospitals set up security checkpoints.\nOver my eight years with my mom in Hangzhou, I became accustomed to the tense, overstretched environment of Chinese hospitals. But as I got older, I spent less and less time with her. I attended a boarding school at 14, returning home only once a week. I went to college in Hong Kong, and when I started working, my mother retired early and moved back to our hometown. Thatâ€™s when she started taking her two-day trips to see the nephrologist back in Hangzhou. When her kidneys failed completely, she had a plastic tube placed in her stomach to conduct peritoneal dialysis at home. In 2020, fortunately, she received a kidney transplant.\nIt was only partially successful, though, and she suffers from a host of complications, including malnutrition, borderline diabetes, and difficulty sleeping. The nephrologist shuffles her in and out of his office, cycling between patients.\nHer relationship with my father also became more strained, and three years ago, they split up. I moved to New York City. Whenever she brings up her sickness during our semi-regular calls, I donâ€™t know what to say, except to suggest she see a doctor soon.\nWhen my mother was first diagnosed with kidney disease in the 2000s, she would look up guidance on Baidu, Chinaâ€™s dominant search engine. Baidu was later embroiled in a series of medical ad scandals, including one over the death of a college student whoâ€™d tried unproven therapies he found through a sponsored link. Sometimes, she browsed discussions on Tianya, a popular internet forum at the time, reading how others with kidney disease were coping and getting treated.\nLater, like many Chinese, she turned to social media platforms such as WeChat, Douyin, Zhihu, and XiaohongshuiXiaohongshuXiaohongshu, which translates to â€œlittle red bookâ€ in Chinese, is a lifestyle e-commerce and social media platform.READ MORE for health information. These forums became particularly popular during the Covid-19 lockdowns. Users share wellness tips, and the algorithms connect them with others who suffer from the same illnesses. Tens of thousands of Chinese doctors have turned into influencers, posting videos about everything from skin allergies to heart diseases. Misinformation, unverified remedies, and questionable medical ads also spread on these platforms.\nMy mother picked up obscure dietary advice from influencers on WeChat. Unprompted, Baiduâ€™s algorithm fed her articles about diabetes. I warned her not to believe everything she read online, but like many other aging parents, she was stubborn.\nThe rise of AI chatbots has opened a new chapter in online medical advice. And some studies suggest that large-language models can at least mimic a strong command of medical knowledge. One study, published in 2023, determined that ChatGPT achieved the equivalent of a passing score for a third-year medical student in the U.S. Medical Licensing Examination. Last year, Google said its fine-tuned Med-Gemini models did even better on a similar benchmark, while a specialized model trained on Metaâ€™s Llama likewise excelled in medical exams.\nResearch on tasks that more closely mirror daily clinical practice, such as diagnosing illnesses, is tantalizing to AI advocates. In one 2024 study, published as a preprint and not yet peer reviewed, researchers fed clinical data from a real emergency room to OpenAIâ€™s GPT-4o and o1 and found they both outperformed physicians in making diagnoses. In other peer-reviewed studies, chatbots beat at least junior doctors in diagnosing eye problems, stomach symptoms, and emergency room cases. In June, Microsoft claimed it had built an AI-powered system that could diagnose cases four times more accurately than physicians, creating a â€œpath to medical superintelligence.â€ Of course, researchers are also flagging risks of biases and hallucinations that could lead to incorrect diagnoses, mistreatments, and deeper health care disparities.\nAs Chinese LLM companies rushed to catch up with their U.S. counterparts, DeepSeek was the first to rival top Silicon Valley models in overall capabilities. It has performed well on medical tests too. In one recent study, researchers found that DeepSeekâ€™s R1 performed similarly or better than OpenAIâ€™s o1 in some medical tasks, such as diagnostic reasoning. Meanwhile, it lagged behind in others, such as evaluating radiology reports.\nIgnoring some of the limitations, users in the U.S. and China are turning to these chatbots regularly for medical advice. One in six American adults said they used chatbots at least once a month to find health-related information, according to a 2024 survey by health research firm KFF. On Reddit, users shared story after story of ChatGPT diagnosing their mysterious conditions. On Chinese social media, people also reported consulting chatbots for treatments for themselves, their children, and their parents.\nAn electronics factory worker in Jiangsu province, who declined to be named for privacy reasons, told me he consulted three different chatbots after his mother was diagnosed with uterine cancer, just to check if her doctor was right in telling her not to worry. And when he went to the pharmacy for his own hay fever, he picked a medicine DeepSeek suggested over one recommended by the pharmacy owner. â€œ[Owners] always recommend the most expensive ones,â€ he said.\nReal Kuang, a photographer in the city of Chengdu, asks DeepSeek about her parentsâ€™ health issues: how to treat her fatherâ€™s throat inflammation, whether they should take calcium supplements, if her mother should get shoulder surgery. â€œHuman doctors are not as patient or generous with details and the thought process,â€ Kuang told me. â€œDeepSeek made us feel more cared for.â€\nMy mother has told me that whenever she steps into her nephrologistâ€™s office, she feels like a schoolgirl waiting to be scolded. She fears annoying the doctor with her questions. She also suspects that the doctor values the number of patients and earnings from prescriptions over her well-being.\nBut in the office of Dr. DeepSeek, she is at ease.\nâ€œDeepSeek makes me feel like an equal,â€ she said. â€œI get to lead the conversation and ask whatever I want. It lets me get to the bottom of everything.â€\nSince she began to engage with it in early February, my mother has reported anything and everything to the AI: changes in her kidney functions and glucose levels, a numb finger, blurry vision, the blood oxygen levels recorded on her Apple watch, coughing, a dizzy feeling after waking up. She asks for advice on food, supplements, and medicines.\nâ€œAre pecans right for me?â€ she asked in April. DeepSeek analyzed the nutâ€™s nutritional composition, flagged potential health risks, and offered portion recommendations.\nâ€œHere is an ultrasound report of my transplanted kidney,â€ she typed, uploading the document. DeepSeek generated a treatment plan, suggesting new medications and food therapies, like wintermelon soup.\nâ€œIâ€™m 57, post-kidney transplantation. I take tacrolimus [an immunosuppressant] at 9 a.m. and 9 p.m. My weight is 39.5 kg. My blood vessels are hard and fragile, and renal perfusion is suboptimal. This is todayâ€™s diet. Please help analyze the energy and nutritional composition. Thank you!â€ She then listed everything sheâ€™d eaten on that day. DeepSeek suggested she reduce her protein intake and add more fiber.\nTo every question, it responds confidently, with a mix of bullet points, emojis, tables, and flow charts. If my mother said thank you, it added little encouragement.\nâ€œYou are not alone.â€\nâ€œIâ€™m so happy with your improvement!â€\nSometimes, it closes with an emoji of a star or cherry blossom.\nâ€œDeepSeek is so much better than doctors,â€ she texted me one day.\nMy motherâ€™s reliance on DeepSeek grew over the months. Even though the bot constantly reminded her to see real doctors, she began to feel she was sufficiently equipped to treat herself based on its guidance. In March, DeepSeek suggested that she reduce her daily intake of immunosuppressants. She did. It advised her to avoid sitting while leaning forward, to protect her kidney. She sat straighter. Then, it recommended lotus root starch and green tea extract. She bought them both.\nIn April, my mother asked DeepSeek how much longer her new kidney would last. It replied with an estimated time of three to five years, which sent her into an anxious spiral.\nWith her consent, I shared excerpts of her conversations with DeepSeek with two U.S.-based nephrologists.\nDeepSeekâ€™s answers, according to the doctors, were full of errors. Dr. Joel Topf, a nephrologist and associate clinical professor of medicine at Oakland University in Michigan, told me that one of its suggestions to treat her anemia â€” using a hormone called erythropoietin â€”Â could increase the risks of cancer and other complications. Several other treatments DeepSeek suggested to improve kidney functions were unproven, potentially harmful, unnecessary, or a â€œkind of fantasy,â€ Topf told me.\nI asked how he would have answered her question about how long her kidney will survive. â€œI am usually less specific,â€ he said. â€œInstead of telling people how long theyâ€™ve got, we talk about the fraction that will be on dialysis in two or five years.â€\nDr. Melanie Hoenig, an associate professor at Harvard Medical School and nephrologist at the Beth Israel Deaconess Medical Center in Boston, told me that DeepSeekâ€™s dietary suggestions seem more or less reasonable. But she said DeepSeek had suggested completely wrong blood tests and mixed up my motherâ€™s original diagnosis with another very rare kidney disease.\nâ€œIt is sort of gibberish, frankly,â€ Hoenig said. â€œFor someone who does not know â€“â€“ it would be hard to know which parts were hallucinations and which are legitimate suggestions.â€\nResearchers have found that chatbotsâ€™ competence on medical exams do not necessarily translate into the real world. In exam questions, symptoms are clearly laid out. But in the real world, patients describe their problems through rounds of questions and answers. They often donâ€™t know which symptoms are relevant and rarely use the correct medical terminology. Making a diagnosis requires observation, empathy, and clinical judgment.\nIn a study published in Nature Medicine earlier this year, researchers designed an AI agent that acts as a pseudo patient and simulates how humans speak, using it to test LLMsâ€™ clinical capabilities across 12 specialties. All the LLMs did much worse than how they performed in exams. Shreya Johri, a Ph.D. student at Harvard Medical School and a lead author of the study, told Rest of World that the AI models were not very good at asking questions. They also lagged in connecting the dots when someoneâ€™s medical history or symptoms were scattered across rounds of dialogues. â€œItâ€™s important that people treat it with a pinch of salt,â€ Johri said of the LLMs.\nIn another study led by researchers at Oxford University, published as a preprint and not yet peer reviewed, members of the general public were asked to identify health conditions and a subsequent course of action using either large language models or conventional methods, such as search engines and checking the National Health Service website. Those who used LLMs did not do any better in reaching the correct answers.\nAndrew Bean, a doctoral candidate at Oxford and the lead author of the study, told me that during the experiment, users omitted important symptoms in their prompts or failed to identify the correct answer when the chatbot suggested a few different options. Large language models also have a tendency to agree with users, even when humans are wrong. â€œThere are certainly a lot of risks that come with not having experts in the loop,â€ he said.\nAs my mother bonded with DeepSeek, health care providers across China embraced large language models.\nSince the release of DeepSeek R1 in January, hundreds of hospitals have incorporated the model into their processes. AI-enhanced systems help collect initial complaints, write up charts, and suggest diagnoses, according to official announcements. Partnering with tech companies, large hospitals use patient data to train their own specialized models. One hospital in Sichuan province introduced â€œDeepJoint,â€ a model for orthopaedics that analyzes CT or MRI scans to generate surgical plans. A hospital in Beijing developed â€œStone Chat AI,â€ which answers patientsâ€™ questions about urinary tract stones.\nIn the past, one doctor could only work in one clinic. Now, one doctor may be able to run two or three clinics at the same time.â€\nThe tech industry now views health care as one of the most promising frontiers for AI applications. DeepSeek itself has begun recruiting interns to annotate medical data, in order to improve its modelsâ€™ medical knowledge and reduce hallucinations. Alibaba announced in May that its health careâ€“focused chatbot, trained on top of its Qwen models, passed Chinaâ€™s medical qualification exams across 12 disciplines. Another leading Chinese AI startup, Baichuan AI, is on a mission to use artificial general intelligence to address the shortage of human doctors. â€œWhen we can create a doctor, thatâ€™s when we have achieved AGI,â€ its founder Wang Xiaochuan told a Chinese outlet. Baichuan AI declined my interview request.\nRudimentary â€œAI doctorsâ€ are popping up in the countryâ€™s most popular apps. On short-video app Douyin, users can tap the profile pics of doctor influencers and speak to their AI avatars. Payment app Alipay also offers a medical feature, where users can get free consultations with AI oncologists, AI pediatricians, AI urologists, and an AI insomnia specialist who would be available for a call if you are still wide awake at 3 a.m. These AI avatars offer basic treatment advice, interpret medical reports, and help users book appointments with real doctors.\nDr. Tian Jishun, a gynecologist in Hangzhou, agreed to lend his persona to Alipay as the company built up its fleet of 200 AI doctors. Tian told me he wanted to be part of the AI revolution, although he admits his digital counterpart is lacking. â€œItâ€™s like the first iPhone,â€ he told me. â€œYou never know what the future will be like.â€\nZhang Chao, founder of AI health care startup Zuoshou Yisheng, developed an AI primary care doctor on top of Alibabaâ€™s Qwen models. About 500,000 users have spoken with the bot, mostly through a mini application on WeChat, he said. People have inquired about minor skin conditions, their childrenâ€™s illnesses, or sexually transmitted diseases.\nChina has banned â€œAI doctorsâ€ from generating prescriptions, but there is little regulatory oversight on what they say. Companies are left to make their own ethical decisions. Zhang, for example, has banned his bot from addressing questions about childrenâ€™s drug use. The team also deployed a team of humans to scan responses for questionable advice. Zhang said he was overall confident with the botâ€™s performance. â€œThereâ€™s no correct answer when it comes to medicine,â€ Zhang said. â€œItâ€™s all about how much itâ€™s able to help the users.â€\nAI doctors are also coming to offline clinics. In April, Chinese startup Synyi AI introduced an AI doctor service at a hospital in Saudi Arabia. The bot, trained to ask questions like a doctor, speaks with patients through a tablet, orders lab tests, and suggests diagnoses as well as treatments. A human doctor then reviews the suggestions. Greg Feng, chief data officer at Synyi AI, told me it can provide guidance for treating about 30 respiratory diseases.\nFeng said that the AI is more attentive and compassionate than humans. It can switch genders to make the patient more comfortable. And unlike human doctors, it can address patientsâ€™ questions for as long as they want. Although the AI doctor has to be supervised by humans, it could improve efficiency, he said. â€œIn the past, one doctor could only work in one clinic,â€ Feng said. â€œNow, one doctor may be able to run two or three clinics at the same time.â€\nEntrepreneurs claim that AI can solve problems in health care access, such as the overcrowding of hospitals, the shortage of medical staff, and the ruralâ€“urban gap in quality care. Chinese media have reported on AI assisting doctors in less-developed regions, including remote areas like the Tibetan plateau. â€œIn the future, residents of small cities might be able to enjoy better health care and education, thanks to AI models,â€ Wei Lijia, a professor in economics at Wuhan University, told me. His study, recently published in the Journal of Health Economics, found that AI assistance can curb overtreatment and enhance physiciansâ€™ performance in medical fields beyond their specialty. â€œYour mother,â€ he said, â€œwould not need to travel to the big cities to get treated.â€\nOther researchers have raised concerns related to consent, accountability, and biases that could actually exacerbate health care disparities. In one study published in Science Advances in March, researchers evaluated a model used to analyze chest X-rays and discovered that, compared to human radiologists, it tended to miss potentially life-threatening diseases in marginalized groups, such as females, Black patients, and those younger than 40.\nâ€œI want to be very cautious in saying that AI will help reduce the health disparity in China or in other parts of the world,â€ said Lu Tang, a professor of communication at Texas A&M University who studies medical AI ethics. â€œThe AI models developed in Beijing or Shanghai â€¦ might not work very well for a peasant in a small mountain village.â€\nWhen I called my mother and told her what the American nephrologists had said about DeepSeekâ€™s mistakes, she said she was aware that DeepSeek had given her contradictory advice. She understood that chatbots were trained on data from across the internet, she told me, and did not represent an absolute truth or superhuman authority. She had stopped eating the lotus seed starch it had recommended.\nBut the care she gets from DeepSeek also goes beyond medical knowledge: itâ€™s the chatbotâ€™s steady presence that comforts her.\nI remembered asking why she didnâ€™t direct another type of question she often puts to DeepSeek â€” about English grammar â€” to me. â€œYou would find me annoying for sure,â€ she replied. â€œBut DeepSeek would say, â€˜Letâ€™s talk more about this.â€™ It makes me really happy.â€\nMy one-child policy generation has grown up, and our parents are joining Chinaâ€™s rapidly growing elderly population. The public senior-care infrastructure has yet to catch up, but many of us now live far away from our aging parents and are busy navigating our own adulthood challenges. Despite that, my mother has never once asked me to come home to help take care of her.\nShe understands what it means for a woman to move away from home and step into the larger world. In the 1980s, she did just that â€” leaving her rural family, where she cooked and did laundry for her parents and younger brother, to attend a teacher training school. She respects my independence, sometimes to an extreme. I call my mother once every week or two. She almost never calls me, afraid she will catch me at a bad time, when Iâ€™m working or hanging out with friends.\nBut even the most understanding parents need someone to lean on. A friend my age in Washington, D.C., who also immigrated from China, recently discovered her own motherâ€™s bond with DeepSeek. Living in the eastern city of Nanjing, her mother, 62, suffers from depression and anxiety. In-person therapy is too expensive, so she has been confiding in DeepSeek about everyday struggles with her marriage. DeepSeek responds with detailed analyses and to-do lists.\nâ€œI called her daily when my mother was very depressed and anxious. But for young people like us, itâ€™s hard to keep up,â€ my friend told me. â€œThe good thing about AI is she can say what she wants at any moment. She doesnâ€™t need to think about the time difference or wait for me to text back.â€\nI called her daily when my mother was very depressed and anxious. But for young people like us, itâ€™s hard to keep up,â€ my friend told me.\nZhang Jiansheng, a 36-year-old entrepreneur, created an AI-powered tablet that can speak to people with Alzheimerâ€™s disease. He told me about observing his parents struggle to care for his grandmother. Itâ€™s hard not to get irritated by the behavioral changes of an Alzheimerâ€™s patient, he explained, but AI is patient. â€œAI has no emotions,â€ he said. â€œIt will keep offering encouragement, praise, and comfort to the elderly.â€\nMy mother still turns to DeepSeek when she gets worried about her health. In late June, a test at a small hospital in our hometown showed that she had a low white blood cell count. She reported it to DeepSeek, which suggested follow-up tests. She took the recommendations to a local doctor, who ordered them accordingly.\nThe next day, we got on a call. It was my 8 p.m. and her 8 a.m. I told her to see the nephrologist in Hangzhou as soon as possible.\nShe refused, insisting she was fine with Dr. DeepSeek. â€œItâ€™s so crowded there,â€ she said, raising her voice. â€œThinking about that hospital gives me a headache.â€\nShe eventually agreed to see the doctor. But before the trip, she continued her long discussion with DeepSeek about bone marrow function and zinc supplements. â€œDeepSeek has information from all over the world,â€ she argued. â€œIt gives me all the possibilities and options. And I get to choose.â€\nI thought back to a conversation weâ€™d had earlier about DeepSeek. â€œWhen Iâ€™m confused, and I have no one to ask, no one I can trust, I go to it for answers,â€ sheâ€™d told me. â€œI donâ€™t have to spend money. I donâ€™t have to wait in line. I donâ€™t have to do anything.â€\nShe added, â€œEven though it canâ€™t give me a fully comprehensive or scientific answer, at least it gives me an answer.â€",
      "publish_datetime": "2026-01-29T19:01:38.024303Z",
      "scraping_timestamp": "2026-01-30T09:01:38.038504Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI",
      "points": 180,
      "num_comments": 94,
      "engagement_score": 368.0
    },
    {
      "link": "https://davidoks.blog/p/a-lot-of-population-numbers-are-fake",
      "author": "David Oks",
      "title": "A lot of population numbers are fake - David Oks",
      "source": "hackernews",
      "content": "Hereâ€™s the story of a remarkable scandal from a few years ago.In the South Pacific, just north of Australia, there is a small, impoverished, and remote country called Papua New Guinea. Itâ€™s a country that Iâ€™ve always found absolutely fascinating. If thereâ€™s any outpost of true remoteness in the world, I think itâ€™s either in the outer mountains of Afghanistan, in the deepest jungles of central Africa, or in the highlands of Papua New Guinea. (PNG, we call it.) Hereâ€™s my favorite fact: Papua New Guinea, with about 0.1 percent of the worldâ€™s population, hosts more than 10 percent of the worldâ€™s languages. Two villages, separated perhaps only by a few miles, will speak languages that are not mutually intelligible. And if you go into rural PNG, far into rural PNG, youâ€™ll find yourself in places that time forgot.But hereâ€™s a question about Papua New Guinea: how many people live there?The answer should be pretty simple. National governments are supposed to provide annual estimates for their populations. And the PNG government does just that. In 2022, it said that there were 9.4 million people in Papua New Guinea. So 9.4 million people was the official number.But how did the PNG government reach that number?The PNG government conducts a census about every ten years. When the PNG government provided its 2022 estimate, the previous census had been done in 2011. But that census was a disaster, and the PNG government didnâ€™t consider its own findings credible. So the PNG government took the 2000 census, which found that the country had 5.5 million people, and worked off of that one. So the 2022 population estimate was an extrapolation from the 2000 census, and the number that the PNG government arrived at was 9.4 million.But this, even the PNG government would admit, was a hazy guess.About 80 percent of people in Papua New Guinea live in the countryside. And this is not a countryside of flat plains and paved roads: PNG is a country of mountain highlands and remote islands. Many places, probably most places, donâ€™t have roads leading to them; and the roads that do exist are almost never paved. People speak different languages and have little trust in the central government, which simply isnâ€™t a force in most of the country. So traveling across PNG is extraordinarily treacherous. Itâ€™s not a country where you can send people to survey the countryside with much ease. And so the PNG government really had no idea how many people lived in the country.Late in 2022, word leaked of a report that the UN had commissioned. The report found that PNGâ€™s population was not 9.4 million people, as the government maintained, but closer to 17 million peopleâ€”roughly double the official number. Researchers had used satellite imagery and household surveys to find that the population in rural areas had been dramatically undercounted.This was a huge embarrassment for the PNG government. It suggested, first of all, that they were completely incompetent and had no idea what was going on in the country that they claimed to govern. And it also meant that all the economic statistics about PNGâ€”which presented a fairly happy pictureâ€”were entirely false. Papua New Guinea had been ranked as a â€œlower-middle incomeâ€ country, along with India and Egypt; but if the report was correct then it was simply a â€œlower-incomeâ€ country, like Afghanistan or Mali. Any economic progress that the government could have cited was instantly wiped away.But it wasnâ€™t as though the government could point to census figures of its own. So the countryâ€™s prime minister had to admit that he didnâ€™t know what the population was: he didnâ€™t know, he said, whether the population is â€œ17 million, or 13 million, or 10 million.â€ It basically didnâ€™t matter, he said, because no matter what the population was, â€œI cannot adequately educate, provide health cover, build infrastructures and create the enabling law and order environmentâ€ for the countryâ€™s people to succeed.But in the end, the PNG government won out. To preserve its dignity, it issued a gag order on the report, which has still never been released. There was some obscure behind-the-scenes bureaucratic wrangling, and in 2023 the UN shelved the report and agreed with the PNG governmentâ€™s existing estimate. And so today, PNG officially has approximately 10 million people, perfectly in line with what had been estimated before.The truth, of course, is that we have no idea how many people live in Papua New Guinea.Last week, someone calling themselves Bonesaw went viral on Twitter for a post that claimed that Chinaâ€™s population numbers were entirely fake. China, they said, had been lying about its population for decades: it actually had only about 500 million people. In fact practically every non-Western country had been lying about its population. Indiaâ€™s numbers were also badly exaggerated: the idea that there are 1.5 billion Indians was absurd. The true population of the world, Bonesaw said, was significantly less than 1 billion people.This is obviously an extremely stupid idea. Itâ€™s possible that Chinese population numbers are mildly exaggerated, but the most credible estimatesâ€”the ones advanced by Yi Fuxianâ€”are that the exaggeration is on the order of a few percentage points. (Itâ€™s also worth noting that no reputable source has yet backed Yi Fuxianâ€™s theory.) Actually faking the existence of billions of people would require a global conspiracy orders of magnitude more complex than anything in human history. Tens or hundreds of thousands of people, spread across every country in the world, would have to be in on it. Local, regional, and national governments would all have to be involved; also the UN, the World Bank, the IMF, every satellite company, every NGO that does work in any of these places. Every election would have to be fake. Every government database would have to be full of fake names. And all for what? To get one over on the dumb Westerners? So we can dismiss Bonesawâ€™s claim pretty easily. But, as much as I hate to admit it, his argument does have a kernel of truth. And that kernel of truth is this: we simply have no idea how many people live in many of the worldâ€™s countries.This is not the case for most countries, of course. In wealthy countries, like Germany or Japan or Sweden, populations are generally trusting and bureaucracies are generally capable. Sweden, for its part, maintains such an accurate daily birth-and-death count of population numbers that it no longer even needs to conduct a census. And population numbers are also not so much of a problem in countries like China, India, or Vietnam. These places might be poorer, but they have strong central governments that have a strong interest in knowing whatâ€™s going on inside the country. Population counts might be slightly overstated in these places because fertility is falling faster than expected (which could be the case in a country like India where fertility rates are falling quickly), or because local officials are exaggerating the number of students in their schools to secure more education subsidies (thatâ€™s Yi Fuxianâ€™s theory of population counts in China), or because more people have emigrated than expected (as was the case in Paraguay when a census revealed its population to be smaller than officials expected). But if the state is in full control of a country, it will want to know whatâ€™s going on inside that country; and that starts with the simple fact of knowing how many people live there.But â€œthe state being in full control of a countryâ€ is not a criterion that holds in much of the world. Which brings us to Nigeria.Nigeria is a huge place. Officially, itâ€™s a nation of 240 million people, which would make it the most populous country in Africa and the sixth most populous country in the world. And without a doubt, there are a lot of people in Nigeria. But we actually have no idea how many there are.Like PNG, Nigeria is supposed to conduct a census every 10 years. But in Nigeria, the census is a politically fraught thing. Nigeria is not a natural polity, and its ongoing unity as a single country is fragile. And so Nigerian elites expend enormous effort to ensure that Nigeria remains one country. They have two important tools at their disposal. The first is the relative representation of different regions in the Nigerian state. And the second is the distribution of Nigeriaâ€™s vast oil revenues. Both of theseâ€”how many seats a state is given in the Nigerian parliament, and how large a share of oil revenues it receivesâ€”are determined by its share of the population.So local elites have a strong incentive to exaggerate the number of people in their region, in order to secure more oil revenue, while national elites have a strong incentive to balance populations across states in order to maintain the precarious balance of power between different regions. And so the overwhelming bias in Nigerian population counts is toward extremely blatant fraud.Itâ€™s long been the case that censuses in Nigeria are shoddy affairs. When Nigeria was a colony of Britain, its censuses were limited to Lagos, a few townships, and a small number of villages: so the 1931 census for Nigeria yielded numbers that were too low by as much as 75 percent. Once Nigeria became independent, in 1960, the bias swung from underestimation to overestimation. Nigeriaâ€™s first census as an independent state came in 1962, and it immediately caused a political problem: the ruling regime was dominated by northern elites, but the census found that southern Nigeria had more people. And so another census was ordered the next year, which conveniently found an extra eight million people in the north. This pattern of brazenly false numbers continued for decades. The next census, in 1973, was such an obvious fraud that the government opted not even to publish the results. For eighteen years after that there was no attempt even to conduct a census. The next census, in 1991, was by far the most credible, and it shocked many people by finding that the population was about 30 percent smaller than estimated. But even that one was riddled with fraud. Many states reported that every single household had exactly nine people. In 2006, Nigeria tried once again to count its population. And as luck would have it, it found that since the last census each stateâ€™s proportion of the national population had remained exactly the same: so there was no need to change the composition of the Nigerian parliament or the distribution of oil revenues. But this census was an extremely rocky affair. The city of Lagos, for instance, rejected the results of the census, which it claimed undercounted its population in order to preserve northern power; so it conducted its own (technically illegal) census and found that it had eight million more people than the national census had reckoned. And there was also a good deal of violence that accompanied the census: about ten people were killed in clashes around the census, usually in regions with separatist activity. The whole experience was so difficult that Nigeria has opted not to repeat it. The 2006 census was the last time that Nigeria has tried to count how many people live in the country.So the Nigerian governmentâ€™s figure of 240 million people is, as is the case in Papua New Guinea, an extrapolation from a long-ago census figure. Is it credible? Very few people think so. Even the head of Nigeriaâ€™s population commission doesnâ€™t believe that the 2006 census was trustworthy, and indeed said that â€œno census has been credible in Nigeria since 1816.â€ (Nigeriaâ€™s president fired him shortly thereafter.) There are plenty of reasons to think that Nigeriaâ€™s population might be overstated. It would explain, for instance, why in so many ways there appear to be tens of millions of missing Nigerians: why so few Nigerians have registered for national identification numbers, or why Nigerian voter turnout is so much lower than voter turnout in nearby African nations (typically in the 20s or low 30s, compared to the 50s or 60s for Ghana, Cameroon, or Burkina Faso), or why SIM card registration is so low, or why Nigerian fertility rates have apparently been dropping so much faster than demographers expected.None of this evidence is conclusive, of course. (There are credible third partiesâ€”like the Against Malaria Foundationâ€”that believe that Nigerian population counts might actually be understated.) But the crucial thing, as in Papua New Guinea, is that we donâ€™t know how many people live in Nigeria. It might be that there are 240 million Nigerians, as the Nigerian government claims; or that there are 260 million Nigerians; or that there are only 180 million. We donâ€™t know. But we have plenty of reason to think that the official numbers have little relationship to reality.What about other countries?Nigeria is not the only poor country with an extremely patchy history of censuses. Indeed we find that countless poor nations with weak states have only the vaguest idea how many people they govern. The Democratic Republic of the Congo, which by most estimates has the fourth-largest population in Africa, has not conducted a census since 1984. Neither South Sudan nor Eritrea, two of the newest states in Africa (one created in 2011 and the other in 1991), has conducted a census in their entire history as independent states. Afghanistan has not had one since 1979; Chad since 1991; Somalia since 1975.The various bodies that interest themselves in national populations, from the World Bank to the CIA, reliably publish population numbers for each of these countries. But without grounding in trustworthy census data, we simply have no idea if the numbers are real or not. Estimates for Eritreaâ€™s population vary by a factor of two. Afghanistan could have anywhere between 38 and 50 million people. Estimates for the DRCâ€™s 2020 population range from 73 million to 104 million. How did the country reach its official number for that year, 94.9 million? We have no idea. â€œIt is unclear how the DRC national statistical office derived its estimate,â€ the U.S. Census Bureau said, â€œas there is no information in its 2020 statistical yearbook.â€Many other countries do conduct more regular censuses, but do a terrible job of it. Enumerators are hired cheaply and do a bad job, or they quit halfway through, or they go unpaid and just refuse to submit their data. An unknowable number simply submit fake numbers. These are not, after all, technical experts or trained professionals; they are random people sent into remote places, often with extremely poor infrastructure, and charged with determining how many people live there. It is exceptionally difficult to do that and come out with an accurate answer.So even those countries that do conduct regular or semi-regular censuses often arrive at inaccurate results. The most recent South African census, for instance, undercounted the population by as much as 31 percentâ€”and that is one of the wealthier and better-run nations in Africa. In poorer and less functional countries, statistical capacity is often just nonexistent. Take, for instance, the testimony of the former director of Sudanâ€™s statistical bureau, who said that the most accurate census in Sudanâ€™s history was conducted in 1956, when the country was still under British rule. It shouldnâ€™t be new to anyone that population data in the poor world is bad. Weâ€™ve known about these problems for a long time. And for an equally long time, weâ€™ve had a preferred solution in mind. Technology can compensate for the deterioration of human coordination: we have satellites.Satellites have two great benefits for counting populations. First, satellites can see pretty much any part of the world from space, and so you entirely obviate the logistical problem of sending people into remote areas: all you need is a small count of some portion of the area under study, which you can use to ground your estimates in something like reality. And second, you donâ€™t have to rely on local governments to obtain the dataâ€”so you can get away from the bad incentives of, say, Nigerian elites.But satellite data can only tell us so much. A satellite can look at a house, but it canâ€™t determine whether three people live there, or six people, or eight people. And often the problem is worse than that. Sometimes a satellite canâ€™t tell whatâ€™s a building and whatâ€™s a feature of the landscape. Dense cities are a problem; and so, by the way, are junglesâ€”satellites canâ€™t penetrate thick forest cover, and there are quite a few people around the world who still live in forests. (The â€œforest peopleâ€ of central Africa, for instance, or a few million of the Adivasi in India.)So guessing population numbers from high-resolution satellite imagery is an extraordinarily difficult problem. The various companies that guess population numbers from satellite imageryâ€”working with groups like the World Health Organization that might be interested in mapping, say, malaria casesâ€”take different approaches to tackling this problem. And the different approaches they take can lead to wildly different results. For example: Meta and WorldPop both used satellite imagery to predict the population of the city of Bauchi, in northeastern Nigeria. But the numbers that they reached were entirely different, because they take different approaches: Meta uses a deep learning model to detect individual buildings in images and then distributes population proportionally across those structures, while WorldPop feeds a machine-learning model with dozens of variables (land cover, elevation, road networks, so on) and uses that to predict population. Meta guessed that Bauchi has 127,000 children under the age of five; WorldPop says that it has 254,000, twice as many. So Metaâ€™s estimate is about 50 percent lower than WorldPopâ€™s. We see similar differences in other regions. Meta says that Ganjuwa, also in northeastern Nigeria, has 76,000 children under the age of five; WorldPop says that it has 162,000.And when we do have ground-truth data, we tend to find that satellite-based data doesnâ€™t perform much better. Last year, three Finnish scientists published a study in Nature looking at satellite-based population estimates for rural areas that were cleared for the construction of dams. This was a useful test for the satellite data, because in resettling the people of those areas local officials were required to count the local population in a careful way (since resettlement counts determine compensation payments), and those counts could be compared to the satellite estimates. And again and again, the Finnish scientists found that the satellite data badly undercounted the number of people who lived in these areas. The European Commissionâ€™s GSH-POP satellite tool undercounted populations by 84 percent; WorldPop, the best performer, still underestimated rural populations by 53 percent. The pattern held worldwide, with particularly large discrepancies in China, Brazil, Australia, Poland, and Colombia. Nor is it just rural areas being resettled: WorldPop and Meta estimated slums in Nigeria and Kenya to be a third of their actual size.(All of this, by the way, is a good reason to think that the report that the UN commissioned on Papua New Guineaâ€™s population is probably inaccurate. And indeed, when the PNG government conducted a new census in 2024, its results broadly supported its own numbers. But we are not out of the woods yet: that census was also riddled with accusations of severe undercounting. So again we must return to the central fact: we just donâ€™t know how many people live in Papua New Guinea.)So satellite data is not a panacea. It might be that in the future the tools advance to the point where they can produce reliable estimates of human populations in areas of arbitrary size. But we are not really close to that point. Where does that leave us?I donâ€™t think thereâ€™s any reason to embrace the sort of idiotic conspiracism of Bonesaw. We simply have no reason to think that the number of people in the world is dramatically different from what official estimates indicate; indeed while there are specific cases where the numbers might be dramatically off, thereâ€™s just no reason to think that this is the case for every country. There are many places, like perhaps Papua New Guinea, where population counts are probably too low. The only thing that can be said with any reliability is that we simply donâ€™t know how many people live in these countries.Given that we donâ€™t have much evidence of a systematic bias in population countsâ€”Nigeria might overcount, but Sudan might undercount, and at scale these differences should cancel outâ€”the best we can do is assume that there is a sort of â€œlaw of large numbersâ€ for population counts: the more units we have under consideration, the more closely the numbers should hew to reality. So population counts for individual countries, particularly in Africa, are probably badly inaccurate. It wouldnâ€™t be surprising if the total population for Africa is off-base by some amount. But we donâ€™t have much reason to think that the global population is very different from what we believe it to be.But itâ€™s good to be reminded that we know a lot less about the world than we think. Much of our thinking about the world runs on a statistical edifice of extraordinary complexity, in which raw numbersâ€”like population counts, but also many othersâ€”are only the most basic inputs. Thinking about the actual construction of these numbers is important, because it encourages us to have a healthy degree of epistemic humility about the world: we really know much less than we think.",
      "publish_datetime": "2026-01-29T14:01:48.017872Z",
      "scraping_timestamp": "2026-01-30T09:01:48.027398Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 334,
      "num_comments": 277,
      "engagement_score": 888.0
    },
    {
      "link": "https://softwarefordays.com/post/software-is-mostly-all-you-need/",
      "author": null,
      "title": "Software is Mostly All You Need",
      "source": "hackernews",
      "content": "January\n8, 2026\nSoftware is Mostly All You Need\nNeural Networks at Buildtime, Software at Runtime\nOver the last 6 months and the last 6 weeks in particular, AI coding agents have shown to be incredibly capable at writing software. Tasks that traditionally required weeks of human labor can now be done in days if not hours. Even more incredibly, software systems that are designed from the start to harness AI coding agents exhibit many of the characteristics of the neural nets that were integral to their creation in the first place. These AI-native software systems are learned, not designed. Code is the policy, deployment is the episode, and the bug report is the reward signal - well-architected coding agents can drive this loop with little human intervention. Unlike traditional reinforcement learning architectures, they are encoded in CPU instruction sets instead of neural network weights, but they are learned just the same.\nThe success of coding agents and the software systems built thereon carry lessons about where to apply AI agents in general as well. Coding, like many other creative tasks, requires judgment. How best to implement some function with input A and output B; how to name some variable; whether to share some function or implement a new version; etc. Neural networks excel at judgment (more on why below). Yet many of the agentic deployments we are seeing in the wild are against tasks that can be fully specified as explicit instructions. Of course, traditional software excels at executing explicit instructions. Any programming language can be executed on todayâ€™s machinery at billions of instructions per second.\nCoding agents get this exactly right, since by definition they are making a series of judgments when writing code at buildtime and leaving the execution of such code to machines operating at runtime. The best performing architectures follow suit, delegating judgment to neural networks and execution to traditional software, even when the executable artifacts are produced entirely by AI.\nSome Agents in Practice #\nMany agentic AI projects are failing â€” agentic drift, opaque debugging, brittle autonomy. Meanwhile, Claude Code has driven significant productivity gains by doing something different: it writes code that humans review and deploy, producing artifacts that are durable, version-controlled, and deterministic.\nThese failures and successes reflect a fundamental architectural difference.\nJudgment and Execution Historically #\nHumans have historically done two different types of jobs for different reasons, and AI changes each differently.\nJudgment is fuzzy classification that cannot be specified as explicit rules. This variable should be made private, not public; this handwritten letter is a â€œBâ€, not a â€œPâ€; this customer complaint is about a refund, not fraud; this image contains a receipt; this element on some unfamiliar page is â€œthe login button.â€ Humans did these tasks because traditional CPU-based Von Neumann machines simply could not. The rules could not be written down, and even today exist only as learned boundaries in high-dimensional space. Minimization of a loss function via gradient descent in a vastly dimensional space draws these boundaries inside neural networks without confinement to the nouns and verbs of English, C, or even Rust (lol).\nExecution is discrete logic that can be specified as explicit rules. If complaint type is refund and days since purchase is less than 30, approve; if machine type is CPAP and facility code is X, the SKU is ABC-123; click the element with selector a[href=\"/login\"]. Humans did these tasks, even though Von Neumann machines theoretically could and are more reliable and faster, because writing and operating software systems that encode these rules was expensive. The investment was not worth the savings not because of any fuzziness inherent to the task.\nCommon Conflations Today #\nDominant agent architectures conflate judgment and execution, frequently using neural networks for both. The consensus definition of an agent â€” â€œan LLM runs tools in a loop to achieve a goalâ€ â€” clarifies the mechanism but not the problem space.\nFrameworks like browser-use and Stagehand embody this conflation. Consider browser-use:\nagent = Agent(task=\"Find the top HN post\", llm=llm, browser=browser)await agent.run()\nOr Stagehand:\nawait stagehand.act(\"click on the stagehand repo\");await agent.execute(\"Get to the latest PR\");\nIn both cases, the LLM performs judgment (which element is â€œthe stagehand repoâ€?) and execution (click it, figure out the next step, click that). The entire loop is neural. No durable artifact emerges. The LLM is the runtime.\nWhy Execution Requires Traditional Software #\nNeural networks lack the properties that execution requires: determinism, auditability, and precision on edge cases.\nConsider this business logic from a system that processes medical equipment orders (from Docflow Labs, my startup):\nconst scriptedMachineCode = extractMachineCodeFromScripted(scriptedMachine);if (scriptedMachineCode && machineType) {\nconst machineMake = lookupMachineSku(\nmachineType,\nscriptedMachineCode,\nclassification\n);\nif (machineMake) {\nmachineSku = machineMake;\n}}if (!machineSku || machineSku.trim() === \"\") {\nconst facilityMachineCode = extractMachineCodeFromFacility(facility);\nif (facilityMachineCode && machineType) {\nconst machineMake = lookupMachineSku(\nmachineType,\nfacilityMachineCode,\nclassification\n);\nif (machineMake) {\nmachineSku = machineMake;\n}\n}}\nThis code handles combinations that may occur once a year â€” a rare facility, an unusual machine type, a specific classification. The code provides 100% precision even for edge cases. When a billing dispute arises and someone asks why the system chose rental versus purchase for a particular patient, the logic can be traced line by line. It lives in version control and is semantically transparent, deterministic, and auditable.\nA neural network approximating this function cannot provide these properties. Sparse training data will never cover the combinatorial space. Moreover, it blurs boundaries that business requires to be sharp. And it fails opaquely â€” gradients and activations offer no affordance for debugging. Decisions in this substrate are semantically opaque, non-deterministic, and untraceable.\nThe Stagehand Example: Half Right #\nStagehandâ€™s act(\"click on the stagehand repo\") correctly implements judgment via a neural network in some sense. Which element on any dynamically chosen page corresponds to the â€œstagehand repoâ€ cannot be represented in traditional software. There are too many permutations of page layout. The fuzziness of these boundaries is best approached by neural networks in massively multidimensional space minimizing some loss function against many examples.\nIn another sense, however, Stagehandâ€™s architecture is limited. We may know ahead of time which webpage we are attempting a click against and it may change infrequently, requiring only a one-time (or few-time) judgment.\nYet Stagehand produces no executable artifact by design. Instead, the LLM returns a selector, which gets cached opaquely outside version control. On cache miss, the LLM re-engages at runtime to re-interpret the instruction, invoking a neural net.\nA better architecture might still allow the LLM to make a judgment and return a selector, but afford positioning this judgment squarely at buildtime. The selector gets emitted as code into a Playwright script. The script is committed to version control, reviewed, and deployed. On failure â€” because the site changed and the selector broke â€” the development process re-engages. An AI agent rewrites the script. Same judgment, different artifact. The selector becomes a semantically transparent piece of the underlying software system, not ephemeral runtime state.\nA Better Architecture #\nNeural nets may remain at runtime when tackling judgments that can only be made dynamically at runtime. Every other LLM agent belongs at buildtime accelerating the production of executable software.\ncomplaint_text = get_complaint()complaint_type = llm.classify(\ncomplaint_text,\ncategories=[\"refund\", \"fraud\", \"shipping\", \"other\"])if complaint_type == \"refund\":\nif days_since_purchase < 30 and item_condition == \"unopened\":\napprove_refund()\nelse:\nescalate_to_human()elif complaint_type == \"fraud\":\nflag_for_review()\nThe workflow orchestrator is traditional software. It calls out to neural networks for judgment tasks: classification, extraction, interpretation â€” the fuzzy pattern matching that cannot be specified as rules. Then it executes business logic itself, deterministically. The execution paths are explicit, auditable, and version-controlled.\nThis is not a new pattern. Production ML systems already work this way: a model classifies, code acts. Whatâ€™s new is that AI agents can write the code, dissolving the apparent tradeoff between RPA (deterministic but brittle) and AI agents (adaptive but unpredictable).\nDevelopment Time Approaching Runtime #\nSoftware systems have historically maintained a clear separation between two domains: development (humans writing code, days or weeks) and execution (CPUs running code, nanoseconds). AI coding agents close this gap. The theoretical limit as development time approaches zero is runtime:\nlimdevtimeâ†’0buildtime=runtime\\lim_{\\text{devtime} \\to 0} \\text{buildtime} = \\text{runtime}\nâ€‹devtimeâ†’0â€‹limâ€‹â€‹buildtime=runtime\nEven if AI never achieves nanosecond times for writing software, timescales of hours, minutes, and perhaps even seconds allow software systems to adapt to feedback as it arrives.\nAs AI agents get more capable, the distinction between â€œwriting codeâ€ and â€œrunning codeâ€ may dissolve. What emerges resembles reinforcement learning with a different substrate. In traditional RL, a neural network observes state, outputs an action, receives a reward signal, and updates its weights. The network is the adaptive element.\nSubstitute software for the neural network and the structure remains identical. The system observes data â€” requests, errors, metrics, user complaints. Code executes a response. Feedback arrives. An AI agent updates the code. Same adaptive loop, different computable substrate.\nThe difference in representation matters. Neural networks encode behavior in opaque weight matrices. Software encodes behavior in symbolic, human-readable form. Software can be audited, debugged, and surgically modified if necessary. A single fallback chain can be altered without retraining an entire model and hoping it generalizes correctly. The symbolic substrate preserves the properties that production systems often require: interpretability, debuggability, auditability, and surgical modifiability. When the learned update mechanism provides adaptability, you get the benefits of RL without the costs.\nAdaptable Software Systems #\nIronically, software is still mostly all you need at runtime.\nNeural networks are best reserved for judgment â€” the fuzzy tasks we cannot otherwise specify in language â€” and for buildtime acceleration. Neural networks will not replace traditional software, but rather enable its proliferation into corners of the economy that could benefit from reliable discrete logical execution at a fraction of historical costs.\nAn architecture where neural networks handle runtime judgment, software handles execution, and AI agents accelerate buildtime creates a symbolic substrate that is nonetheless adaptable â€” auditability, determinism, and precision alongside the adaptability of learned systems.\nThis is what weâ€™re building at Docflow Labs: adaptive systems with a symbolic substrate. If this resonates, say hello!",
      "publish_datetime": "2026-01-30T00:02:05.184410Z",
      "scraping_timestamp": "2026-01-30T09:02:05.189203Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 48,
      "num_comments": 39,
      "engagement_score": 126.0
    },
    {
      "link": "https://www.macrumors.com/2026/01/28/patreon-apple-tax/",
      "author": "@rsgnl",
      "title": "Apple to Soon Take Up to 30% Cut From All Patreon Creators in iOS App - MacRumors",
      "source": "hackernews",
      "content": "Apple to Soon Take Up to 30% Cut From All Patreon Creators in iOS AppApple has set a new deadline of November 1, 2026 for all Patreon creators to switch from Patreon's legacy billing system to the App Store's in-app purchase system in the Patreon app on the iPhone and iPad, as reported by TechCrunch.\nNote: This image has been edited to include a pile of cash.Patreon is a platform where creators such as YouTubers can receive payments from fans, which can be a valuable revenue stream alongside ads and sponsorships.\nApple initially told Patreon that its creators must move to the App Store's in-app purchase system by November 2025, or else Patreon would risk removal from the App Store, but the deadline was pushed back. Apple considers payments from supporters to creators on Patreon to be digital goods that it is entitled to receive a commission on.\nApple receives a 30% commission on in-app purchases and subscriptions, but this drops to 15% for a subscription that has been ongoing for more than a year.\nPatreon gives creators the option to either increase their prices in the iOS app only, or absorb the fee themselves, keeping prices the same across platforms.\nOn the iPhone and iPad, Patreon users who wish to support a creator can sidestep the App Store's commission by completing their payment via Patreon's website.\nPatreon said it is disappointed with how Apple has navigated this policy.\nAccording to TechCrunch, only 4% of Patreon creators are still using the platform's legacy billing system, with the rest having already switched over.\nPatreon has shared a FAQ with more details for creators.\nPopular StoriesApple today introduced its first two physical products of 2026: a second-generation AirTag and the Black Unity Connection Braided Solo Loop for the Apple Watch.\nRead our coverage of each announcement to learn more:Apple Unveils New AirTag With Longer Range, Louder Speaker, and More\nApple Introduces New Black Unity Apple Watch BandBoth the new AirTag and the Black Unity Connection Braided...Alongside iOS 26.2.1, Apple today released an updated version of iOS 12 for devices that are still running that operating system update, eight years after the software was first released.\niOS 12.5.8 is available for the iPhone 5s and the iPhone 6, meaning Apple is continuing to support these devices for 13 and 12 years after launch, respectively. The iPhone 5s came out in September 2013,...Update: Apple Creator Studio is now available.\nApple Creator Studio launches this Wednesday, January 28. The all-in-one subscription provides access to the Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage apps, with U.S. pricing set at $12.99 per month or $129 per year.\nA subscription to Apple Creator Studio also unlocks \"intelligent features\" and \"premium...Apple today introduced the second-generation AirTag, with key features including longer range for tracking items and a louder speaker.\nFor those who are not familiar, the AirTag is a small accessory that you can attach to your backpack, keys, or other items. Then, you can track the location of those items in the Find My app on the iPhone, iPad, Mac, Apple Watch, and iCloud.com.\nThe new...Apple today confirmed to Reuters that it has acquired Q.ai, an Israeli startup that is working on artificial intelligence technology for audio.\nApple paid close to $2 billion for Q.ai, according to sources cited by the Financial Times. That would make this Apple's second-biggest acquisition ever, after it paid $3 billion for the popular headphone and audio brand Beats in 2014.\nQ.ai has...",
      "publish_datetime": "2026-01-29T09:02:42.980933Z",
      "scraping_timestamp": "2026-01-30T09:02:42.982447Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 1044,
      "num_comments": 863,
      "engagement_score": 2770.0
    },
    {
      "link": "https://utcc.utoronto.ca/~cks/space/blog/python/PythonPackageToolsMyIgnoring",
      "author": null,
      "title": "You're using a too-old browser",
      "source": "hackernews",
      "content": "You're using a suspiciously old browser\nYou're probably reading this page because you've attempted to access\nsome part of my blog (Wandering Thoughts) or\nCSpace, the wiki thing it's part of. Unfortunately\nyou're using a browser version that my anti-crawler precautions consider\nsuspicious, most often because it's too old (most often this applies to\nversions of Chrome). Unfortunately, as of early 2025 there's a plague\nof high volume crawlers (apparently in part to gather data for LLM\ntraining) that use a variety of old browser user agents, especially\nChrome user agents. To reduce the load on\nWandering Thoughts I'm experimenting with\n(attempting to) block all of them, and you've run into this.\nIf this is in error and you're using a current version of your\nbrowser of choice, you can contact me at my current place at the\nuniversity (you should be able to work out the email address\nfrom that). If possible, please let me know what browser you're\nusing and so on, ideally with its exact User-Agent string.\nA special note for people using Vivaldi\nDue to an ongoing attack, you may need to change\nthe\n\"User Agent Brand Masking\" setting so that your Vivaldi identifies\nitself as Vivaldi, instead of Google Chrome. This applies to even\nthe current version of Vivaldi.\nA special note for people using archive.*\nYou may be seeing this through archive.today, archive.ph, archive.is,\nand so on. Unfortunately, archive.* crawls pages to archive in a way that\nis impossible to distinguish from malicious actors. They use old Chrome\nUser-Agent values, crawl from IP address blocks that are widely distributed\nand not clearly identified as theirs, and some of their IP addresses have\nfalsified reverse DNS entries that claim they are googlebot IP addresses\n(which is something that is normally done only by quite bad actors). I\nsuggest that you use archive.org, which is a better behaved archival\ncrawler and can crawl my blog (Wandering Thoughts).\nChris Siebenmann, 2025-02-17",
      "publish_datetime": "2026-01-30T08:03:02.278370Z",
      "scraping_timestamp": "2026-01-30T09:03:02.279708Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI",
      "points": 8,
      "num_comments": 0,
      "engagement_score": 8.0
    },
    {
      "link": "https://github.com/xenodium/agent-shell",
      "author": "xenodium",
      "title": "GitHub - xenodium/agent-shell: A native Emacs buffer to interact with LLM agents powered by ACP",
      "source": "hackernews",
      "content": "Emacs Agent Shell\nğŸ‘‰ Support this work via GitHub Sponsors by @xenodium (check out my blog)\nThis project needs your funding\nAs you pay for those useful LLM tokens, consider sponsoring development and maintenance of this project. With your help, I can make this effort more sustainable.\nThank you!\nAlvaro\nagent-shell\nA native Emacs shell to interact with LLM agents powered by ACP (Agent Client Protocol).\nWith agent-shell, you can chat with the likes of Gemini CLI, Claude Code, Auggie, Mistral Vibe, or any other ACP-driven agent.\nWatch on YouTube\nNews\nagent-shell 0.25 updates.\nagent-shell 0.17 improvements + MELPA.\nagent-shell 0.5 improvements.\nIntroducing Emacs agent-shell (powered by ACP).\nIntroducing acp.el.\nRelated projects\nacp.el: An ACP (Agent Client Protocol) implementation in Emacs lisp.\nagent-shell-manager: Tabulated view and management of agent-shell buffers.\nagent-shell-sidebar: A sidebar add-on for agent-shell.\nagent-review: Code review interface for agent-shell.\nagent-shell-attention.el: Mode-line attention tracker for agent-shell.\nIcons\nThanks to Lobe Icons for the lovely icons.\nSetup\nExternal dependencies\nClaude Code\nFor Anthropicâ€™s Claude Code, follow Zedâ€™s claude-code-acp instructions, typically something like:\nnpm install -g @zed-industries/claude-code-acp\nNote: The -g flag is required to install the binary globally so itâ€™s available in your PATH. After installation, verify itâ€™s available by running which claude-code-acp in your terminal.\nCodex\nFor OpenAIâ€™s Codex, install zed/codex-acp and ensure the `codex-acp` executable is in PATH.\nGemini CLI\nFor Googleâ€™s Gemini CLI, be sure to get a recent release supporting the --experimental-acp flag.\nGoose\nFor Goose CLI, install goose and ensure the `goose` executable is in PATH.\nCursor\nFor Cursor agent, install with:\nnpm install -g @blowmage/cursor-agent-acp\nSee https://github.com/blowmage/cursor-agent-acp-npm for details.\nQwen Code\nFor Qwen Code, install with:\nnpm install -g @qwen-code/qwen-code@latest\nSee https://github.com/QwenLM/qwen-code for details.\nAuggie\nFor Auggie CLI, install with:\nnpm install -g @augmentcode/auggie\nSee https://docs.augmentcode.com/cli/overview for details.\nMistral Vibe\nFor Mistral Vibe, install with:\nuv tool install mistral-vibe\nSee https://github.com/mistralai/mistral-vibe for details.\nFactory Droid\nFor Factory Droid, install the droid-acp client:\nnpm install -g droid-acp\nSee https://github.com/yaonyan/droid-acp for details.\nPi\nFor Pi coding agent, install the pi-acp adapter:\nnpm install -g pi-acp\nSee https://github.com/svkozak/pi-acp for details.\nInstallation\nagent-shell is powered by built-in comint-shell, via shell-maker, available on MELPA.\nBoth agent-shell and its dependency acp.el are now available on MELPA.\nYou can install via:\n(use-package agent-shell\n:ensure t\n:ensure-system-package\n;; Add agent installation configs here\n((claude . \"brew install claude-code\")\n(claude-code-acp . \"npm install -g @zed-industries/claude-code-acp\")))\nThis will automatically install the required dependencies (acp.el and shell-maker).\nDoom Emacs\nIf you are using Doom Emacs and would like to use the package! macro:\n(package! shell-maker)\n(package! acp)\n(package! agent-shell)\nRun doom sync and restart.\nInclude require before configuration:\n(require 'acp)\n(require 'agent-shell)\n;; rest of config...\nConfiguration\nConfigure authentication for the agent providers you want to use.\nEnvironment variables\nPass environment variables to the spawned agent process by customizing the `agent-shell-*-environment` variable with `agent-shell-make-environment-variables`. The helper accepts key/value pairs and exports them when the agent starts.\n(setq agent-shell-anthropic-claude-environment\n(agent-shell-make-environment-variables\n\"ANTHROPIC_API_KEY\" (auth-source-pass-get \"secret\" \"anthropic-api-key\")\n\"HTTPS_PROXY\" \"http://proxy.example.com:8080\"))\nInheriting environment variables\nBy default, the agent process starts with a minimal environment. To inherit environment variables from the parent Emacs process, use the `:inherit-env t` parameter in `agent-shell-make-environment-variables`:\n(setenv \"ANTHROPIC_API_KEY\" (auth-source-pass-get \"secret\" \"anthropic-api-key\"))\n(setq agent-shell-anthropic-claude-environment\n(agent-shell-make-environment-variables :inherit-env t))\nThis ensures that environment variables like `PATH`, `HOME`, and others from your Emacs session are available to the agent process, while still allowing you to override or add specific variables.\nLoading environment variables from files\nYou can load environment variables from .env files using the `:load-env` parameter. This supports both single and multiple files:\n;; Load from a single .env file\n(setq agent-shell-anthropic-claude-environment\n(agent-shell-make-environment-variables\n:load-env \"~/.env\"\n\"CUSTOM_VAR\" \"custom_value\"))\n;; Load from multiple .env files\n(setq agent-shell-anthropic-claude-environment\n(agent-shell-make-environment-variables\n:load-env '(\"~/.env\" \".env.local\")\n:inherit-env t))\nThe .env files should contain variables in the format `KEY=value`, with one variable per line. Comments (lines starting with `#`) and empty lines are ignored.\nAnthropic Claude\nFor login-based authentication (default):\n(setq agent-shell-anthropic-authentication\n(agent-shell-anthropic-make-authentication :login t))\nFor API key authentication:\n;; With string\n(setq agent-shell-anthropic-authentication\n(agent-shell-anthropic-make-authentication :api-key \"your-anthropic-api-key-here\"))\n;; With function\n(setq agent-shell-anthropic-authentication\n(agent-shell-anthropic-make-authentication\n:api-key (lambda () (auth-source-pass-get \"secret\" \"anthropic-api-key\"))))\nFor alternative Anthropic-compatible API endpoints, configure via environment variables:\n(setq agent-shell-anthropic-claude-environment\n(agent-shell-make-environment-variables\n\"ANTHROPIC_BASE_URL\" \"https://api.moonshot.cn/anthropic\"\n\"ANTHROPIC_MODEL\" \"kimi-k2-turbo-preview\"\n\"ANTHROPIC_SMALL_FAST_MODEL\" \"kimi-k2-turbo-preview\"))\nGoogle Gemini\nFor login-based authentication (default):\n(setq agent-shell-google-authentication\n(agent-shell-google-make-authentication :login t))\nFor API key authentication:\n;; With string\n(setq agent-shell-google-authentication\n(agent-shell-google-make-authentication :api-key \"your-google-api-key-here\"))\n;; With function\n(setq agent-shell-google-authentication\n(agent-shell-google-make-authentication\n:api-key (lambda () (auth-source-pass-get \"secret\" \"google-api-key\"))))\nFor Vertex AI authentication:\n(setq agent-shell-google-authentication\n(agent-shell-google-make-authentication :vertex-ai t))\nOpenAI Codex\nFor login-based authentication (default):\n(setq agent-shell-openai-authentication\n(agent-shell-openai-make-authentication :login t))\nFor API key authentication:\n;; With string\n(setq agent-shell-openai-authentication\n(agent-shell-openai-make-authentication :api-key \"your-openai-api-key-here\"))\n;; With function\n(setq agent-shell-openai-authentication\n(agent-shell-openai-make-authentication\n:api-key (lambda () (auth-source-pass-get \"secret\" \"openai-api-key\"))))\nGoose\nFor OpenAI API key authentication:\n;; With string\n(setq agent-shell-goose-authentication\n(agent-shell-make-goose-authentication :openai-api-key \"your-openai-api-key-here\"))\n;; With function\n(setq agent-shell-goose-authentication\n(agent-shell-make-goose-authentication\n:openai-api-key (lambda () (auth-source-pass-get \"secret\" \"openai-api-key\"))))\nQwen Code\nFor OAuth login-based authentication:\n(setq agent-shell-qwen-authentication\n(agent-shell-qwen-make-authentication :login t))\nAuggie\nFor login-based authentication (default):\n(setq agent-shell-auggie-authentication\n(agent-shell-make-auggie-authentication :login t))\nFor no authentication (when using alternative authentication methods):\n(setq agent-shell-auggie-authentication\n(agent-shell-make-auggie-authentication :none t))\nMistral Vibe\nFor API key authentication:\n;; With string\n(setq agent-shell-mistral-authentication\n(agent-shell-mistral-make-authentication :api-key \"your-mistral-api-key-here\"))\n;; With function (reusing the API key configured in vibe)\n(setq agent-shell-mistral-authentication\n(agent-shell-mistral-make-authentication\n:api-key (lambda ()\n(string-trim\n(shell-command-to-string \"source ~/.vibe/.env; echo $MISTRAL_API_KEY\")))))\nCustomizing Available Agents\nBy default, agent-shell includes configurations for all supported agents (Claude Code, Gemini CLI, Codex, Goose, Qwen Code, and Auggie). You can customize which agents are available through the agent-shell-agent-configs variable.\nUsage\nQuick Start\nM-x agent-shell - Start or reuse any of the known agents.\nYou can select and start any of the known agent shells (see agent-shell-agent-configs) via the agent-shell interactive command and enables reusing existing shells when available. With a prefix argument (C-u M-x agent-shell), it forces starting a new shell session, thus instantiating multiple agent shells.\nSpecific Agent Commands\nStart a specific agent shell session directly:\nM-x agent-shell-anthropic-start-claude-code - Start a Claude Code agent session\nM-x agent-shell-auggie-start-agent - Start an Auggie agent session\nM-x agent-shell-openai-start-codex - Start a Codex agent session\nM-x agent-shell-google-start-gemini - Start a Gemini agent session\nM-x agent-shell-goose-start-agent - Start a Goose agent session\nM-x agent-shell-cursor-start-agent - Start a Cursor agent session\nM-x agent-shell-mistral-start-vibe - Start a Mistral Vibe agent session\nM-x agent-shell-qwen-start - Start a Qwen Code agent session\nM-x agent-shell-droid-start-agent - Start a Factory Droid agent session\nM-x agent-shell-pi-start-agent - Start a Pi coding agent session\nSetting a default agent for all new shells\nYou can set a default agent to use for all new shells started via agent-shell like so:\n(setq agent-shell-preferred-agent-config (agent-shell-anthropic-make-claude-code-config))\nConfiguring MCP servers\nYou can configure MCP servers directly via agent-shell. This allows you to avoid having to repeat\nconfigurations across every agent that you use.\n(setq agent-shell-mcp-servers\n'(((name . \"notion\")\n(type . \"http\")\n(headers . [])\n(url . \"https://mcp.notion.com/mcp\"))))\nRunning agents in Devcontainers / Docker containers (Experimental)\nagent-shell provides rudimentary support for running agents and shell commands in containers.\nUse agent-shell-container-command-runner to prefix the command that starts the agent, or a shell command that should be run so it is executed inside the container.\nStatic command list\n(setq agent-shell-container-command-runner '(\"devcontainer\" \"exec\" \"--workspace-folder\" \".\"))\nFunction-based configuration\nFor dynamic per-agent containers, provide a function that takes the current agent-shell buffer and returns the command list:\n(setq agent-shell-container-command-runner\n(lambda (buffer)\n(let ((config (agent-shell-get-config buffer)))\n(pcase (map-elt config :identifier)\n('claude-code '(\"docker\" \"exec\" \"claude-dev\" \"--\"))\n('gemini-cli '(\"docker\" \"exec\" \"gemini-dev\" \"--\"))\n(_ '(\"devcontainer\" \"exec\" \".\"))))))\nPer-session containers\nYou can use different containers for different shell sessions, even of the same agent type:\n(setq agent-shell-container-command-runner\n(lambda (buffer)\n;; Different container based on project\n(if (string-match \"project-a\" (buffer-name buffer))\n'(\"docker\" \"exec\" \"project-a-dev\" \"--\")\n'(\"docker\" \"exec\" \"project-b-dev\" \"--\"))))\nNote that any :environment-variables you may have passed to acp-make-client will not apply to the agent process running inside the container. Itâ€™s expected to inject environment variables by means of your devcontainer configuration / Dockerfile.\nNext, set an agent-shell-path-resolver-function that resolves container paths in the local working directory, and vice versa.\nAgent shell provides the agent-shell--resolve-devcontainer-path function for use with devcontainers specifically: it reads the workspaceFolder specified in .devcontainer/devcontainer.json, or uses the default value of /workspaces/<repository-name> otherwise.\n(setq agent-shell-path-resolver-function #'agent-shell--resolve-devcontainer-path)\nNote that this allows the agent to access files on your local file-system. While care has been taken to restrict access to files in the local working directory, itâ€™s probably possible for a malicious agent to circumvent this restriction.\nOptional: to prevent the agent running inside the container to access your local file-system altogether and to have it read/modify files inside the container directly, in addition to setting the resolver function, disable the â€œread/write text fileâ€ client capabilities:\n(setq agent-shell-text-file-capabilities nil)\nInhibiting minor modes during file writes\nSome minor modes (for example, aggressive-indent-mode) can interfere with an agentâ€™s edits.\nAgent Shell can temporarily disable selected per-buffer minor modes while applying edits.\n(setopt agent-shell-write-inhibit-minor-modes '(aggressive-indent-mode))\nAll of the above settings can be applied on a per-project basis using directory-local variables.\nKeybindings\nC-c C-c - Interrupt current agent operation\nTAB and Shift-TAB - Navigate interactive elements\nEvil\nEvil users may want to rebind RET for inserting a new line in insert\nmode and sending the prompt in normal mode.\nAlso, when viewing diffs (before accepting changes) it may be annoying\nhaving to enter insert mode to send keys (y/n/p/q/etc). If this is\nyour case, you can make these buffers start in Emacs mode (you can\nalways go to Evil modes if you need to with C-z).\n(use-package agent-shell\n:config\n;; Evil state-specific RET behavior: insert mode = newline, normal mode = send\n(evil-define-key 'insert agent-shell-mode-map (kbd \"RET\") #'newline)\n(evil-define-key 'normal agent-shell-mode-map (kbd \"RET\") #'comint-send-input)\n;; Configure *agent-shell-diff* buffers to start in Emacs state\n(add-hook 'diff-mode-hook\n(lambda ()\n(when (string-match-p \"\\\\*agent-shell-diff\\\\*\" (buffer-name))\n(evil-emacs-state)))))\nCustomizations\nCustom variableDescription\nagent-shell-agent-configsThe list of known agent configurations.\nagent-shell-anthropic-authenticationConfiguration for Anthropic authentication.\nagent-shell-anthropic-claude-commandCommand and parameters for the Anthropic Claude client.\nagent-shell-anthropic-claude-environmentEnvironment variables for the Anthropic Claude client.\nagent-shell-anthropic-default-model-idDefault Anthropic model ID.\nagent-shell-anthropic-default-session-mode-idDefault Anthropic session mode ID.\nagent-shell-auggie-authenticationConfiguration for Auggie authentication.\nagent-shell-auggie-commandCommand and parameters for the Auggie client.\nagent-shell-auggie-environmentEnvironment variables for the Auggie client.\nagent-shell-completion-mode-hookHook run after entering or leaving â€˜agent-shell-completion-modeâ€™.\nagent-shell-container-command-runnerCommand prefix for executing commands in a container.\nagent-shell-context-sourcesSources to consider when determining M-x agent-shell automatic context.\nagent-shell-cursor-commandCommand and parameters for the Cursor agent client.\nagent-shell-cursor-environmentEnvironment variables for the Cursor agent client.\nagent-shell-display-actionDisplay action for agent shell buffers.\nagent-shell-droid-authenticationConfiguration for Factory Droid authentication.\nagent-shell-droid-commandCommand and parameters for the Factory Droid ACP client.\nagent-shell-droid-environmentEnvironment variables for the Factory Droid ACP client.\nagent-shell-embed-file-size-limitMaximum file size in bytes for embedding with ContentBlock::Resource.\nagent-shell-file-completion-enabledNon-nil automatically enables file completion when starting shells.\nagent-shell-github-commandCommand and parameters for the GitHub Copilot agent client.\nagent-shell-github-environmentEnvironment variables for the GitHub Copilot agent client.\nagent-shell-google-authenticationConfiguration for Google authentication.\nagent-shell-google-gemini-commandCommand and parameters for the Gemini client.\nagent-shell-google-gemini-environmentEnvironment variables for the Google Gemini client.\nagent-shell-goose-authenticationConfiguration for Goose authentication.\nagent-shell-goose-commandCommand and parameters for the Goose client.\nagent-shell-goose-environmentEnvironment variables for the Goose client.\nagent-shell-header-styleStyle for agent shell buffer headers.\nagent-shell-highlight-blocksWhether or not to highlight source blocks.\nagent-shell-mcp-serversList of MCP servers to initialize when creating a new session.\nagent-shell-mistral-authenticationConfiguration for Mistral AI authentication.\nagent-shell-mistral-commandCommand and parameters for the Mistral Vibe client.\nagent-shell-mistral-default-model-idDefault Mistral AI model ID.\nagent-shell-mistral-default-session-mode-idDefault Mistral AI session mode ID.\nagent-shell-mistral-environmentEnvironment variables for the Mistral Vibe client.\nagent-shell-openai-authenticationConfiguration for OpenAI authentication.\nagent-shell-openai-codex-commandCommand and parameters for the OpenAI Codex client.\nagent-shell-openai-codex-environmentEnvironment variables for the OpenAI Codex client.\nagent-shell-opencode-authenticationConfiguration for OpenCode authentication.\nagent-shell-opencode-commandCommand and parameters for the OpenCode client.\nagent-shell-opencode-environmentEnvironment variables for the OpenCode client.\nagent-shell-path-resolver-functionFunction for resolving remote paths on the local file-system, and vice versa.\nagent-shell-permission-iconIcon displayed when shell commands require permission to execute.\nagent-shell-prefer-viewport-interactionNon-nil makes â€˜agent-shellâ€™ prefer viewport interaction over shell interaction.\nagent-shell-preferred-agent-configDefault configuration to use for all new shells.\nagent-shell-qwen-authenticationConfiguration for Qwen Code authentication.\nagent-shell-qwen-commandCommand and parameters for the Qwen Code client.\nagent-shell-qwen-environmentEnvironment variables for the Qwen Code client.\nagent-shell-screenshot-commandThe program to use for capturing screenshots.\nagent-shell-section-functionsAbnormal hook run after overlays are applied (experimental).\nagent-shell-show-config-iconsWhether to show icons in agent config selection.\nagent-shell-show-welcome-messageNon-nil to show welcome message.\nagent-shell-text-file-capabilitiesWhether agents are initialized with read/write text file capabilities.\nagent-shell-thought-process-expand-by-defaultWhether thought process sections should be expanded by default.\nagent-shell-thought-process-iconIcon displayed during the AIâ€™s thought process.\nagent-shell-tool-use-expand-by-defaultWhether tool use sections should be expanded by default.\nagent-shell-transcript-file-path-functionFunction to generate the full transcript file path.\nagent-shell-ui-mode-hookHook run after entering or leaving â€˜agent-shell-ui-modeâ€™.\nagent-shell-user-message-expand-by-defaultWhether user message sections should be expanded by default.\nagent-shell-write-inhibit-minor-modesMinor modes to temporarily disable during agent file writes.\nCommands\nBindingCommandDescription\nagent-shellStart or reuse an existing agent shell.\nagent-shellâ€“display-bufferToggle agent SHELL-BUFFER display.\nagent-shell-anthropic-start-claude-codeStart an interactive Claude Code agent shell.\nagent-shell-auggie-start-agentStart an interactive Auggie agent shell.\nagent-shell-clear-bufferClear the current shell buffer.\nagent-shell-completion-modeToggle agent shell completion with @ or / prefix.\nagent-shell-cursor-start-agentStart an interactive Cursor agent shell.\nC-<tab>agent-shell-cycle-session-modeCycle through available session modes for the current `agent-shellâ€™ session.\nagent-shell-delete-interaction-at-pointDelete interaction (request and response) at point.\nagent-shell-droid-start-agentStart an interactive Factory Droid agent shell.\nagent-shell-fakes-load-sessionLoad and replay a traffic session from file.\nagent-shell-github-start-copilotStart an interactive GitHub Copilot agent shell.\nagent-shell-google-start-geminiStart an interactive Gemini CLI agent shell.\nagent-shell-goose-start-agentStart an interactive Goose agent shell.\nagent-shell-help-menuTransient menu for `agent-shellâ€™ commands.\nagent-shell-insert-fileInsert a file into `agent-shellâ€™.\nagent-shell-insert-shell-command-outputExecute a shell command and insert output as a code block.\nC-c C-cagent-shell-interruptInterrupt in-progress request and reject all pending permissions.\nagent-shell-jump-to-latest-permission-button-rowJump to the latest permission button row.\nagent-shell-mistral-start-vibeStart an interactive Mistral Vibe agent shell.\nagent-shell-modeMajor mode for agent shell.\nagent-shell-new-shellStart a new agent shell.\nS-<return>agent-shell-newlineInsert a newline, and move to left margin of the new line.\nC-<down> or M-nagent-shell-next-inputCycle forwards through input history.\nn or TABagent-shell-next-itemGo to next item.\nagent-shell-next-permission-buttonJump to the next button.\nagent-shell-open-transcriptOpen the transcript file for the current `agent-shellâ€™ buffer.\nagent-shell-openai-start-codexStart an interactive Codex agent shell.\nagent-shell-opencode-start-agentStart an interactive OpenCode agent shell.\nC-c C-oagent-shell-other-bufferSwitch to other associated buffer (viewport vs shell).\nC-<up> or M-pagent-shell-previous-inputCycle backwards through input history, saving input.\np or <backtab>agent-shell-previous-itemGo to previous item.\nagent-shell-previous-permission-buttonJump to the previous button.\nagent-shell-prompt-composeCompose an `agent-shellâ€™ prompt in a dedicated buffer.\nagent-shell-queue-requestQueue or immediately send a request depending on shell busy state.\nagent-shell-qwen-startStart an interactive Qwen Code CLI agent shell.\nagent-shell-remove-pending-requestRemove all pending requests or a specific request by REMOVE-INDEX.\nC-x x ragent-shell-rename-bufferRename current shell buffer.\nagent-shell-reset-logsReset all log buffers.\nagent-shell-resume-pending-requestsResume processing pending requests in the queue.\nagent-shell-run-all-testsRun all agent-shell tests in batch mode.\nM-ragent-shell-search-historySearch previous input history.\nagent-shell-send-current-fileInsert a file into `agent-shellâ€™.\nagent-shell-send-dwimSend region or error at point to last accessed shell buffer in project.\nagent-shell-send-fileInsert a file into `agent-shellâ€™.\nagent-shell-send-other-filePrompt to send a file into `agent-shellâ€™.\nagent-shell-send-regionSend region to last accessed shell buffer in project.\nagent-shell-send-screenshotCapture a screenshot and insert it into `agent-shellâ€™.\nC-c RETagent-shell-set-session-modeSet session mode (if any available).\nC-c C-vagent-shell-set-session-modelSet session model.\nRETagent-shell-submitSubmit current input.\nagent-shell-toggleToggle agent shell display.\nagent-shell-toggle-loggingToggle logging.\nagent-shell-ui-backward-blockJump to the previous block.\nagent-shell-ui-forward-blockJump to the next block.\nagent-shell-ui-modeMinor mode for SUI block navigation.\nagent-shell-ui-toggle-fragment-at-pointToggle visibility of fragment body at point.\nagent-shell-versionShow `agent-shellâ€™ mode version.\nagent-shell-view-acp-logsView agent shell ACP logs buffer.\nagent-shell-view-trafficView agent shell traffic buffer.\nagent-shell-viewport-compose-cancelCancel prompt composition.\nagent-shell-viewport-compose-sendSend the viewport composed prompt to the agent shell.\nagent-shell-viewport-compose-send-and-killSend the viewport composed prompt to the agent shell and kill compose buffer.\nagent-shell-viewport-compose-send-and-wait-for-responseSend the viewport composed prompt and display response in viewport.\nagent-shell-viewport-cycle-session-modeCycle through available session modes.\nagent-shell-viewport-edit-modeMajor mode for composing agent shell prompts.\nagent-shell-viewport-interruptInterrupt active agent shell request.\nagent-shell-viewport-next-itemGo to next item.\nagent-shell-viewport-next-pageShow next interaction (request / response).\nagent-shell-viewport-previous-itemGo to previous item.\nagent-shell-viewport-previous-pageShow previous interaction (request / response).\nagent-shell-viewport-refreshRefresh viewport buffer content with current item from shell.\nagent-shell-viewport-replyReply as a follow-up and compose another prompt/query.\nagent-shell-viewport-set-session-modeSet session mode.\nagent-shell-viewport-set-session-modelSet session model.\nagent-shell-viewport-view-lastDisplay the last request/response interaction.\nagent-shell-viewport-view-modeMajor mode for viewing agent shell prompts (read-only).\nIssues\nCan you add support for another agent?\nDoes the agent support ACP (Agent Client Protocol)? If so, agent-shell can likely support this agent. Some agents have ACP support built-in (like gemini-cli). Others require a separate ACP package (like claude-code-acp for claude-code). When filing a feature request to add a new agent, please include a link to the project supporting Agent Client Protocol (built-in or otherwise).\nAgents without ACP support are out of scope for integrating with agent-shell. Having said that, if you do build an ACP layer like claude-code-acp, then agent-shell can work with it.\nagent-shell not behaving as expected?\nCould be the agent itself missing an Agent Client Protocol feature, agent-shell missing the feature, or both :) So which one is it? Itâ€™s hard to tell unless we look at Agent Client Protocol traffic between the two.\nHow do I view/get Agent Client Protocol traffic?\nM-x agent-shell-toggle-logging (make sure logging is ON).\nReproduce the issue\nM-x agent-shell-view-traffic\nBrowse through traffic and see if you can spot the issue. For example, if you see a request sent by the agent asking for user permission, but agent-shell isnâ€™t surfacing this permission, it looks like perhaps agent-shell is missing a feature.\nFor example, hereâ€™s what a session/request_permission request would look like from the traffic viewer.\nSometimes including a traffic screenshot in an issue is enough. Other times including the full traffic is needed. From the traffic viewer, you can M-x acp-traffic-save-to to save as .traffic.\nWhere should I file bug or feature request?\nAgent issues or feature requests\nIf youâ€™re able to determine the agent is missing a feature (or a bug is present) in their Agent Client Protocol implementation, please file an issue directly with the agent folks. For example:\nclaude-code-acp: For Claude Code.\ncodex-acp: For Codex.\nGemini CLI.\nGoose.\nQwen Code.\nagent-shell issues or feature requests\nAlternatively, if you noticed agent-shell is missing a feature (or has a bug), please file an agent-shell issue.\nNot sure where to file an issue?\nFile in agent-shell, but please try to provide details, so I can determine whether agent-shell or the agent itself needs work. Traffic data would be very useful here. Provide a screenshot or a .traffic file if you think itâ€™ll help. See how to get ACP traffic.\nContributing\nBefore Contributing\nBefore implementing new features, please file a feature request first to discuss the proposal. This helps ensure alignment with the projectâ€™s direction and prevents unnecessary work.\nAs the maintainer, I must be mindful of all features I accept since I inherit the code to maintain it. Some features may be better suited as separate packages (like agent-shell-sidebar).\nIâ€™ll gladly promote your package wherever possible.\nStyle (or personal preference TBH)\nThere are lots of ways to accomplish things in elisp. While the following are merely personal preferences, as maintainer, it really simplifies things for me to try to limit the number of ways to accomplish things.\nMaps (use alists)\nThis project relies on alists for much of its functionality. Sure, we can also use plists, hashtables, etc.\nUnless we have a strong argument to use something else, please stick with alists (and : keywords).\n'((:species . \"Cat\")\n(:name . \"Whiskers\")\n(:age . 4)\n(:color . \"Gray\")\n(:favorite-toy . \"Feather Wand\"))\nseq.el\nAccessing and working with lists? Please prefer seq.el, unless we have a strong argument to use an alternative.\n(setq animals\n(list\n'((:species . \"Cat\")\n(:name . \"Whiskers\")\n(:age . 4)\n(:color . \"Gray\"))\n'((:species . \"Dog\")\n(:name . \"Buddy\")\n(:age . 6)\n(:color . \"Brown\"))))\n(seq-first animals)\nmap.el\nAccessing and working with alists? Please prefer map.el unless we have a strong argument to use an alternative.\n(setq animal (seq-first animals))\n(map-elt animal :species)\ncl-lib (limited to cl-defun)\nWhile Iâ€™m a fan of cl-defun, please limit cl usage to cl-defun if possible. Nothing against cl-lib. Iâ€™m just limiting the surface and number of idioms I need to keep in my head to maintain the codebase. Often, seq.el and map.el can do the job just fine.\ncl-defun, on the other hand, please do! Iâ€™m a fan of named parameters (yay for self-documenting), so use &key if possible.\n(cl-defun describe (&key animal)\n\"Describe an ANIMAL, which is an alist of properties like :species, :name, :age, :color.\"\n(message \"This is a %d-year-old %s %s named %s.\"\n(map-elt animal :age 0)\n(map-elt animal :color \"Unknown Color\")\n(map-elt animal :species \"Unknown Species\")\n(map-elt animal :name \"Unnamed\")))\n(describe :animal '((:species . \"Cat\")\n(:name . \"Whiskers\")\n(:age . 4)\n(:color . \"Gray\")))\nCode/feature consistency\nPlease try to look for a similar feature in the code base and replicate an existing pattern usage if possible.\nCode Checks\nBefore submitting a PR, please run:\nM-x checkdoc - Ensures documentation consistency\nM-x byte-compile-file - Identifies compilation warnings\nTests\nIâ€™m aware, weâ€™re a bit light on tests, but we started adding some tests. If adding a new feature, please try to add tests.\nTests live under the tests directory:\nls tests/*tests.el\nRunning tests\nOpening any file under the tests directory will load the agent-shell-run-all-tests command.\nRun tests with M-x agent-shell-run-all-tests.\nContributors\nMade with contrib.rocks.",
      "publish_datetime": "2026-01-29T21:03:08.975700Z",
      "scraping_timestamp": "2026-01-30T09:03:08.989667Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 30,
      "num_comments": 3,
      "engagement_score": 36.0
    },
    {
      "link": "https://ipsj.ixsq.nii.ac.jp/records/229345",
      "author": null,
      "title": "æƒ…å ±å­¦åºƒå ´ï¼šæƒ…å ±å‡¦ç†å­¦ä¼šé›»å­å›³æ›¸é¤¨",
      "source": "hackernews",
      "content": "Item type\nSymposium(1)\nå…¬é–‹æ—¥\n2023-11-10\nã‚¿ã‚¤ãƒˆãƒ«\nã‚¿ã‚¤ãƒˆãƒ«\nPlaying Board Games with a Deep Convolutional Neural Network on the Motorola 6809 8-Bit Microprocessor\nã‚¿ã‚¤ãƒˆãƒ«\nè¨€èª\nen\nã‚¿ã‚¤ãƒˆãƒ«\nPlaying Board Games with a Deep Convolutional Neural Network on the Motorola 6809 8-Bit Microprocessor\nè¨€èª\nè¨€èª\neng\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\nä¸»é¡ŒScheme\nOther\nä¸»é¡Œ\ndeep learning\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\nä¸»é¡ŒScheme\nOther\nä¸»é¡Œ\nquantization\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\nä¸»é¡ŒScheme\nOther\nä¸»é¡Œ\nneural networks\nè³‡æºã‚¿ã‚¤ãƒ—\nè³‡æºã‚¿ã‚¤ãƒ—è­˜åˆ¥å­\nhttp://purl.org/coar/resource_type/c_5794\nè³‡æºã‚¿ã‚¤ãƒ—\nconference paper\nè‘—è€…æ‰€å±\nKayufu\nè‘—è€…æ‰€å±(è‹±)\nen\nKayufu\nè‘—è€…å\nRÃ©mi, Coulom\nè‘—è€…å(è‹±)\nRÃ©mi, Coulom\nè«–æ–‡æŠ„éŒ²\nå†…å®¹è¨˜è¿°ã‚¿ã‚¤ãƒ—\nOther\nå†…å®¹è¨˜è¿°\nWhile training deep-learning neural networks often requires considerable amounts of computing power, inference is efficient, and can be run on small devices. Cell phones are a typical example, but they are still rather powerful. The research presented in this paper takes the challenge to the extreme by running a Go-playing convolutional neural network on the 6809 CPU, an 8-bit microprocessor launched by Motorola in 1978. The software was implemented on a Thomson MO5 microcomputer, and reached a playing strength on par with GNU Go.\nè«–æ–‡æŠ„éŒ²(è‹±)\nå†…å®¹è¨˜è¿°ã‚¿ã‚¤ãƒ—\nOther\nå†…å®¹è¨˜è¿°\nWhile training deep-learning neural networks often requires considerable amounts of computing power, inference is efficient, and can be run on small devices. Cell phones are a typical example, but they are still rather powerful. The research presented in this paper takes the challenge to the extreme by running a Go-playing convolutional neural network on the 6809 CPU, an 8-bit microprocessor launched by Motorola in 1978. The software was implemented on a Thomson MO5 microcomputer, and reached a playing strength on par with GNU Go.\næ›¸èªŒæƒ…å ±\nã‚²ãƒ¼ãƒ ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2023è«–æ–‡é›†\nå·» 2023,\np. 66-69,\nç™ºè¡Œæ—¥ 2023-11-10\nå‡ºç‰ˆè€…\nè¨€èª\nja\nå‡ºç‰ˆè€…\næƒ…å ±å‡¦ç†å­¦ä¼š",
      "publish_datetime": "2026-01-29T15:03:13.425833Z",
      "scraping_timestamp": "2026-01-30T09:03:13.426646Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software",
      "points": 40,
      "num_comments": 10,
      "engagement_score": 60.0
    },
    {
      "link": "https://niyikiza.com/posts/hallucination-defense/",
      "author": null,
      "title": "Vectors | The Hallucination Defense",
      "source": "hackernews",
      "content": "â€œThe AI hallucinated. I never asked it to do that.â€\nThatâ€™s the defense. And hereâ€™s the problem: itâ€™s often hard to refute with confidence.\nA financial analyst uses an AI agent to â€œsummarize quarterly reports.â€ Three months later, forensics discovers the M&A target list in a competitorâ€™s inbox. The agent accessed the files. The agent sent the email. But the prompt history? Deleted. The original instruction? The analystâ€™s word against the logs.\nWithout a durable cryptographic proof binding the human to a scoped delegation, â€œthe AI did itâ€ becomes a convenient defense. The agent canâ€™t testify. It canâ€™t remember. It canâ€™t defend itself.\nLogs Arenâ€™t Proof\nâ€œBut we log everything. We have OAuth logs.â€\nMost production agent systems do log a lot, and thatâ€™s good practice. Logs give visibility into what happened, when, and which component did it:\n2026-01-15T14:32:01Z agent=research-bot action=file_read path=/data/ma/target-corp.pdf\n2026-01-15T14:32:03Z agent=research-bot action=email_send to=external@competitor.com\nWith the right setup (append-only storage, signed timestamps, retention controls), logs can be tamper-evident. They can be excellent evidence that an event occurred inside your system.\nBut in disputes, the question is rarely â€œdid something happen?â€ Itâ€™s:\nWho authorized this class of action, for which agent identity, under what constraints, for how long; and how did that authority flow?\nA common failure mode in agent incidents is not â€œwe donâ€™t know what happened,â€ but:\nWe canâ€™t produce a crisp artifact showing that a specific human explicitly authorized the scope that made this action possible.\nThis gap gets wider in multi-agent systems:\nA human authorizes an orchestrator.\nThe orchestrator spawns sub-agents.\nSub-agents call plugins, third-party services, or external runtimes.\nThe final action executes somewhere that may not share your identity domain, your audit system, or your policy engine.\nIn that world, logs can still show: â€œa valid session existedâ€ and â€œa component with access acted.â€ But it becomes harder to show, with a single verifiable chain, that the final actor was operating under a scope the human actually delegated; rather than under a generic session token, a broad integration credential, or inferred intent.\nThis isnâ€™t a dismissal of logging, approvals, policy engines, or token hardening. Itâ€™s an argument that accountability needs one more artifact: independently verifiable authorization evidence that survives multi-hop execution.\nThatâ€™s the liability gap: between â€œwe recorded an eventâ€ and â€œwe can produce a verifiable delegation chain for it.â€\nAuthorization as a First-Class Artifact\nWhen real money moves, institutions donâ€™t rely on â€œsomeone had a session.â€ They require explicit authorization steps (step-up authentication, approvals, dual control, callbacks) and keep durable records of the authorization decision. In inter-organization rails, messages are authenticated so participants can verify who sent what within that rail.\nNot every bank user personally applies a cryptographic signature to every instruction but there is a more general point:\nIn high-stakes systems, the unit of accountability is the action and its authorization record, not a long-lived session.\nThe check system, for all its well-documented flaws, is still interesting because it treats authorization as an artifact you can present later, not a session you have to reconstruct. In a loose, pre-cryptographic way, it gestures at two properties we want for agent delegation.\nFirst, designated negotiation. Checks are addressed to a payee, and endorsement/deposit rules attempt to control who can successfully negotiate the instrument and where. Restrictive endorsements (â€œfor deposit onlyâ€¦â€) are a crude procedural attempt at holder binding. Itâ€™s not cryptographic enforcement, but the shape is right: an authorization artifact meant for a particular holder or route, rather than a replayable credential.\nSecond, non-amplification. Checks instruct settlement against scarce funds. You can write many checks, but settlement ultimately reconciles against a limited balance (or credit line). Failure may be detected late, but delegation doesnâ€™t create value.\nTenuo Warrants apply both ideas to agent actions with modern enforcement: a warrant is holder-bound to a specific agent key, and attenuable so delegated scope can only narrow as it flows downstream.\nAnd this is the non-repudiation point: if delegation is going to cross tools and sub-agents, you need a durable artifact you can show later that answers who authorized what.\nBut in agent systems we authenticate a session (â€œBob is logged inâ€) and then infer intent from a mixture of logs, prompts, and downstream effects. That works until it doesnâ€™t; especially when an incident involves ambiguous delegation paths, third-party tools, or autonomous sub-agents.\nOAuth is great at what itâ€™s designed to do: delegating access and expressing scopes at the token level. But a bearer token is a portable credential: whoever holds it can use it. You can reduce replay risk with sender-constrained tokens (mTLS, DPoP), but even then a primitive is missing:\nWhere is the action-level authorization artifact that says:\nâ€œThis human authorized this agent identity to perform this class of operations within these constraints for this durationâ€?\nWarrants: Signed Authorization for Every Action\nA Tenuo warrant is a cryptographic, scoped, time-bound authorization object that can be verified independently of the agent runtime and that remains meaningful across multi-hop delegation.\n# Human signs the authorization (via Passkey/WebAuthn, not manual key management)\nwarrant = Warrant.mint(\nissuer=alice_passkey,\nholder=agent_public_key,\ncapability=\"file_read\",\nconstraints={\"path\": Subpath(\"/data/reports\")},\nttl=timedelta(hours=1),\n)\nWhen the agent reads a file, it presents this warrant. The file server validates the signature, checks the constraints, and produces a receipt that pairs authorization evidence with the action metadata.\nA verifier checks:\nIssuer signature (who authorized)\nHolder binding (the caller proves possession of the agent key named in the warrant)\nCapability + constraints + expiry (what was allowed, within which bounds, for how long)\nDelegation chain (how authority flowed across hops, including whether the agent was allowed to delegate)\nThe receipt captures:\nAliceâ€™s signature in the warrant (cryptographic proof of authorization)\nThe constraints (cryptographic proof of authorized scope)\nValidation time (evidence of when it was authorized/accepted)\nAction metadata (evidence of what was requested/executed, depending on what you record)\nLogs describe. Receipts prove.\nThe Attack, Replayed\nSame scenario. Analyst wants to process a batch of vendor invoices. Easier to sign one warrant with a high limit and let the agent handle the rest than approve each transfer individually.\nThe warrant their passkey signed at 3:12 PM:\ntool: transfer\namount: range(0, 50000)\nto: *\nttl: 3600\nEvery other analyst that day processed similar batch sizes. They signed 12-15 warrants each:\ntool: transfer\namount: range(0, 500)\nto: vendors/approved/*\nttl: 60\nThree months later, forensics flags a $48,000 transfer to an external account mixed in with the batch.\nAnalystâ€™s defense: â€œThe AI hallucinated. I was just trying to be efficient.â€\nYour response: Everyone else processed the same volume with task-scoped warrants. You signed one that authorized 100x the limit, to any recipient, for an hour. You signed it.\nThe receipt answers what logs canâ€™t: what did you choose to allow?\nâ€œBut What About Prompt Injection?â€\nIf an attacker hijacks the agent mid-session, doesnâ€™t that break accountability?\nWarrants donâ€™t magically stop prompt injection. They make the blast radius explicit and the authorization undeniable.\nConstraints limit what can happen. The warrant says Subpath(\"/data/reports\"). If the injection tries to read /etc/shadow, it will be deterministically denied. The capability doesnâ€™t exist, regardless of what the prompt says.\nThe attack succeeds. The action doesnâ€™t.\nReceipts prove what was authorized. If something did happen, the warrant chain answers who signed off on the scope that allowed it.\nApproval is explicit. The UI doesnâ€™t say â€œAuthorize Agent.â€ It says â€œAuthorize Agent to read /data/reports for 1 hour.â€\nBroad authorization is a choice. A choice you sign. A choice you own.\nWarrants are both a guardrail (prevention via constraints) and a receipt (accountability via signatures).\nâ€œWhat If the Signing Device Is Compromised?â€\nIf a passkey is stolen, you have a crime scene. The attacker had to compromise a specific device. You know which one, when, and what it signed. The forensics point somewhere.\nIf an OAuth token is stolen, you have a ghost. Bearer tokens have no proof of possession: whoever holds it is authorized. It works from anywhere. Logs show what happened, but nothing ties the action to a device, a user, or a moment of intent.\nA log is an assertion by your system. A receipt is a statement signed by the authorizer.\nTrust the Math\nPrompt filters donâ€™t take the stand. When the breach happens, when the subpoena lands, when the regulator asks â€œprove this was authorized,â€ you donâ€™t want to explain your prompt engineering strategy.\nSignatures bind humans to actions. Holder binding makes stolen warrants useless. Constraints limit blast radius. None of it requires trusting the model.\nYou want receipts.\nTenuo is an open-source authorization framework for AI agents. Ed25519 signatures, capability-based delegation, 27Î¼s verification.\nDeploying agents in production? Letâ€™s talk.",
      "publish_datetime": "2026-01-29T20:03:23.880488Z",
      "scraping_timestamp": "2026-01-30T09:03:23.887453Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI",
      "points": 49,
      "num_comments": 142,
      "engagement_score": 333.0
    },
    {
      "link": "https://techcrunch.com/2026/01/29/apple-buys-israeli-startup-q-ai-as-the-ai-race-heats-up/",
      "author": "Lauren Forristal",
      "title": "Apple buys Israeli startup Q.ai as the AI race heats up | TechCrunch",
      "source": "hackernews",
      "content": "Apple, Meta, and Google are locked in a fierce battle to lead the next wave of AI, and theyâ€™ve recently increased their focus on hardware. With its latest acquisition of the AI startup Q.ai, Apple aims to gain an edge, particularly in the audio sector.\nâ€‹As first reported by Reuters, Apple has acquired Q.ai, an Israeli startup specializing in imaging and machine learning, particularly technologies that enable devices to interpret whispered speech and enhance audio in noisy environments. Apple has been adding new AI features to its AirPods, including the live translation capability introduced last year.\nThe company has also developed technology that detects subtle facial muscle activity, which could help the tech giant enhance the Vision Pro headset.\nThe Financial Times reported that the deal is valued at nearly $2 billion, making it Appleâ€™s second-largest acquisition to date, after buying Beats Electronics for $3 billion in 2014.\nâ€‹Notably, this is the second time CEO Aviad Maizels has sold a company to Apple. In 2013, he sold PrimeSense, a 3D-sensing company that played a key role in Appleâ€™s transition from fingerprint sensors to facial recognition on iPhones.\nQ.ai launched in 2022 and is backed by Kleiner Perkins, Gradient Ventures, and others. â€‹Its founding team, including Maizels and co-founders Yonatan Wexler and Avi Barliya, will join Apple as part of the acquisition.\nThe news comes a few hours ahead of Appleâ€™s first quarterly earnings, in which analysts are estimating revenue at around $138 billion. Itâ€™s also expected to be the companyâ€™s strongest iPhone sales growth in four years.\nTechcrunch event\nBoston, MA\n|\nJune 23, 2026",
      "publish_datetime": "2026-01-29T21:03:46.208447Z",
      "scraping_timestamp": "2026-01-30T09:03:46.209266Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI",
      "points": 110,
      "num_comments": 40,
      "engagement_score": 190.0
    },
    {
      "link": "https://alecmuffett.com/article/143548",
      "author": "Dibago",
      "title": "Somebody used spoofed ADSB signals to raster the meme of JD Vance over Mar-a-Lago using AF2 ICAO identity â€“ Dropsafe",
      "source": "hackernews",
      "content": "Somebody used spoofed ADSB signals to raster the meme of JD Vance over Mar-a-Lago using AF2 ICAO identity\n2026/01/28 09:13:40 GMT\nThis, if it is still visible:\nhttps://globe.adsbexchange.com/?icao=adfdf9&lat=26.678&lon=-80.030&zoom=14.4&showTrace=2026-01-28\nVia:\nSomeone is spoofing as a VC-25A / Air Force One making the meme image on ADSB. Call sign \" VANCE1 \" pic.twitter.com/yZMj7bfJUUâ€” ??_???????????? (@SR_Planespotter) January 28, 2026\nNext up, age verification for ADSB?\nâŠ\nadsb trump\nFediverse reactions\n1 repost\n1 like\nComments\n4 responses to â€œSomebody used spoofed ADSB signals to raster the meme of JD Vance over Mar-a-Lago using AF2 ICAO identityâ€\n2026/01/28\nDibago\nRasterize. (I know, nobody likes nitpicking)\nReply\n2026/01/29\nalecm\nBack when I was using real Tektronix T4014s, we didnâ€™t bother to nitpick.\nReply\n2026/01/30\nthatdouchebag\nHaha, old.\nReply\n2026/01/30\nalecm\nExperienced.\nReply\nLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment * Name *\nEmail *\nWebsite\nÎ”\nMore posts\nAnna Riedl: â€œThe amount of prompt engineering needed to not make medical professionals have wild irrelevant hallucinations is truly extraordinaryâ€\n2026/01/30\nJames Ball: On almost everythingâ€¦evidence suggests that over 60s are in much more urgent need of online protection and education than teenagersâ€¦\n2026/01/29\nState Department confirms: federal censorship shield law incoming | Preston Byrne\n2026/01/28\nWill Congress shield the US from foreign attacks on the First Amendment? | Spectator | Ofcom & British civil society does not understand the 1st amendment\n2026/01/28",
      "publish_datetime": "2026-01-29T09:04:14.375539Z",
      "scraping_timestamp": "2026-01-30T09:04:14.376365Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI",
      "points": 539,
      "num_comments": 146,
      "engagement_score": 831.0
    }
  ]
}