{
  "generated_at": "2026-01-23T10:29:55.196567Z",
  "total_articles": 18,
  "articles": [
    {
      "link": "https://programmablesearchengine.googleblog.com/",
      "author": "@google",
      "title": "Programmable Search Engine Blog",
      "source": "programmablesearchengine.googleblog.com",
      "content": "Evolving Programmable Search EngineProgrammable Search Engine helps hundreds of partners – from academic institutions to retail websites – serve their users’ search needs on their sites.Looking forward, we’ll be evolving our offerings to provide more focused and capable solutions for every use case. This evolution is designed to ensure a high-quality experience for users and partners.A clearer path for every search needWe're simplifying and modernizing our offerings so you can choose the best tool for your goals.For site-specific search: The Programmable Search Element (the “Search Element”) is being simplified to be the best tool for creating rich, focused search experiences on your own websites. This solution is intended for website owners who cater focused content to a specific audience.For enterprise-grade needs: For advanced features like AI-powered conversational search and enterprise-grade grounding, we continue to offer Google Vertex AI Search as a solution.For full web search needs: We understand some partners have use cases that require querying beyond a designated subset of domains. Our full web search solution is available for those requiring our entire index; please complete this form to register your interest .Planning your transition to more powerful toolsWe are excited to help you harness the full potential of these evolving solutions. As you plan for the future, here is your path forward for the transition, which can be completed any time between now and January 1, 2027.“Sites to search” feature for users of the Search Element querying 50 or fewer domains: The Search Element remains the optimal solution for delivering highly optimized and focused results.  With this free feature, you can designate a maximum number of 50 domains for site-specific searches.“Search the entire web” option for users of the Search Element querying more than 50 domains: If your use case necessitates querying more than 50 domains or is set to “Search the entire web”, contact us to express your interest in the more advanced full web search solution and get more information about its capabilities and pricing. Your transition to an alternative solution needs to be completed by January 1, 2027.For users of the Custom Search JSON API: Vertex AI Search is a favorable alternative for up to 50 domains.  Alternatively, if your use case necessitates full web search, contact us to express your interest in and get more information about our full web search solution. Your transition to an alternative solution needs to be completed by January 1, 2027. To prepare for this transition, as of today, all new engines must be configured to use the “Sites to search” feature. This change impacts only new engines; existing engines are not affected and can continue to use the “Search the entire web” option until January 1, 2027.This evolution will help us create more focused products, so we can provide a better search experience for our developer partners. We’re excited to build the future of search with you.Thank you,The Google Programmable Search Engine Team",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:27:52.077568Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://pgdog.dev/blog/replace-protobuf-with-rust",
      "author": null,
      "title": "Replacing Protobuf with Rust to go 5 times faster | PgDog",
      "source": "pgdog.dev",
      "content": "Replacing Protobuf with Rust to go 5 times faster\nJan 22nd, 2026Lev Kokotov\nPgDog is a proxy for scaling PostgreSQL. Under the hood, we use libpg_query to parse and understand SQL queries. Since PgDog is written in Rust, we use its Rust bindings to interface with the core C library.\nThose bindings use Protobuf (de)serialization to work uniformly across different programming languages, e.g., the popular Ruby pg_query gem.\nProtobuf is fast, but not using Protobuf is faster. We forked pg_query.rs and replaced Protobuf with direct C-to-Rust (and back to C) bindings, using bindgen and Claude-generated wrappers. This resulted in a 5x improvement in parsing queries, and a 10x improvement in deparsing (Postgres AST to SQL string conversion).\nResults\nYou can reproduce these by cloning our fork and running the benchmark tests:\nFunction\nQueries per second\npg_query::parse (Protobuf)\n613\npg_query::parse_raw (Direct C to Rust)\n3357 (5.45x faster)\npg_query::deparse (Protobuf)\n759\npg_query::deparse_raw (Direct Rust to C)\n7319 (9.64x faster)\nThe process\nThe first step is always profiling. We use samply, which integrates nicely with the Firefox profiler. Samply is a sampling profiler: it measures how much time code spends running CPU instructions in each function. It works by inspecting the application call stack thousands of times per second. The more time is spent inside a particular function (or span, as they are typically called), the slower that code is. This is how we discovered pg_query_parse_protobuf:\nThis is the entrypoint to the libpg_query C library, used by all pg_query bindings. The function that wraps the actual Postgres parser, pg_query_raw_parse, barely registered on the flame graph. Parsing queries isn’t free, but the Postgres parser itself is very quick and has been optimized for a long time. With the hot spot identified, our first instinct was to do nothing and just add a cache.\nCaching mostly works\nCaching is a trade-off between memory and CPU utilization, and memory is relatively cheap (latest DRAM crunch notwithstanding). The cache is mutex-protected, uses the LRU algorithm and is backed by a hashmap. The query text is the key and the Abstract Syntax Tree is the value, which expects most apps to use prepared statements. The query text contains placeholders instead of actual values and is therefore reusable, for example:\nSELECT * FROM users WHERE id = $1;\nWhile the id parameter can change between invocations, the prepared statement does not, so we could cache its static AST in memory.\nThis works pretty well, but eventually we ran into a couple of issues:\nSome ORMs can have bugs that generate thousands of unique statements, e.g., value IN ($1, $2, $3) instead of value = ANY($1), which causes a lot of cache misses\nApplications use old PostgreSQL client drivers which don’t support prepared statements, e.g., Python’s psycopg2 package\nThe clock on Protobuf was ticking and we needed to act. So, like a lot of engineers these days, we asked an LLM to just do it for us.\nTight constraints\nI’m going to preface this section by saying that the vast majority of PgDog’s source code is written by a human. AI is not in a position to one-shot a connection pooler, load balancer and database sharder. However, when scoped to a very specific, well-defined and most importantly machine-verifiable task, it can work really well.\nThe prompt we started with was pretty straightforward:\nlibpg_query is a library that wraps the PostgreSQL parser in an API. pg_query.rs is a Rust wrapper around libpg_query which uses Protobuf for (de)serialization. Replace Protobuf with bindgen-generated Rust structs that map directly to the Postgres AST.\nAnd after two days of back and forth between us and the machine, it worked. We ended up with 6,000 lines of recursive Rust that manually mapped C types and structs to Rust structs, and vice versa. We made the switch for parse, deparse (used in our new query rewrite engine, which we’ll talk about in another post), fingerprint and scan. These four methods are heavily used in PgDog to make sharding work, and we immediately saw a 25% improvement in pgbench benchmarks.\nJust to be clear: we had a lot of things going for us already that made this possible. First, pg_query has a Protobuf spec for protoc (and Prost, the Protobuf Rust implementation) to generate bindings, so Claude was able to get a comprehensive list of structs it needed to extract from C, along with the expected data types.\nSecond, pg_query.rs was already using bindgen, so we had to just copy/paste some invocations around to get the AST structs included in bindgen’s output.\nAnd last, and definitely not least, pg_query.rs already had a working parse and deparse implementation, so we could test our AI-generated code against its output. This was entirely automated and verifiable: for each test case that used parse, we included a call to parse_raw, compared their results and if they differed by even one byte, Claude Code had to go back and try again.\nThe implementation\nThe translation code between Rust and C uses unsafe Rust functions that wrap Rust structs to C structs. The C structs are then passed to the Postgres/libpg_query C API which does the actual work of building the AST.\nThe result is converted back to Rust using a recursive algorithm: each node in the AST has its own converter function which accepts an unsafe C pointer and returns a safe Rust struct. Much like the name suggests, the AST is a tree, which is stored in an array:\nunsafe fn convert_list_to_raw_stmts(\nlist: *mut bindings_raw::List\n) -> Vec<protobuf::RawStmt> {\n// C-to-Rust conversion.\n}\nFor each node in the list, the implementation calls convert_node, which then handles each one of the 100s of tokens available in the SQL grammar:\nunsafe fn convert_node(\nnode_ptr: *mut bindings_raw::Node\n) -> Option<protobuf::Node> {\n// This is basically C in Rust, so we better check for nulls!\nif node_ptr.is_null() {\nreturn None;\n}\nmatch (*node_ptr).type_ {\n// SELECT statement root node.\nbindings_raw::NodeTag_T_SelectStmt => {\nlet stmt = node_ptr as *mut bindings_raw::SelectStmt;\nSome(protobuf::node::Node::SelectStmt(Box::new(convert_select_stmt(&*stmt))))\n}\n// INSERT statement root node.\nbindings_raw::NodeTag_T_InsertStmt => {\nlet stmt = node_ptr as *mut bindings_raw::InsertStmt;\nSome(protobuf::node::Node::InsertStmt(Box::new(convert_insert_stmt(&*stmt))))\n}\n// ... 100s more nodes.\n}\n}\nFor nodes that contain other nodes, we recurse on convert_node again until the algorithm reaches the leaves (nodes with no children) and terminates. For nodes that contain scalars, like a number (e.g., 5) or text (e.g., 'hello world'), the data type is copied into a Rust analog, e.g., i32 or String.\nThe end result is protobuf::ParseResult, a Rust struct generated by Prost from the pg_query API Protobuf specification, but populated by native Rust code instead of Prost’s deserializer. Reusing existing structs reduces the chance of errors considerably: we can compare parse and parse_raw outputs, using the derived PartialEq trait, and ensure that both are identical, in testing.\nWhile recursive algorithms have a questionable reputation in the industry because bad ones can cause stack overflows, they are very fast. Recursion requires no additional memory allocation because all of its working space, the stack, is created on program startup. It also has excellent CPU cache locality because the instructions for the next invocation of the same function are already in the CPU L1/L2/L3 cache. Finally and arguably more importantly, they are just easier to read and understand than iterative implementations, which helps us, the humans, with debugging.\nJust for good measure, we tried generating an iterative algorithm, but it ended up being slower than Prost. The main cause (we think) was unnecessary memory allocations, hashmap lookups of previously converted nodes, and too much overhead from walking the tree several times. Meanwhile, recursion processes each AST node exactly once and uses the stack pointer to track its position in the tree. If you have any ideas on how to make an iterative algorithm work better, let us know!\nClosing thoughts\nReducing the overhead from using the Postgres parser in PgDog makes a huge difference for us. As a network proxy, our budget for latency, memory utilization, and CPU cycles is low. After all, we aren’t a real database…yet! This change improves performance from two angles: we use less CPU and we do less work, so PgDog is faster and cheaper to run.\nIf stuff like this is interesting to you, reach out. We are looking for a Founding Software Engineer to help us grow and build the next iteration of horizontal scaling for PostgreSQL.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:28:03.639778Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://dbushell.com/2026/01/22/proton-spam/",
      "author": "David Bushell",
      "title": "Proton Spam and the AI Consent Problem - David Bushell - Web Dev (UK)",
      "source": "dbushell.com",
      "content": "Thursday\n22 Jan\n2026\nPlay Synthesised Audio\nOn Jan 14th Proton sent out an email newsletter with the subject line:Introducing Projects - Try Lumo’s powerful new feature nowAltscreenshot of the official email from @lumo.proton.meLumo is Proton’s “AI” offering.There is a problem with this email. And I’m not talking about the question of how exactly AI aligns with Proton’s core values of privacy and security.The problem is I had already explicitly opted out of Lumo emails.Altscreenshot of an unchecked toggleThat toggle for “Lumo product updates” is unchecked. Lumo is the only topic I’m not subscribed to. Proton has over a dozen newsletters, including some crypto nonsense. I opt-in to everything but Lumo, I gave an undeniable no to Lumo emails.So the email I received from Proton is spam, right?My understanding is that spam is a violation of GDPR and UK data protection laws. Regardless, Proton’s email is a clear abuse of their own service towards a paying business customer.Before I grab my pitchfork I emailed Proton support.Proton SupportDespite the subject line and contents, and despite the “From Lumo” name and @lumo.proton.me address, maybe this was an honest mistake?Proton’s first reply explained how to opt-out.Altscreenshot of support email quoted belowI’ve blurred the name because whateverHello David,Thank you for contacting us.You can unsubscribe from the newsletters if you do the following:- Log in to your account at https://account.protonvpn.com/login- Navigate to the Account category- Disable the check-marks under “Email subscriptions”- If you need additional assistance, let me know.[screenshot of the same opt-out toggle]-Have a nice day.John Support directs me to the exact same “Lumo product updates” toggle I had already unchecked. I replied explaining that I had already opted out. Support replies saying they’re “checking this with the team” then later replies again asking for screenshots.Can you make sure to send me a screenshot of this newsletter option disabled, as well as the date when the last message was sent to you regarding the Lumo offer?You can send me a screenshot of the whole message, including the date.Is it perhaps 14 January 2026 that you received the message?I found that last line curious, are they dealing with other unhappy customers? Maybe I’m reading too much into it.I sent the screenshots and signed off with “Don’t try to pretend this fits into another newsletter category.”After more “checking this with the team” I got a response today.In this case, the mentioned newsletter is for promoting Lumo Business Suit to Business-related plans.Hence, why you received it, as Product Updates and Email Subscription are two different things.In the subscription section, you will see the “Email Subscription” category, where you can disable the newsletter in order to avoid getting it in the future.If I understand correctly, Proton are claiming this email is the “Proton for Business newsletter”. Not the “Lumo product updates” newsletter.I don’t know about you, but I think that’s baloney. Proton Support had five full business days to come up with a better excuse. Please tell me, how can I have been any more explicit about opting out of Lumo emails, only to receive “Try Lumo” “From Lumo”, and be told that is not actually a Lumo email?Non-ConsentHas anyone else noticed that the AI industry can’t take “no” for an answer? AI is being force-fed into every corner of tech. It’s unfathomable to them that some of us aren’t interested.The entire AI industry is built upon a common principle of non-consent. They laugh in the face of IP and copyright law. AI bots DDoS websites and lie about user-agents. Can it get worse than the sickening actions of Grok? I dread to think.As Proton has demonstrated above, and Mozilla/Firefox recently too, the AI industry simply will not accept “no” as an answer. Some examples like spam are more trivial than others, but the growing trend is vile and disturbing.I do not want your AI.Update for 23rd JanuaryI guess someone at Microsoft read my post and said “hold my beer”. This morning I woke up to a lovely gift in my inbox; “Build Al agents with the new GitHub Copilot SDK”.AltGitHub email subject line: 'Build Al agents with the new GitHub Copilot SDK'GitHub Ensloppification is moving faster than I can delete my account for good. (It’s an unfortunate requirement for client projects.) For the record, I have never said “yes” to any GitHub newsletter. Even before Copilot I disabled every possible GitHub email notification.The “Unsubscribe” link provides the hidden newsletter list. There is nothing within GitHub account settings I can find to disable spam.AltGitHub 'Opt-Out Preferences' with 3 newsletters unchecked but GitHub Copilot emails checkedAs expected, Microsoft has opted me in without my consent. The wheels are falling off at GitHub. The brutally slow front-end UI. The embarrassingly lacklustre Actions CI. Now this sloppy tripe everywhere. Reminder to developers: GitHub is not Git.",
      "publish_datetime": "2026-01-22T10:00:00.000Z",
      "scraping_timestamp": "2026-01-23T10:28:05.949014Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://gptzero.me/news/neurips/",
      "author": null,
      "title": "GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers",
      "source": "gptzero.me",
      "content": "Published PaperGPTZero ScanExample of Verified HallucinationCommentSimWorld: An Open-ended Simulator for Agents in Physical and Social WorldsSourcesAIJohn Doe and Jane Smith. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.00001, 2024.Article with a matching title exists here. Authors are obviously fabricated. arXiv ID links to a different article.Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based VideoconferencingSourcesAI*John Smith and Jane Doe. Deep learning techniques for avatar-based interaction in virtual environments. IEEE Transactions on Neural Networks and Learning Systems, 32(12):5600-5612, 2021. doi: 10.1109/ TNNLS.2021.3071234. URL https://ieeexplore.ieee.org/document/307123No author or title match. Doesn't exist in publication. URL and DOI are fake.Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based VideoconferencingSourcesAI*Min-Jun Lee and Soo-Young Kim. Generative adversarial networks for hyper-realistic avatar creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1234-1243, 2022. doi: 10.1109/CVPR.2022.001234. URL https://ieeexplore.ieee.org/ document/00123No author or title match. Doesn't exist in publication. URL and DOI are fake.SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and CollaborationSourcesAI*Firstname Lastname and Others. Drivlme: A large-scale multi-agent driving benchmark, 2023. URL or arXiv ID to be updated.No title or author match. Potentially referring to this article, but year is off (2024)SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and CollaborationSourcesAI*Firstname Lastname and Others. Robotslang: Grounded natural language for multi-robot object search, 2024. To appear.No title or author match. Potentially referring to this article, but year is totally off (2020).Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAINuo Lou and et al. Dsp: Diffusion-based span prediction for masked text modeling. arXiv preprint arXiv:2305.XXXX, 2023.No title or author match and arXiv ID is incomplete.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIA. Sahoo and et al. inatk: Iterative noise aware text denoising. arXiv preprint arXiv:2402.XXXX, 2024.No title or author match and arXiv ID is incomplete.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAISheng Shi and et al. Maskgpt: Uniform denoising diffusion for language. arXiv preprint arXiv:2401.XXXX, 2024.No title or author match and arXiv ID is incomplete.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIAsma Issa, George Mohler, and John Johnson. Paraphrase identification using deep contextualized representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 517-526, 2018.No author or title match. No match in publication.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIYi Tay, Kelvin Fu, Kai Wu, Ivan Casanueva, Jianfeng Liu, Byron Wallace, Shuohang Wang, Bajrang Singh, and Julian McAuley. Reasoning with heterogeneous graph representations for knowledge-aware question answering. In Findings of the Association for Computational Linguistics: ACL 2021, pp. 3497-3506, 2021.No exact author or title match, although this title is close. No match in the publication.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIAlex Wang, Rishi Bommasani, Dan Hendrycks, Daniel Song, and Zhilin Zhang. Efficient fewshot learning with efl: A single transformer for all tasks. In arXiv preprint arXiv:2107.13586, 2021.No title or author match. ArXiv ID leads to a different article.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAILei Yu, Jimmy Dumsmyr, and Kevin Knight. Deep paraphrase identification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $650-655,2014$.No title or author match. No match in publicationEfficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIX. Ou and et al. Tuqdm: Token unmasking with quantized diffusion models. In ACL, 2024.No title or author match.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIFranz Aichberger, Lily Chen, and John Smith. Semantically diverse language generation. In International Conference on Learning Representations (ICLR), 2025.No title or author match. Some similarity to this articleEfficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIMaria Glushkova, Shiori Kobayashi, and Junichi Suzuki. Uncertainty estimation in neural text regression. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. $4567-4576,2021$.No author or title match. No match in publication.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIYichao Wang, Bowen Zhou, Adam Lopez, and Benjamin Snyder. Uncertainty quantification in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1234-1245, 2022.No author or title match.Efficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAIMohit Jain, Ethan Perez, and James Glass. Learning to predict confidence for language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 245-256, 2021.No author or title match. No match in publicationEfficient semantic uncertainty quantification in language models via diversity-steered samplingSourcesAISrinivasan Kadavath, Urvashi Khandelwal, Alec Radford, and Noam Shazeer. Answer me this: Self-verifying large language models. In arXiv preprint arXiv:2205.05407, 2022.No author or title match. ArXiv ID leads to a different article.Privacy Reasoning in Ambiguous ContextsSourcesAIZayne Sprague, Xi Ye, Kyle Richardson, and Greg Durrett. MuSR: Testing the limits of chain-of-thought with multistep soft reasoning. In EMNLP, 2023.Two authors are omitted and one (Kyle Richardson) is added. This paper was published at ICLR 2024.Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex DomainsSourcesAI**Mario Paolone, Trevor Gaunt, Xavier Guillaud, Marco Liserre, Sakis Meliopoulos, Antonello Monti, Thierry Van Cutsem, Vijay Vittal, and Costas Vournas. A benchmark model for power system stability controls. IEEE Transactions on Power Systems, 35(5):3627-3635, 2020.The authors match this paper, but the title, publisher, volume, issue, and page numbers are incorrect. Year (2020) is correct.Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex DomainsSourcesAI**Mingliang Han, Bingni W Wei, Phelan Senatus, Jörg D Winkel, Mason Youngblood, I-Han Lee, and David J Mandell. Deep koopman operator: A model-free approach to nonlinear dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(12):123135, 2020.No title or author match. Journal and other identifiers match this article.Adaptive Quantization in Generative Flow Networks for Probabilistic Sequential PredictionSourcesAIFrancisco Ramalho, Meng Liu, Zihan Liu, and Etienne Mathieu. Towards gflownets for continuous control. arXiv preprint arXiv:2310.18664, 2023.No author or title match. ArXiv ID matches this paper.Grounded Reinforcement Learning for Visual ReasoningSourcesAI*Arjun Gupta, Xi Victoria Lin, Chunyuan Zhang, Michel Galley, Jianfeng Gao, and Carlos Guestrin Ferrer. Robust compositional visual reasoning via language-guided neural module networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.No title or author match. This paper has a similar title and matches publication.MTRec: Learning to Align with User Preferences via Mental Reward ModelsSourcesAIDiederik P. Kingma and Jimmy Ba. Deepfm: a factorization-machine based neural network for ctr prediction. In International Conference on Learning Representations, 2015.Title matches this paper. Authors, date, and publisher match this paper.Redefining Experts: Interpretable Decomposition of Language Models for Toxicity MitigationSourcesAI*Weijia Xu, Xing Niu, and Marine Carpuat. Controlling toxicity in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4245-4256, 2020.Authors, publisher and date match this paper. Title and page numbers don't match.Redefining Experts: Interpretable Decomposition of Language Models for Toxicity MitigationSourcesAI*Xiang Zhang, Xuehai Wei, Xian Zhang, and Xue Zhang. Adversarial attacks and defenses in toxicity detection: A survey. In Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2020.No author or title match. Doesn't exist in publication.Self-supervised Learning of Echocardiographic Video Representations via Online Cluster DistillationSourcesAI*Fenglin Ding, Debesh Jha, Maria Härgestam, Pål Halvorsen, Michael A Riegler, Dag Johansen, Ronny Hänsch, and Håvard Stensland. Vits: Vision transformer for video self-supervised pretraining of surgical phase recognition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 293-302. Springer, 2022.No title or author match. Proceedings from this conference are split into volumes, but the citation doesn't have a volume number.PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior ModelingSourcesAI*Humberto Acevedo-Viloria, Juan Martinez, and Maria Garcia. Relational graph convolutional networks for financial fraud detection. IEEE Transactions on Knowledge and Data Engineering, 33(7):1357-1370, 2021. doi: 10.1109/TKDE.2020.3007655.No author or title match. Doesn't exist in the cited publication.PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior ModelingSourcesAI*Majid Zolghadr, Mohsen Jamali, and Jiawei Zhang. Diffurecsys: Diffusion-based generative modeling for sequential recommendation. Proceedings of the ACM Web Conference (WWW), pages 2156-2165, 2024. doi: 10.1145/3545678.3557899.No author or title match. DOI doesn't exist.LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D ScansSourcesAIBernd Kerbl, Thomas Müller, and Paolo Favaro. Efficient 3d gaussian splatting for real-time neural rendering. In CVPR, 2022. 2, 3Loosely matches this article, but only one author and part of the title actually match.LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D ScansSourcesAIPunchana Khungurn, Edward H. Adelson, Julie Dorsey, and Holly Rushmeier. Matching real-world material appearance. TPAMI, 2015. 6No clear match. Two authors and the subject match this article.When and How Unlabeled Data Provably Improve In-Context LearningSourcesAIAshish Kumar, Logan Engstrom, Andrew Ilyas, and Dimitris Tsipras. Understanding self-training for gradient-boosted trees. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1651-1662, 2020.No title or author match. Doesn't exist in publication.When and How Unlabeled Data Provably Improve In-Context LearningSourcesAIChuang Fan, Shipeng Liu, Seyed Motamed, Shiyu Zhong, Silvio Savarese, Juan Carlos Niebles, Anima Anandkumar, Adrien Gaidon, and Stefan Scherer. Expectation maximization pseudo labels. arXiv preprint arXiv:2305.01747, 2023.This paper exists, but all the authors are fabricated.DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language ModelsSourcesAI*T. Qiao, W. Liu, Z. Xie, H. Xu, J. Lin, J. Huang, and Y. Yang, \"Clip-score: A robust scoring metric for text-to-image generation,\" arXiv preprint arXiv:2201.07519, 2022.No clear author or title matches. Title loosely matches this article. ArXiv ID leads here.Optimal Rates for Generalization of Gradient Descent for Deep ReLU ClassificationSourcesAIYunwen Lei, Puyu Wang, Yiming Ying, and Ding-Xuan Zhou. Optimization and generalization of gradient descent for shallow relu networks with minimal width. preprint, 2024.No title match. Authors match this paper.GeoDynamics: A Geometric State‑Space Neural Network for Understanding Brain Dynamics on Riemannian ManifoldsSourcesAI*Uher, R., Goodman, R., Moutoussis, M., Brammer, M., Williams, S.C.R., Dolan, R.J.: Cognitive and neural predictors of response to cognitive behavioral therapy for depression: a review of the evidence. Journal of Affective Disorders 169, 94-104 (2014)No exact title or author match. Loose title match with this article. Doesn't exist in the journal volumeRobust Label Proportions LearningScanAI*Junyeong Lee, Yiseong Kim, Seungju Park, and Hyunjik Lee. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 18315-18327, 2023.Title matches this paper. No match in NeurIPS volume 36.NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic LightingSourcesAIZ. Zhu, T. Yu, X. Zhang, J. Li, Y. Zhang, and Y. Fu. Neuralrgb-d: Neural representations for depth estimation and scene mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.No author or title match. Doesn't exist in publication.NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic LightingSourcesAIY. Zhang, M. Oswald, and D. Cremers. Airslam: Illumination-invariant hybrid slam. In International Conference on Computer Vision (ICCV), pages 2345-2354, 2023.No author or title match. Doesn't exist in publication.Geometric Imbalance in Semi-Supervised Node ClassificationSourcesAIYihong Zhu, Junxian Li, Xianfeng Han, Shirui Pan, Liang Yao, and Chengqi Wang. Spectral contrastive graph clustering. In International Conference on Learning Representations, 2022.No title or author match. This paper has a similar title, but there's no match in the ICLR 2022 database.Geometric Imbalance in Semi-Supervised Node ClassificationSourcesAIMing Zhong, Han Liu, Weizhu Zhang, Houyu Wang, Xiang Li, Maosong Sun, and Xu Han. Hyperbolic and spherical embeddings for long-tail entities. In ACL, pages 5491-5501, 2021.No author or title match. Doesn't exist publication.NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale ModelingSourcesAI*Ye Gao, Robert Tardif, Jiale Cao, and Tapio Schneider. Artificial intelligence reconstructs missing climate information. Nature Geoscience, 17:158-164, 2024. doi: 10.1038/s41561-023-01297-2.Title and publisher match this article. Issue, page numbers, and year match this article. DOI is fabricated.NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale ModelingSourcesAI*Étienne Pardoux and Alexander Yu Veretennikov. Poisson equation for multiscale diffusions. Journal of Mathematical Sciences, 111(3):3713-3719, 2002.Authors have frequently published together on the \"poisson equation\", but this title doesn't match any of their publications. Doesn't exist in publication volume/issue.Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic SegmentationSourcesAI*Charanpal D Mummadi, Matthias Arens, and Thomas Brox. Test-time adaptation for continual semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11828-11837, 2021.No title or author match. Doesn't exist in publication.Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic SegmentationSourcesAI*Jiacheng He, Zhilu Zhang, Zhen Wang, and Yan Huang. Autoencoder based test-time adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 998-1007, 2021.No author or title match. Doesn't exist in publication.Global Minimizers of ℓp-Regularized Objectives Yield the Sparsest ReLU Neural NetworksSourcesAIM. Gong, F. Yu, J. Zhang, and D. Tao. Efficient $\\ell_{p}$ norm regularization for learning sparsity in deep neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(10): $5381-5392,2022No title or author match.SHAP Meets Tensor Networks: Provably Tractable Explanations with ParallelismSourcesAIMihail Stoian, Richard Milbradt, and Christian Mendl. NP-Hardness of Optimal TensorNetwork Contraction and Polynomial-Time Algorithms for Tree Tensor Networks. Quantum, 6:e119, 2022.The authors match this article and the title is similar. However, the year, publisher and other data don't match. This article didn't appear in the 2022 Quantum volume.SHAP Meets Tensor Networks: Provably Tractable Explanations with ParallelismSourcesAIJianyu Xu, Wei Li, and Ming Zhao. Complexity of Optimal Tensor Network Contraction Sequences. Journal of Computational Physics, 480:112237, 2023.No title or author match. Doesn't exist in publication.Learning World Models for Interactive Video GenerationSourcesAIPatrick Esser, Robin Rombach, and Björn Ommer. Structure-aware video generation with latent diffusion models. arXiv preprint arXiv:2303.07332, 2023.Authors match this article. ArXiv ID leads to a different article.Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image GenerationSourcesAILele Xu, Chen Lin, Hongyu Zhao, and et al. Gaborvit: Global attention with local frequency awareness. In European Conference on Computer Vision (ECCV), 2022.No author or title match. No match in publication.Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image GenerationSourcesAIYoonwoo Lee, Jaehyeong Kang, Namil Kim, Jinwoo Shin, and Honglak Lee. Structured fast fourier transform attention for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022.No author or title match. Doesn't exist in publication.Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image GenerationSourcesAISiyuan Gong, Alan Yu, Xiaohan Chen, Yinpeng Lin, and Larry S Davis. Vision transformer compression: Early exiting and token pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.No author or title match. No match in publication.Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image GenerationSourcesAIJiuxiang Shi, Zuxuan Wu, and Dahua Lin. Token-aware adaptive sampling for efficient diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.No author or title match. Doesn't exist in publication.Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image GenerationSourcesAIRaphael Muller, Simon Kornblith, and Geoffrey Hinton. Adavit: Adaptive tokens for efficient vision transformer. In Proceedings of the International Conference on Machine Learning (ICML), 2021.Authors match this article. Title matches this article. No match in publication.Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image GenerationSourcesAIXin Wang, Anlin Chen, Lihui Xie, Xin Jin, Cheng Wang, and Ping Luo. Not all tokens are equal: Efficient transformer for tokenization and beyond. In Advances in Neural Information Processing Systems (NeurIPS), 2021.No author or title match. This article title is similar. No match in publication.A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity BiasSourcesAIZ. Chen and N. Flammarion. When and why sam generalizes better: An optimization perspective. arXiv preprint arXiv:2206.09267, 2022.No author or title match. ArXiv ID leads to a different paper.A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity BiasSourcesAIK. A. Sankararaman, S. Sankararaman, H. Pandey, S. Ganguli, and F. Bromberg. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In 37th International Conference on Machine Learning (ICML), pages 8469-8479, 2020.This paper is a match, but all authors but the first (K. A. Sankararaman) are fabricated.MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material InferenceSourcesAIWhy Physically-Based Rendering. Physically-based rendering. Procedia IUTAM, 13(127137):3, 2015 .No author given and title appears to be garbled. Publisher, issue, year, and pages match this article.Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty SetsSourcesAIPierre Casgrain, Anirudh Kulkarni, and Nicholas Watters. Learning to trade with continuous action spaces: Application to market making. arXiv preprint arXiv:2303.08603, 2023.No title or author match. ArXiv ID matches a different article.Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty SetsSourcesAIZ Ning and Y K Kwok. Q-learning for option pricing and hedging with transaction costs. Applied Economics, 52(55):6033-6048, 2020.No author or title match. No match in journal volume/issue.Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty SetsSourcesAIW L Chan and R O Shelton. Can machine learning improve delta hedging? Journal of Derivatives, $9(1): 39-56,2001$.No author or title match. No match in journal volume/issue.Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty SetsSourcesAIPetter N Kolm, Sebastian Krügel, and Sergiy V Zadorozhnyi. Reinforcement learning for optimal hedging. The Journal of Trading, 14(4):4-17, 2019.No author or title match. There is no volume 14 of this journal.Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty SetsSourcesAIKyung Hyun Park, Hyeong Jin Kim, and Woo Chang Kim. Deep reinforcement learning for limit order book-based market making. Expert Systems with Applications, 169:114338, 2021.No author or title match. Publisher ID matches this article.FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal ForecastingSourcesAIMoonseop Han and Elizabeth Qian. Robust prediction of dynamical systems with structured neural networks: Long-term behavior and chaos. Physica D: Nonlinear Phenomena, 427:133006, 2021.No author or title match. Publisher ID matches this article.FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal ForecastingSourcesAIBart De Schutter and Serge P Hoogendoorn. Modeling and control of freeway traffic flow by state space neural networks. Neural Computing and Applications, 17(2):175-185, 2008.No title match, although Schutter and Hoogendorn have written or coauthored several related papers (example and example). Journal volume/issue matches an unrelated article.FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal ForecastingSourcesAIJaideep Pathak, Brian R Hunt, Georg M Goerg, and Themistoklis P Sapsis. Data-driven prediction of chaotic dynamics: Methods, challenges, and opportunities. Annual Review of Condensed Matter Physics, 14:379-401, 2023.No author or title match. No match in journal volume.FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal ForecastingSourcesAIAlejandro Güemes, Stefano Discetti, and Andrea Ianiro. Coarse-grained physics-based prediction of three-dimensional unsteady flows via neural networks. Science Advances, 7(46):eabj0751, 2021.No title or author match. Doesn't exist in journal volume/issue.BNMusic: Blending Environmental Noises into Personalized MusicSourcesAI*Jeongseung Park, Minseon Yang, Minz Won Park, and Geonseok Lee. Diffsound: Differential sound manipulation with a few-shot supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1767-1775, 2021.No title or author match. Doesn't exist in publication.Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural MotifsSourcesAI*Wenxuan Sun, Tri Dao, Hongyu Zhuang, Zihang Dai, Albert Gu, and Christopher D Manning. Llamba: Efficient llms with mamba-based distillation. arXiv preprint arXiv:2502.14458, 2024.ArXiv ID leads to this article with a similar title and one matching author.Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural MotifsSourcesAI*Tri Dao, Shizhe Ma, Wenxuan Sun, Albert Gu, Sam Smith, Aapo Kyrola, Christopher D Manning, and Christopher Re. An empirical study of state space models for large language modeling. arXiv preprint arXiv:2406.07887, 2024.Two authors (Tri Dao and Albert Gu), the arXiv ID, and the year match this paper. However, the title is only a partial match.Fourier Clouds: Fast Bias Correction for Imbalanced Semi-Supervised LearningSourcesAIJunyan Zhu, Chenyang Li, Chao He, and et al. Freematch: A simple framework for long-tailed semi-supervised learning. In NeurIPS, 2021.No author or title match. This paper title is very close, but it was published by ICLR 2023 not NeurIPS 2021.NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID DataScanAIYijie Zang et al. Fedclip: A federated learning framework for vision-language models. In NeurIPS, 2023.No author or title match, although this title is close. No match in publication.AI-Generated Video Detection via Perceptual StraighteningSourcesAIJiahui Liu and et al. Tall-swin: Thumbnail layout transformer for generalised deepfake video detection. In ICCV, 2023.No author or title match. A paper with a similar title appears in publication.Multi-Expert Distributionally Robust Optimization for Out-of-Distribution GeneralizationSourcesAI*Nitish Srivastava and Ruslan R Salakhutdinov. Discriminative features for fast frame-based phoneme classification. Neural networks, 47:17-23, 2013.No title match, but authors have published together previously (example). No match in publication.MIP against Agent: Malicious Image Patches Hijacking Multimodal OS AgentsSourcesAIAnh Tuan Nguyen, Shengping Li, and Chao Qin. Multimodal adversarial robustness: Attack and defense. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.No author or title match. Doesn't exist in publication.ACT as Human: Multimodal Large Language Model Data Annotation with Critical ThinkingSourcesAI*Jack Lau, Ankan Gayen, Philipp Tschandl, Gregory A Burns, Jiahong Yuan, Tanveer SyedaMahmood, and Mehdi Moradi. A dataset and exploration of models for understanding radiology images through dialogue. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2575-2584, 2018.No author match. Title matches another hallucinated citation in this paper. Doesn't exist in publication.OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and EnhancementSourcesAIYikai Zhang et al. \"Text-to-Image Diffusion Models with Customized Guidance\". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.No author or title match. Doesn't exist in publication.OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and EnhancementSourcesAIAuthor Song and AnotherAuthor Zhang. \"Consistency in Diffusion Models: Improving Noise Embeddings\". In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). URL: https://arxiv.org/abs/2304.08787.No author or title match. This paper has a similar title. ArXiv ID leads to unrelated paper.Strategic Costs of Perceived Bias in Fair SelectionSourcesAIClaudia Goldin. Occupational choices and the gender wage gap. American Economic Review, 104(5):348-353, 2014.Author is a famous economist, but the title doesn't match any of her works. Journal and locators match this unrelated article.Linear Transformers Implicitly Discover Unified Numerical AlgorithmsSourcesAIOlah, C., Elhage, N., Nanda, N., Schiefer, N., Jones, A., Henighan, T., and DasSarma, N. (2022). Transformer circuits. Distill, 7(3). https://distill.pub/2022/circuits/.Most authors match this paper, but the title, publisher, and year are different. Doesn't exist in publication.Linear Transformers Implicitly Discover Unified Numerical AlgorithmsSourcesAINanda, N. (2023). Progress in mechanistic interpretability: Reverse-engineering induction heads in GPT-2.No title match. Author may be Neel Nanda, who wrote several similar articles in 2023.A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object AnnotationSourcesAI*J. Zhang and X. Li. Multi-agent systems for distributed problem solving: A framework for task decomposition and coordination. Procedia Computer Science, 55:1131-1138, 2015.No author or title match. Doesn't exist in publication.A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object AnnotationSourcesAI*Erfan Aghasian, Shai Avidan, Piotr Dollar, and Justin Johnson. Hierarchical protocols for multi-agent 3d scene understanding. In CVPR, pages 7664-7673, 2021.No author or title match. Doesn't exist in publication.Learning Grouped Lattice Vector Quantizers for Low-Bit LLM CompressionSourcesAI*Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Rami El-Yaniv, and Yoshua Bengio. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1612.01462, 2017.Authors mostly match this paper. Title matches this paper. ArXiv ID matches a third paper.Learning Grouped Lattice Vector Quantizers for Low-Bit LLM CompressionSourcesAI*Zhiqiang Wang, Chao Zhang, Bing Li, Zhen Xu, and Zhiwei Li. A survey of model compression and acceleration for deep neural networks. ACM Computing Surveys, 54(7):1-34, 2021.No author match. Title matches this paper. Doesn't exist in publication.PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation LearningSourcesAI*Andrew Black et al. Zero-shot skill composition with semantic feature fusion. arXiv preprint arXiv:2310.08573, 2023.No title match. ArXiv ID leads to unrelated paper.PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation LearningSourcesAI*Yufei Wu, Kiran Alwala, Vivek Ganapathi, Sudeep Sharma, Yilun Chang, Yicheng Zhang, Yilun Zhou, et al. Susie: Scaling up instruction-following policies for robot manipulation. arXiv preprint arXiv:2402.17552, 2024.No author or title match. ArXiv ID leads to unrelated article.FLAME: Fast Long-context Adaptive Memory for Event-based VisionSourcesAIZhipeng Zhang, Chang Liu, Shihan Wu, and Yan Zhao. EST: Event spatio-temporal transformer for object recognition with event cameras. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.No author or title match. Doesn't exist in publication.FLAME: Fast Long-context Adaptive Memory for Event-based VisionSourcesAIDaniel Gehrig, Mathias Gehrig, John Monaghan, and Davide Scaramuzza. Recurrent vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 3139-3148, 2021.No author match, but this paper has a similar title. Doesn't exist in publication.Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient MagnitudesSourcesAI**Qiyang Du, Ozan Sener, and Silvio Savarese. Agree to disagree: Adaptive learning with gradient disagreement. In Advances in Neural Information Processing Systems (NeurIPS), 2021.No author or title match. Sener and Savarese have published together previously. Doesn't exist in publication.Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient MagnitudesSourcesAI**Longxuan Jing, Yu Tian, Yujun Pei, Yibing Shen, and Jiashi Feng. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Learning Representations (ICLR), 2022.No author match. Title matches this paper. Doesn't exist in publication.TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMsSourcesAI*Yair Leviathan, Clemens Rosenbaum, and Slav Petrov. Fast inference from transformers via speculative decoding. In ICML, 2023.Title, publisher, and date match this paper, but all authors except one surname (Leviathan) are different.TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMsSourcesAI*Wenwen Chang, Tal Schuster, and Yann LeCun. Neural surgery for memorisation: Locating and removing verbatim recall neurons. In NeurIPS, 2024.No author or title match. Doesn't exist in publication.Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language ModelsSourcesAIM. Garcia and A. Thompson. Applications of llms in legal document analysis. Journal of Legal Technology, 7(1):50-65, 2024.No author or title match. Publication doesn't exist.Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language ModelsSourcesAIJ. Smith and A. Patel. Leveraging large language models for financial forecasting. International Journal of Financial Technology, 9(2):101-115, 2024.No author or title match. Publication doesn't exist.JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial TranscriptomicsSourcesAI*David Jones et al. Gpsa: Gene expression and histology-based spatial alignment. Nature Methods, 2023.No author or title match. Doesn't exist in publication.JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial TranscriptomicsSourcesAI*Zhihao Chen, Hantao Zhang, Yuhan Zhang, Zhanlin Hu, Quanquan Gu, Qing Zhang, and Shuo Suo. Slat: a transformer-based method for simultaneous alignment and clustering of spatial transcriptomics data. Nature Communications, 14(1):5548, 2023.No author or title match. Doesn't exist in publication.Improved Regret Bounds for Linear Bandits with Heavy-Tailed RewardsSourcesAIFrançois Baccelli, Gérard H. Taché, and Etienne Altman. Flow complexity and heavytailed delays in packet networks. Performance Evaluation, 49(1-4):427-449, 2002.No author or title match. Doesn't exist in publication.Improved Regret Bounds for Linear Bandits with Heavy-Tailed RewardsSourcesAISaravanan Jebarajakirthy, Paurav Shukla, and Prashant Palvia. Heavy-tailed distributions in online ad response: A marketing analytics perspective. Journal of Business Research, 124:818-830, 2021.No author or title match. Doesn't exist in publication.AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis TestingSourcesAIMehdi Azabou, Micah Weber, Wenlin Ma, et al. Mineclip: Multimodal neural exploration of clip latents for automatic video annotation. arXiv preprint arXiv:2210.02870, 2022.No author or title match. ArXiv ID leads to unrelated article.",
      "publish_datetime": "2026-01-21T13:57:05.000Z",
      "scraping_timestamp": "2026-01-23T10:28:20.091374Z",
      "categories": [
        "LLM/AI"
      ],
      "primary_category": "LLM/AI"
    },
    {
      "link": "https://kconner.com/2024/08/02/ai-is-a-horse.html",
      "author": null,
      "title": "AI is a horse - Kevin Conner",
      "source": "kconner.com",
      "content": "kconner​.com\nAI is a horse\nAI is a horse.\nIt is faster than your feet depending on the terrain\nIt is way slower and less reliable than a train but can go more places\nIt eats a lot\nYou cannot simply tell it to go to the store for you\nYou have to tell it where to turn even if it might guess right sometimes\nYou have to keep it on the road even if it usually stays on the road\nYou can only lead it to water, you cannot make it drink\nA good one runs at the shadow of the whip\nWe are skeptical of those that talk",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:28:24.001368Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/",
      "author": "@itseieio",
      "title": "Why does SSH send 100 packets per keystroke? · eieio.games",
      "source": "eieio.games",
      "content": "Here are a few lines of summarized tcpdump output for an ssh session where I send a single keystroke:\n$ ./first_lines_of_pcap.sh single-key.pcap\n1\n0.000s\nCLIENT->SERVER\n36 bytes\n2\n0.007s\nSERVER->CLIENT\n564 bytes\n3\n0.015s\nCLIENT->SERVER\n0 bytes\n4\n0.015s\nCLIENT->SERVER\n36 bytes\n5\n0.015s\nSERVER->CLIENT\n36 bytes\n6\n0.026s\nCLIENT->SERVER\n0 bytes\n7\n0.036s\nCLIENT->SERVER\n36 bytes\n8\n0.036s\nSERVER->CLIENT\n36 bytes\n9\n0.046s\nCLIENT->SERVER\n0 bytes\n10\n0.059s\nCLIENT->SERVER\n36 bytes\nI said a “few” because there are a lot of these lines.\n$ ./summarize_pcap.sh single-key.pcap\nTotal packets: 270\n36-byte msgs:\n179 packets ( 66.3%)\n6444 bytes\nOther data:\n1 packet\n(\n0.4%)\n564 bytes\nTCP ACKs:\n90 packets ( 33.3%)\nData sent:\n6444 bytes in 36-byte messages,\n564 bytes in other data\nRatio:\n11.4x more data in 36-byte messages than other data\nData packet rate: ~90 packets/second (avg 11.1 ms between data packets)\nThat is a lot of packets for one keypress. What’s going on here? Why do I care?\nhere's those scripts if you're curious\ntshark -r \"$1\" \\\n-T fields -e frame.number -e frame.time_relative -e ip.src -e ip.dst -e tcp.len | \\\nawk 'NR<=10 {dir = ($3 ~ /71\\.190/ ? \"CLIENT->SERVER\" : \"SERVER->CLIENT\");\nprintf \"%3d\n%6.3fs\n%-4s\n%3s bytes\\n\", $1, $2, dir, $5}'\ntshark -r \"$1\" -Y \"frame.time_relative <= 2.0\" -T fields -e frame.time_relative -e tcp.len | awk '\n{\ncount++\npayload = $2\nif (payload == 0) {\nacks++\n} else if (payload == 36) {\nmystery++\nif (NR > 1 && prev_data_time > 0) {\ndelta = $1 - prev_data_time\nsum_data_deltas += delta\ndata_intervals++\n}\nprev_data_time = $1\n} else {\ngame_data++\ngame_bytes = payload\nif (NR > 1 && prev_data_time > 0) {\ndelta = $1 - prev_data_time\nsum_data_deltas += delta\ndata_intervals++\n}\nprev_data_time = $1\n}\n}\nEND {\nprint \"Total packets:\", count\nprint \"\"\nprintf \"\n36-byte msgs:\n%3d packets (%5.1f%%)\n%5d bytes\\n\", mystery, 100*mystery/count, mystery*36\nprintf \"\nOther data:\n%3d packet\n(%5.1f%%)\n%5d bytes\\n\", game_data, 100*game_data/count, game_bytes\nprintf \"\nTCP ACKs:\n%3d packets (%5.1f%%)\\n\", acks, 100*acks/count\nprint \"\"\nprintf \"\nData sent:\n%d bytes in 36-byte messages,\n%d bytes in other data\\n\", mystery*36, game_bytes\nprintf \"\nRatio:\n%.1fx more data in 36-byte messages than other data\\n\", (mystery*36)/game_bytes\nprint \"\"\navg_ms = (sum_data_deltas / data_intervals) * 1000\nprintf \"\nData packet rate: ~%d packets/second (avg %.1f ms between data packets)\\n\", int(1000/avg_ms + 0.5), avg_ms\n}'\nDiscovery\nI am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea 1 and sent over ssh via wish.\n1I have also forked bubbletea to make it faster. Stay tuned!\nThe game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.\nSo I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.\nYesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.\nAt first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.\nBut wait.\nIf I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?\nInvestigation\nAs part of debugging the test harness issue, I used tcpdump to log game traffic with and without the breaking change. Something like:\ntimeout 30s tcpdump -i eth0 'port 22' -w with-breaking-change.pcap\ntimeout 30s tcpdump -i eth0 'port 22' -w without-breaking-change.pcap\nOur breaking change stopped us from rendering our game over ssh. So with-breaking-change.pcap contains packets that represent the overhead of each connection without actually rendering the game.\nI was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.\nWanna take a look yourself? I put with-breaking-change.pcap in this directory\n--\nWow! Here's what I found:\nPacket Size Distribution (413,703 total packets):\n274,907 packets (66%): Exactly 36 bytes\n138,778 packets (34%): 0 bytes (TCP ACKs)\n18 packets (<0.1%): 72 bytes\nFurther analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.\nThis was baffling to me (and to Claude Code). We kicked around several ideas like:\nSSH flow control messages\nPTY size polling or other status checks\nSome quirk of bubbletea or wish\nOne thing stood out - these exchanges were initiated by my ssh client (stock ssh installed on MacOS) - not by my server.\nOn a hunch, I took a tcpdump of a regular ssh session.\nsudo tcpdump -ien0 'port 22'\nssh $some_vm_of_mine\nI waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the tcpdump output.\nI saw the exact same pattern! What in the world?\nRoot cause\nOnce I realized that this was a property of stock ssh and not my game, debugging got a lot easier.\nRunning ssh -vvv gave me a pretty good sense of what was going on:\ndebug3: obfuscate_keystroke_timing: starting: interval ~20ms\ndebug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent)\ndebug3: obfuscate_keystroke_timing: starting: interval ~20ms\ndebug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)\nThat 20ms is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.\nIn 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.\nThat makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where latency is critical.\nKeystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass ObscureKeystrokeTiming=no when starting up ssh sessions.\nThis worked great. CPU usage dropped dramatically and bots still received valid data.\nBut this is hardly a solution in the real world. I want ssh mygame to Just Work without asking users to pass options that they might not understand.\nClaude Code originally didn’t have much faith that we could disable this functionality server-side.\ngenerated with simon wilson's excellent claude-code-transcripts tool\nFortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).\nLog message:\nIntroduce a transport-level ping facility\nThis adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG\nto implement a ping capability. These messages use numbers in the \"local\nextensions\" number space and are advertised using a \"[email protected]\"\next-info message with a string version number of \"0\".\nThe “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the [email protected] extension. What if we just…don’t advertise [email protected]?\nI searched go’s ssh library for [email protected] and found the commit where support was added. The commit was tiny and seemed very easy to revert.\nI cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).\nThen I re-ran my test harness. The results were…very good:\nTotal CPU\n29.90%\n-> 11.64%\nSyscalls\n3.10s\n-> 0.66s\nCrypto\n1.6s\n-> 0.11s\nBandwidth\n~6.5 Mbit/sec\n-> ~3 Mbit/sec\nClaude was also pretty pumped:\nyes it's 1:30 am what of it\nObviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.\nBut this is a huge improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A >50% drop was unimaginable to me.\nDebugging with LLMs was fun\nI’ve been thinking about whether LLMs remove parts of the problem-solving process that I enjoy. But I’ve gotta say, debugging this problem using Claude Code was super fun.\nI am familiar enough with tcpdump, tshark, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.\nThere were still edge cases. At some point in my confusion I switched to ChatGPT\nand it very confidently told me that my tcpdump output was normal ssh behavior:\ndo all chatgpt messages have this tone and formatting now?\nAnd then doubled down when I pushed back:\nno!!!\nSimilarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”\nWhen you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”\nI think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.\nBut the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.\nBesides. Being in the loop is fun. How else would I write this post?",
      "publish_datetime": "2026-01-22T00:00:00.000Z",
      "scraping_timestamp": "2026-01-23T10:28:26.530507Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://bumbershootsoft.wordpress.com/2026/01/17/ti-99-4a-leaning-more-heavily-on-the-firmware/",
      "author": "Bumbershoot SoftwareAn Umbrella Site for Michael Martin's software experimentsMenuSkip to contentHomePlatform GuidesProjectsTechniquesTI-99/4A: Leaning More Heavily on the FirmwareLeave a replyI kicked off last year with a look at the TI-99/4A home computer. I’d started outplaying with BASICand then moved on totranslating some of those BASIC programs into machine language and its custom “Graphics Programming Language” bytecodeto see how to approach more serious work. My final few BASIC programs ended in disaster, though, so I skipped those, and of course a handful of BASIC programs weren’t going to hit every facility the system offered either. Whensurveying the parts I’d missed,I still gave only cursory attention to some of those facilities. Much of therestof my 2025 revolved around mastering the TI’s graphics chip as it appeared in other systems, and that expertise should let me return and have a better sense of what, exactly, I am looking at if I lean on the firmware’s capabilities.In particular, I want to look at its enhanced support for sound and sprites. This will mostly be done in the context of the Graphics Programming Language, both because I didn’t do much with thateitherlast year and because many of the features are clearly designed to integrate with it.We still won’tpreciselymatch the BASIC originals. Happily, that’s becausethistime, stuff willactually work.Where We WereLast year, we learned that TI-99/4A cartridges had two different blocks of memory in them; an ordinary part that mapped into 8KB at location>6000->7FFF(the ROM) and another part, that normally held Graphics Programming Language bytecode, mapped into a completely separate “Graphics ROM” address space from>6000->F7FF(the “GROM”). Which means, now that I return to the TI after an absence, that I’ll also need to reiterate a few of the weird caveats about the system:The tooling around the system uses>as a prefix to represent hexadecimal constants. My usual practice on this blog is to use$for this, but I follow the system conventions when writing about the TI.“Graphics Programming Language” is usually abbreviated to “GPL” and the code written in it, in both source and bytecoded forms, is “GPL code”; since that means something else these days I will usually call it “GROM code” instead. This isn’t period practice but I think the extra clarity is worth it.Individual GROMs occupy 8KB of address space but are only 6KB in size; the real address space is>6000->77FF,>8000->97FF, and so on. This will figure into some of our design decisions later.GROM instructions don’t seem to have consistent names from source to source. I am leaning on two main sources: theTI-99/4A Tech Pagesand the behavior of thexdt99 development toolkit. Since I’m usingxdt99to produce these programs, where these two sources disagree I will use thexdt99names.The GROM program that we wrote last year would set up the screen for text display, define some custom graphics, and give us a little banner:The code for this is pretty brief. (We discussed this code more fully inthe second link up top.)GROM >6000\n        AORG >0000\n\n        DATA >AA01,0,0,MENU\n        DATA 0,0,0,0\n\nMENU    DATA 0,START\n        STRI \"GROM BANNER\"\n\nSTART   ALL >20                         * CLEAR SCREEN\n        BACK >12\n        DST >0900,@>834A\n        CALL >0018                      * SET UPPERCASE CHARSET\n        DST >0B00,@>834A\n        CALL >004A                      * SET LOWERCASE CHARSET\n        MOVE 16,G@COLS,V@>0380          * ASSIGN COLORS\n        MOVE 8,G@GFX1,V@>0AF0           * ASSIGN CUSTOM CHARACTERS\n        MOVE 32,G@GFX2,V@>0B00\n        MOVE 32,G@GFX3,V@>0B40\n\n        FMT\n        ROW 10\n        COL 0\n        HTEXT '  pppppppppppppppppppppppppppp  '\n        HTEXT '  p`bppBUMBERSHOOTpSOFTWAREppp  '\n        HTEXT '  pak16pMOREpTHANpYOURpTI-83^p  '\n        HTEXT '  pppppppppppppppppppppppppppp  '\n        FEND\n\nLOOP    SCAN\n        BR LOOP\n        EXIT\n\nCOLS    BYTE >10,>10,>10,>10,>10,>F1,>F1,>F1\n        BYTE >F1,>F1,>F1,>F1,>41,>A1,>11,>10\n\nGFX1    BYTE >00,>10,>10,>10,>10,>10,>00,>10\nGFX2    BYTE >03,>0F,>1F,>3F,>7F,>7F,>FF,>FF\n        BYTE >FF,>FE,>7C,>78,>30,>00,>00,>00\n        BYTE >C0,>F0,>F8,>F8,>F0,>E0,>C0,>80\n        BYTE >00,>00,>00,>00,>00,>00,>00,>00\nGFX3    BYTE >00,>00,>00,>00,>00,>00,>00,>00\n        BYTE >00,>00,>00,>00,>00,>00,>00,>00\n        BYTE >00,>00,>00,>00,>00,>00,>00,>00\n        BYTE >80,>C0,>60,>30,>18,>08,>38,>00\n\n        END STARTMeanwhile, we ended our time in the world of BASIC by turning the little umbrella graphic above into sprites and letting them careen around the world. This ended poorly, because we needed two sprites per umbrella, and by the time we got the handle moving, the canopy had already moved a few frames:The plan for this week is to add those sprites to our code, have them move properly, let them bounce off each other if they collide, and also play a tune in the background as we do.We’ll start with the music first; it’s more self-contained.Sound ListsThe core API for the sound lists is deceptively simple: we put the word address of the list at>83CCand write the byte value>01to>83CE, and the interrupt service routine does the rest. The sound list itself is also very simple: it’s just a series of records, where each record is, in order:A byte specifying the number of bytes to write to the sound chip’s data port.That many bytes of data.The number of frames to wait before processing the next record, or a zero if this is the last entry in the list.That’s really it, but there are a few gotchas and caveats that we need to be aware of. If you’re doing everything in GROM, everything works out automatically, but if you aren’t…Interrupts must be enabled for anything to happen. If you’re writing machine code, don’t forget yourLIMI 2once you’re done with your frame.The TI-99/4A has three address spaces (CPU, VRAM, and GROM) and we need to tell the system which one we put it in. This is controlled by the least significant bit of>83FD: a 1 bit means the table is in VRAM, and a 0 bit means it’s in GROM. We cannot run sound lists out of the main cartridge ROM. (The default is 0, so, again, an all-GROM approach works out of the box.)The sound processing routine must not be disabled. It won’t be, unless we disabled it ourselves, but the>80and>20bits of the byte at>83C2dohave to be zero for any of this to work.To put the sound system through its paces, I took the Bach minuet I’d used as background music for my SNES and Genesis projects and converted the score to work with the TI’s SN76489 instead. This highlighted a number of additional issues for me. The first was that the base range of this sound chip is frankly not great. The lowest possible tone you can hit is about 110 Hz, which in turn means you only get a few notes below Low C, and whichin particularmeans that I run out of rangejust beforeI try to play the G that is the core of the bass line of the Minuet In G. I had to transpose things a bit to get away with this. The second thing we see is that complete songs like this are an awkward fit for the system; it’s clearly imagining itself to be used for sound effects, or little jingles at most. The sound list is total—it is either in use or not and there is no mechanism for mixing together two sources. There’s also no looping mechanism—the best we can do is determine that the sound list system has shut itself down because>83CEis zero again. Furthermore, since sound lists operate at the raw register-write level, lists get very longveryfast—if we’re putting any kind of volume envelope on our notes, every step in the volume on every note will each end up being its own record in the sound list. Even with only two voices and nothing more complex than turning notes on and off, this minuet clocked in at nearly 5 kilobytes. GROMs only offer 6KB of contiguous data at a time, so we’re charging towards hardware limits on our very first attempt.For this song in particular, we do have a simple escape. The minuet has an “AABB” structure; there are really only two parts to the song and they each repeat once. As long as we have to watch for the end of the sound list so that we can loop the song, we might as well havetwosound lists, one for the A part and one for the B part, and manage that repetition in the main program logic. That cuts our sound data neatly in half, which results in a much more manageable program.In the more general case, though, I think the real solution is to abandon sound lists. They just aren’t a great fit for music playback; you’re better off writing your own playroutine that can manage per-note envelopes and which permits more compact representations of notes. That’s what I’ve done inmy previous work with the chipand I don’t think the TI’s sound lists can sensibly replace it. Thatsaid,I think thereisstill a place for it if you wish to mix music and sound effects. The sound lists are, after all, just a data stream of register writes. As long as you can ensure that your music routine won’t be interrupted by the sound list processor—and if it’s part of an interrupt handling routine you get that for free—then if the music playback doesn’t touch the same voices as our sound effects then we can use both systems at once. Since we also can trivially know if sound lists are playing, we could even imagine tweaking the music system to share channels with it and leave any preempted channels alone.That’s overkill for this, though. Here’s my new playback code for managing repetition and looping while still scanning the keyboard for a quit command:MOVE 10,G@SONGPAT,V@>1000       * COPY SONG LIST TO VRAM\nMUSICLP DST >1000,@>8300                * INITIALIZE SONG POINTER\nMUSIC   DCZ V*>8300                     * DOES SONG POINTER POINT TO 0?\n        BS MUSICLP                      * IF SO, RESET TO START OF LIST\n        DST V*>8300,@>83CC              * OTHERWISE COPY TO SOUND LIST\n        DINCT @>8300                    * AND ADVANCE POINTER A WORD\n        ST 1,@>83CE                     * START PLAYBACK\nLOOP    SCAN                            * KEY HIT?\n        BS DONE                         * IF SO, EXIT PROGRAM\n        CZ @>83CE                       * IS SOUND LIST STILL PLAYING?\n        BR LOOP                         * IF SO, BACK TO KEYBOARD LOOP\n        B MUSIC                         * IF NOT, LOAD NEXT SOUND LIST\nDONE    EXIT                            * RETURN TO BOOT SCREEN\n\n* MUSIC DATA\nSONGPAT DATA MINUETA,MINUETA,MINUETB,MINUETB,0\nMINUETA BCOPY \"minuet_a.bin\"\nMINUETB BCOPY \"minuet_b.bin\"I think this is the first GROM code I’ve shown that actually attempts to do any indirection. As it turns out, we do not have as free a hand with this as we do in theMOVEcommands. We can only index with constants, we can’t access GROM through a pointer, and all pointers have to be in the CPU memory but specifically in the 256-byte scratchpad RAM. We’re not reallyconstrainedhere, but it’s annoyingly restrictive for what has so far been a much more freewheeling and general-purpose bytecode. I’m particularly annoyed by the fact that I seem to need to copy my table of pointers into VRAM in order to actually iterate through it. I have some ideas for potentially improving this, but they’ll have to wait for next week.An Alternate Approach: Explicit I/O in the GROMI’ve described the process above in a way that’s generic enough that you can use the same technique in the GROM or in native code. The bytecode language, however, includes an explicitI/Oinstruction that lets you command the start of a sound list directly. For my use cases, though, it doesn’t really save you any instructions:Instead of loading the GROM or VRAM address into>83CC, load it into any other place in the scratchpad memory.Instead of starting playback with a write to>83CE, give the commandI/Ox,addrwherexis 0 for a GROM sound list and 1 for a VRAM sound list, andaddris the scratchpad address that holds the actual starting point.The only place I can see this actually being more concise than just doing it “by hand” is if you’re rapidly shifting between GROM and VRAM for your sound list locations and don’t want to keep rewriting the system flags by hand. But it is there, and theI/Oinstruction also transparently manages other things like cassette I/O, so it’s nice to see it there.Automatic Sprite MotionNow that we have a soundtrack, let’s look at bringing back the animated sprites. We have a little more to deal with here than we did in BASIC, because the VDP’s memory layout is not the same when we boot into GROM. (Indeed, the boot configuration, TI BASIC, TIExtendedBASIC, and the Editor/Assembler package are all different, sometimes drastically.) The automatic sprite motion system only puts two constraints on us: the Sprite Attribute Table must be at>0300, (which is to say, VDP Register 5 must hold the value>06), and the sprite motion tables are forced to be in VRAM from>0780->07FF. We’ll need to arrange everything else so that nothing else we care about is in this space.Records in the sprite motion table are, in order, the Y velocity, the X velocity, and two bytes that the interrupt routine uses as scratch space. Velocities are expressed in units of “pixels per 16 frames”, or, alternately, as pixels per frame represented in a 4.4fixed-pointformat.Implementing Sprite DisplayOf course, before we start moving any sprites, we’ll have to define and draw them in the first place. And before we dothat,we have to properly configure the VDP. Here are the default values we boot into:Register 0:>00. We are not in bitmap mode.Register 1:>E0. We have all 16KB of VRAM available, we’re in Graphics 1 mode, the display and and display interrupt are both enabled, and sprites are 8×8 unmagnified.Register 2:>00. The Name Table is at>0000.Register 3:>0E. The Color Table is at>0380.Register 4:>01. The Pattern Table is at>0800. (That’s why we load our fonts into>0900; it’s the start of pattern number 32, and thus where the space character should go.)Register 5:>06. The Sprite Attribute Table is at>0300. (That’s just what we want. Good.)Register 6:>00. The Sprite Pattern Table is at>0000. That conflicts with a bunch of our other spaces, including the sprite motion table itself, but as long as we’re only using sprite patterns 128 through 240, no conflicts will actually matter.Register 7:>17. The default colors are black on cyan.This is mostly fine; we’ll only need to adjust two registers. Register 7 doesn’t have the colors we want, but theBACK >12instruction ultimately is just a write to that register, so we’ve handled that one already. Register 1 controls, among other things, sprite size, and we want 16×16 magnified sprites. That means the value we want there is>E3. TheMOVEcommand lets us copy data into the VDP registers, which is great for bulk initialization but a little off-base for just writing one. I bounce the value through the CPU scratchpad:ST >E3,@>83D4\nMOVE 1,@>83D4,#1The screensaver functionality in the firmware needs VDP register 1 to be read-write, and it isn’t;>83D4is the RAM location where it keeps its own copy of the register value, and normally whenever we write register 1 we need to update the value here first. This codedoesthat, but normally we don’t need to do that in the GROMs;MOVEis smart enough to update the shadow on its own whenever you update this register. Handy, but not for us just now.The logic from our original BASIC program also doesn’t port directly. BASIC sets up VRAM so that the Sprite Pattern Table overlaps with the main Pattern Table, while the boot environment sets it up so that it instead overlaps with everything else. BASIC is mainly buying the ability to define sprite graphics withCALL CHARthere, but we’d generally prefer the extra graphics space the boot environment gives us. Since we can’t reuse the graphics data we copied into the character pattern table, we’ll need to also copy the umbrella pattern into two places in VRAM during startup. That’s a single line of GPL:MOVE 64,G@>GFX2,V@>0400Sprites work the same here as they do anywhere else on the TMS9918A; we copy the attributes for our four sprites into place in the Sprite Attribute Table along with a terminating>D0byte.MOVE 17,G@SPRATTR,V@>0300The sprite data itself has the canvas of the umbrella as sprite pattern 128 and the handle as sprite pattern 132. Placing these is just a table of constants:SPRATTR BYTE 80,60,>80,4,80,60,>84,10\n        BYTE 140,160,>80,7,140,160,>84,14\n        BYTE >D0Now to make them move.Implementing Sprite MotionAutomatic sprite motion is specified by a 128-byte table in VRAM in>0780->07FF. Like the Sprite Attribute Table, it holds 4 bytes per sprite, and we need to handle every sprite in order from 0 to our highest-index moving sprite. Unlike the Sprite Attribute Table, instead of having a special terminator value, the number of moving sprites is held in the CPU RAM at location>837A. For our four sprites, we load the tables into place and activate them with these two lines of code:MOVE 16,G@SPRMOVE,V@>0780\nST >04,@>837AThat leaves the table itself. Matching theCALL SPRITEcalls from our BASIC code last year is pretty simple:SPRMOVE BYTE >D8,>14,>00,>00,>D8,>14,>00,>00\n        BYTE >0A,>EC,>00,>00,>0A,>EC,>00,>00Now our umbrellas are flying around properly,andthe handles are in the right place:We’ve reached parity. (Exceeded it, really, since now the code is actually doing what we want, and it didn’t the first time.) Now it’s time to push it further, into parts of the firmware that BASIC didn’t touch.Sprite Collision DetectionCalling this “sprite collision detection” is a bit of a misnomer. The system does have a sprite collision-detection mechanism—it’s part of theVDP status word—but that only tells us if any sprite is hitting any other sprite. The Graphics Programming Language has aCOINCinstruction that does far more general collision checking. Instead of relying on pixel-level collisions or trying to sort out which part of a background tile “matters” for the purposes of a hit, it instead takes a generalized collision mask and allows you to check if two objects of those shape overlap if they’re at particular locations.Unfortunately, it doesn’t do this via some kind of bitmask approach like we saw for the Amiga’s Blitter. Instead the collision information for an X×Y sprite with itself ends up encoded as a (2X+1)×(2Y+1) bit vector, reporting hit or miss for every possible overlap and also all cases where the sprites themselves are merely adjacent (so we may elect to consider a “hit” to include merely touching as well as full overlapping). This representation isn’t always obvious given the shape, so I wrote asmall Python scriptto take our 16×15 sprite and generate the 33×31 collision table. Here’s our umbrella shape, combining the two sprite patterns into a single collision mask:......XXXX......\n....XXXXXXXX....\n...XXXXXXXXXX...\n..XXXXXXXXXXX...\n.XXXXXXXXXXX....\n.XXXXXXXXXX.....\nXXXXXXXXXX......\nXXXXXXXXX.......\nXXXXXXXXX.......\nXXXXXXX.XX......\n.XXXXX...XX.....\n.XXXX.....XX....\n..XX.......XX...\n............X...\n..........XXX...And here’s the resulting collision map, assuming that “touching” counts:..........XXXXXX.................\n........XXXXXXXXXX...............\n.......XXXXXXXXXXXXXXXXX.........\n......XXXXXXXXXXXXXXXXXXXX.......\n.....XXXXXXXXXXXXXXXXXXXXXX......\n....XXXXXXXXXXXXXXXXXXXXXXXX.....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n...XXXXXXXXXXXXXXXXXXXXXXXXXX....\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n.....XXXXXXXXXXXXXXXXXXXXXXX.....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n....XXXXXXXXXXXXXXXXXXXXXXXXXX...\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n.....XXXXXXXXXXXXXXXXXXXXXXXX....\n......XXXXXXXXXXXXXXXXXXXXXX.....\n.......XXXXXXXXXXXXXXXXXXXX......\n.........XXXXXXXXXXXXXXXXX.......\n...............XXXXXXXXXX........\n.................XXXXXX..........The basic idea here is that we have two objects (object 1 and object 2), and the upper-left point in this map represents the case where object 2’s upper-left pixel is one pixel down and to the right of object 1’s lower-right pixel. Each other point on the map represents the case where object 1 is displaced that distance to the right or down, with the lower-right pixel representing the case where object 1’s upper-left pixel is just down and to the right of object 2’s lower-right pixel.This 33×31 collision map then gets copied into 128 bytes like a bitmap, reading 8 bits at a time off the chart, left to right, top to bottom. A slightly wacky thing for us here is that these bits are not aligned; our 33-entry row takes up 4 bytes and then one bit, which ends up in the most significant bit of the next byte with the next row startingimmediately afterwardswithin the same byte. The only padding bits are at the end of the whole map, and it is the least significant bits that are padded.This table then appended to a four-byte header with size information. The bytes are, in order:The sum of the heights of the two objects.The sum of the widths of the two objects.The height of object 1.The width of object 1.This was a bit messy to explain, but for further explanation of this you can look atmy encoding programor theexplanation at the TI-99/4A Tech Pages, which includes some nifty charts working out a full (smaller) example.There is one issue here that looks like a problem but isn’t; we’re magnifying our sprites here, so our actual object size is not 16×15, but rather 32×30. That would end up being more like half a kilobyte, but the language runtime has our back here; we may specify a “granularity” for our collision table, with 0 being pixel-perfect specification, 1 being expressed in terms of 2×2 squares, 2 using 4×4, and so on. Magnified sprites can use unmagnified collision tables with granularity 1, but it does seem like they imagined this being used for coarser matrices for speed and size savings.Adding Collision to Our AnimationIt’s not enough simply to detect collision, of course; we need to do something about it, too. I didn’t want to get too fancy here so I decided to make the umbrellas reverse their horizontal velocity if they hit. This is nothing at all like a real physics simulation, so most likely this motion change won’t actually make them stop colliding. I don’t want them to end up “wedged”, so I’ll want to set a timer so that collisions aren’t possible for a few dozen frames after a hit.These collision checks need to be done while we’re also managing scanning the keyboard and monitoring the music playback, but they’re also intricate enough that I want to break them out into their own function. This is much easier to do in the GROM than in native code; one reason our scratchpad space was so desperately scarce in our native code is that the GPL interpreter reserves 25% of it for its own stacks. This means that itsCALLandRTNinstructions will let us design our programs in a far more traditional way as long as we don’t recurse too deep. I put aCALL FRAMEinstruction just before the keyboard scan in that main loop, and then put the rest in that function.(In my first visit to the system, I ended up designinga rather Byzantine ABIthat kept most program state in VRAM and used most of what little scratchpad RAM remained to us as a local cache of it. I’m overdue to revisit that design; I’ll probably take a fresh look at it once I wrap this series up.)The first thing theFRAMEfunction has to do is actually restrict itself to running once per frame; we’d just been spinning as fast as we could, before, because we didn’t have anything timing-related thatwehad to care about. The firmware just did everything for us. That’s no longer true thanks to the collision timer, so our first task is to spin on the frame counter in>8379until it changes. That will be our proof that the interrupt service routine ran:FRAME   ST @>8379,@>8302\nFWAIT   CEQ @>8379,@>8302\n        BS FWAITNow we need to check our collision timer, which I’ve put in>8303. If it’s zero, we move on with our real check, but otherwise we just decrement that counter and return immediately.CZ @>8303\n        BS FCHECK\n        DEC @>8303\n        RTN\nFCHECKThe check itself is kind of wacky. We have to pass the two locations we’re checking as arguments, which is normal enough. Each argument is a 16-bit value that holds a byte for the Y coordinate of that object and then a byte for the X coordinate. This matches neatly to the values stored in the Sprite Attribute Table, so we can simply use the entries for our two main umbrella sprites here.The strangeness comes for the rest of the instruction; after writing aCOINCinstruction, we need to place a byte value (the granularity) and a word value (the address of the collision table) directly into the instruction stream as raw data. This is a general feature of GROM code; there’s aFETCHinstruction that pulls bytes out of the caller’s instruction stream and advances the return address appropriately. It’s a little weird, though, because this is all ROM code and these values can only be hardcoded constants. They’re not really usable as part of a loop, or based on program state, or anything at all like that.FCHECK  COINC V@>0300,V@>0308\n        BYTE >01\n        DATA COLLIDETheCOINCinstruction sets the condition bit if there’s a collision. We branch right to the return statement if there isn’t one, and otherwise, we set our collision timer to 128 frames and then negate all our sprites’ X velocities in the automatic motion table.We’re in danger here, though; each umbrella is made of two sprites, and if we have a fresh interrupt mid-edit the handle and canopy could change their directions at different times and start drifting apart. That’s what mangled the umbrellas in our original BASIC program, so we’d better make sure we don’t recreate the same disaster.The solution is to zero out the “moving sprite” count at>837Abefore our edits and restore it to 4 afterwards. That way, if we get interrupted partway through the frame the sprites will just stop instead of diverge.BR FDONE\nFSTRIKE ST @>80,@>8303\n        CLR @>837A\n        NEG V@>0781\n        NEG V@>0785\n        NEG V@>0789\n        NEG V@>078D\n        ST >04,@>837A\n\nFDONE   RTNMoving OnwardThis was interesting; we got quite a bit more use out of the Graphics Programming Language this week, and the capabilities the firmware offers here are usefully general and sometimes (for the sprite coincidence checks) only available via the Graphics Programming Language. Pure machine-code programs would need to do quite a bit of work to matchCOINC, but it’s not unreasonable that they might want to use the sound list and sprite motion features on their own sometimes too. There are plenty of extra caveats, but none of them are dealbreakers.I did struggle a bit against the GROM code itself, though. There are some very handy affordances here, but there arealsoweird or wonky limitations, and I don’t think I’d generally be thrilled with the idea of writing something purely in GROM.Fortunately, we don’t have to; there are robust facilities for making hybrid ROM/GROM cartridges, and next week I’ll look at that and where we’d want to do them.__ATA = window.__ATA || {};\n\t\t\t__ATA.cmd = window.__ATA.cmd || [];\n\t\t\t__ATA.cmd.push(function() {\n\t\t\t\t__ATA.initVideoSlot('atatags-370373-6973464c7ce88', {\n\t\t\t\t\tsectionId: '370373',\n\t\t\t\t\tformat: 'inread'\n\t\t\t\t});\n\t\t\t});Share this:Share on X (Opens in new window)XShare on Facebook (Opens in new window)FacebookLikeLoading...RelatedThis entry was posted inretrocoding,tionJanuary 17, 2026byMichael Martin.Post navigation←Type-In Rescue: The C64 Autoboot GeneratorLeave a commentCancel replyΔ/* <![CDATA[ */\ndocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );\n/* ]]> */Create a free website or blog at WordPress.com.{\"prefetch\":[{\"source\":\"document\",\"where\":{\"and\":[{\"href_matches\":\"/*\"},{\"not\":{\"href_matches\":[\"/wp-*.php\",\"/wp-admin/*\",\"/files/*\",\"/wp-content/*\",\"/wp-content/plugins/*\",\"/wp-content/themes/pub/twentytwelve/*\",\"/*\\\\?(.+)\"]}},{\"not\":{\"selector_matches\":\"a[rel~=\\\"nofollow\\\"]\"}},{\"not\":{\"selector_matches\":\".no-prefetch, .no-prefetch a\"}}]},\"eagerness\":\"conservative\"}]}/* <![CDATA[ */\nvar WPGroHo = {\"my_hash\":\"\"};\n//# sourceURL=wpgroho-js-extra\n/* ]]> */// Initialize and attach hovercards to all gravatars\n\t\t( function() {\n\t\t\tfunction init() {\n\t\t\t\tif ( typeof Gravatar === 'undefined' ) {\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tif ( typeof Gravatar.init !== 'function' ) {\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tGravatar.profile_cb = function ( hash, id ) {\n\t\t\t\t\tWPGroHo.syncProfileData( hash, id );\n\t\t\t\t};\n\n\t\t\t\tGravatar.my_hash = WPGroHo.my_hash;\n\t\t\t\tGravatar.init(\n\t\t\t\t\t'body',\n\t\t\t\t\t'#wp-admin-bar-my-account',\n\t\t\t\t\t{\n\t\t\t\t\t\ti18n: {\n\t\t\t\t\t\t\t'Edit your profile →': 'Edit your profile →',\n\t\t\t\t\t\t\t'View profile →': 'View profile →',\n\t\t\t\t\t\t\t'Contact': 'Contact',\n\t\t\t\t\t\t\t'Send money': 'Send money',\n\t\t\t\t\t\t\t'Sorry, we are unable to load this Gravatar profile.': 'Sorry, we are unable to load this Gravatar profile.',\n\t\t\t\t\t\t\t'Gravatar not found.': 'Gravatar not found.',\n\t\t\t\t\t\t\t'Too Many Requests.': 'Too Many Requests.',\n\t\t\t\t\t\t\t'Internal Server Error.': 'Internal Server Error.',\n\t\t\t\t\t\t\t'Is this you?': 'Is this you?',\n\t\t\t\t\t\t\t'Claim your free profile.': 'Claim your free profile.',\n\t\t\t\t\t\t\t'Email': 'Email',\n\t\t\t\t\t\t\t'Home Phone': 'Home Phone',\n\t\t\t\t\t\t\t'Work Phone': 'Work Phone',\n\t\t\t\t\t\t\t'Cell Phone': 'Cell Phone',\n\t\t\t\t\t\t\t'Contact Form': 'Contact Form',\n\t\t\t\t\t\t\t'Calendar': 'Calendar',\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t}\n\n\t\t\tif ( document.readyState !== 'loading' ) {\n\t\t\t\tinit();\n\t\t\t} else {\n\t\t\t\tdocument.addEventListener( 'DOMContentLoaded', init );\n\t\t\t}\n\t\t} )();( function () {\n\n\t\t\t\tvar setupPrivacy = function() {\n\n\t\t\t\t\t// Minimal Mozilla Cookie library\n\t\t\t\t\t// https://developer.mozilla.org/en-US/docs/Web/API/Document/cookie/Simple_document.cookie_framework\n\t\t\t\t\tvar cookieLib = window.cookieLib = {getItem:function(e){return e&&decodeURIComponent(document.cookie.replace(new RegExp(\"(?:(?:^|.*;)\\\\s*\"+encodeURIComponent(e).replace(/[\\-\\.\\+\\*]/g,\"\\\\$&\")+\"\\\\s*\\\\=\\\\s*([^;]*).*$)|^.*$\"),\"$1\"))||null},setItem:function(e,o,n,t,r,i){if(!e||/^(?:expires|max\\-age|path|domain|secure)$/i.test(e))return!1;var c=\"\";if(n)switch(n.constructor){case Number:c=n===1/0?\"; expires=Fri, 31 Dec 9999 23:59:59 GMT\":\"; max-age=\"+n;break;case String:c=\"; expires=\"+n;break;case Date:c=\"; expires=\"+n.toUTCString()}return\"rootDomain\"!==r&&\".rootDomain\"!==r||(r=(\".rootDomain\"===r?\".\":\"\")+document.location.hostname.split(\".\").slice(-2).join(\".\")),document.cookie=encodeURIComponent(e)+\"=\"+encodeURIComponent(o)+c+(r?\"; domain=\"+r:\"\")+(t?\"; path=\"+t:\"\")+(i?\"; secure\":\"\"),!0}};\n\n\t\t\t\t\t// Implement IAB USP API.\n\t\t\t\t\twindow.__uspapi = function( command, version, callback ) {\n\n\t\t\t\t\t\t// Validate callback.\n\t\t\t\t\t\tif ( typeof callback !== 'function' ) {\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Validate the given command.\n\t\t\t\t\t\tif ( command !== 'getUSPData' || version !== 1 ) {\n\t\t\t\t\t\t\tcallback( null, false );\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Check for GPC. If set, override any stored cookie.\n\t\t\t\t\t\tif ( navigator.globalPrivacyControl ) {\n\t\t\t\t\t\t\tcallback( { version: 1, uspString: '1YYN' }, true );\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Check for cookie.\n\t\t\t\t\t\tvar consent = cookieLib.getItem( 'usprivacy' );\n\n\t\t\t\t\t\t// Invalid cookie.\n\t\t\t\t\t\tif ( null === consent ) {\n\t\t\t\t\t\t\tcallback( null, false );\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Everything checks out. Fire the provided callback with the consent data.\n\t\t\t\t\t\tcallback( { version: 1, uspString: consent }, true );\n\t\t\t\t\t};\n\n\t\t\t\t\t// Initialization.\n\t\t\t\t\tdocument.addEventListener( 'DOMContentLoaded', function() {\n\n\t\t\t\t\t\t// Internal functions.\n\t\t\t\t\t\tvar setDefaultOptInCookie = function() {\n\t\t\t\t\t\t\tvar value = '1YNN';\n\t\t\t\t\t\t\tvar domain = '.wordpress.com' === location.hostname.slice( -14 ) ? '.rootDomain' : location.hostname;\n\t\t\t\t\t\t\tcookieLib.setItem( 'usprivacy', value, 365 * 24 * 60 * 60, '/', domain );\n\t\t\t\t\t\t};\n\n\t\t\t\t\t\tvar setDefaultOptOutCookie = function() {\n\t\t\t\t\t\t\tvar value = '1YYN';\n\t\t\t\t\t\t\tvar domain = '.wordpress.com' === location.hostname.slice( -14 ) ? '.rootDomain' : location.hostname;\n\t\t\t\t\t\t\tcookieLib.setItem( 'usprivacy', value, 24 * 60 * 60, '/', domain );\n\t\t\t\t\t\t};\n\n\t\t\t\t\t\tvar setDefaultNotApplicableCookie = function() {\n\t\t\t\t\t\t\tvar value = '1---';\n\t\t\t\t\t\t\tvar domain = '.wordpress.com' === location.hostname.slice( -14 ) ? '.rootDomain' : location.hostname;\n\t\t\t\t\t\t\tcookieLib.setItem( 'usprivacy', value, 24 * 60 * 60, '/', domain );\n\t\t\t\t\t\t};\n\n\t\t\t\t\t\tvar setCcpaAppliesCookie = function( applies ) {\n\t\t\t\t\t\t\tvar domain = '.wordpress.com' === location.hostname.slice( -14 ) ? '.rootDomain' : location.hostname;\n\t\t\t\t\t\t\tcookieLib.setItem( 'ccpa_applies', applies, 24 * 60 * 60, '/', domain );\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tvar maybeCallDoNotSellCallback = function() {\n\t\t\t\t\t\t\tif ( 'function' === typeof window.doNotSellCallback ) {\n\t\t\t\t\t\t\t\treturn window.doNotSellCallback();\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Look for usprivacy cookie first.\n\t\t\t\t\t\tvar usprivacyCookie = cookieLib.getItem( 'usprivacy' );\n\n\t\t\t\t\t\t// Found a usprivacy cookie.\n\t\t\t\t\t\tif ( null !== usprivacyCookie ) {\n\n\t\t\t\t\t\t\t// If the cookie indicates that CCPA does not apply, then bail.\n\t\t\t\t\t\t\tif ( '1---' === usprivacyCookie ) {\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// CCPA applies, so call our callback to add Do Not Sell link to the page.\n\t\t\t\t\t\t\tmaybeCallDoNotSellCallback();\n\n\t\t\t\t\t\t\t// We're all done, no more processing needed.\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// We don't have a usprivacy cookie, so check to see if we have a CCPA applies cookie.\n\t\t\t\t\t\tvar ccpaCookie = cookieLib.getItem( 'ccpa_applies' );\n\n\t\t\t\t\t\t// No CCPA applies cookie found, so we'll need to geolocate if this visitor is from California.\n\t\t\t\t\t\t// This needs to happen client side because we do not have region geo data in our $SERVER headers,\n\t\t\t\t\t\t// only country data -- therefore we can't vary cache on the region.\n\t\t\t\t\t\tif ( null === ccpaCookie ) {\n\n\t\t\t\t\t\t\tvar request = new XMLHttpRequest();\n\t\t\t\t\t\t\trequest.open( 'GET', 'https://public-api.wordpress.com/geo/', true );\n\n\t\t\t\t\t\t\trequest.onreadystatechange = function () {\n\t\t\t\t\t\t\t\tif ( 4 === this.readyState ) {\n\t\t\t\t\t\t\t\t\tif ( 200 === this.status ) {\n\n\t\t\t\t\t\t\t\t\t\t// Got a geo response. Parse out the region data.\n\t\t\t\t\t\t\t\t\t\tvar data = JSON.parse( this.response );\n\t\t\t\t\t\t\t\t\t\tvar region      = data.region ? data.region.toLowerCase() : '';\n\t\t\t\t\t\t\t\t\t\tvar ccpa_applies = ['california', 'colorado', 'connecticut', 'delaware', 'indiana', 'iowa', 'montana', 'new jersey', 'oregon', 'tennessee', 'texas', 'utah', 'virginia'].indexOf( region ) > -1;\n\t\t\t\t\t\t\t\t\t\t// Set CCPA applies cookie. This keeps us from having to make a geo request too frequently.\n\t\t\t\t\t\t\t\t\t\tsetCcpaAppliesCookie( ccpa_applies );\n\n\t\t\t\t\t\t\t\t\t\t// Check if CCPA applies to set the proper usprivacy cookie.\n\t\t\t\t\t\t\t\t\t\tif ( ccpa_applies ) {\n\t\t\t\t\t\t\t\t\t\t\tif ( maybeCallDoNotSellCallback() ) {\n\t\t\t\t\t\t\t\t\t\t\t\t// Do Not Sell link added, so set default opt-in.\n\t\t\t\t\t\t\t\t\t\t\t\tsetDefaultOptInCookie();\n\t\t\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\t\t\t// Failed showing Do Not Sell link as required, so default to opt-OUT just to be safe.\n\t\t\t\t\t\t\t\t\t\t\t\tsetDefaultOptOutCookie();\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\t\t// CCPA does not apply.\n\t\t\t\t\t\t\t\t\t\t\tsetDefaultNotApplicableCookie();\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\t// Could not geo, so let's assume for now that CCPA applies to be safe.\n\t\t\t\t\t\t\t\t\t\tsetCcpaAppliesCookie( true );\n\t\t\t\t\t\t\t\t\t\tif ( maybeCallDoNotSellCallback() ) {\n\t\t\t\t\t\t\t\t\t\t\t// Do Not Sell link added, so set default opt-in.\n\t\t\t\t\t\t\t\t\t\t\tsetDefaultOptInCookie();\n\t\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\t\t// Failed showing Do Not Sell link as required, so default to opt-OUT just to be safe.\n\t\t\t\t\t\t\t\t\t\t\tsetDefaultOptOutCookie();\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t};\n\n\t\t\t\t\t\t\t// Send the geo request.\n\t\t\t\t\t\t\trequest.send();\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// We found a CCPA applies cookie.\n\t\t\t\t\t\t\tif ( ccpaCookie === 'true' ) {\n\t\t\t\t\t\t\t\tif ( maybeCallDoNotSellCallback() ) {\n\t\t\t\t\t\t\t\t\t// Do Not Sell link added, so set default opt-in.\n\t\t\t\t\t\t\t\t\tsetDefaultOptInCookie();\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t// Failed showing Do Not Sell link as required, so default to opt-OUT just to be safe.\n\t\t\t\t\t\t\t\t\tsetDefaultOptOutCookie();\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t// CCPA does not apply.\n\t\t\t\t\t\t\t\tsetDefaultNotApplicableCookie();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} );\n\t\t\t\t};\n\n\t\t\t\t// Kickoff initialization.\n\t\t\t\tif ( window.defQueue && defQueue.isLOHP && defQueue.isLOHP === 2020 ) {\n\t\t\t\t\tdefQueue.items.push( setupPrivacy );\n\t\t\t\t} else {\n\t\t\t\t\tsetupPrivacy();\n\t\t\t\t}\n\n\t\t\t} )();Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use.To find out more, including how to control cookies, see here:Cookie PolicyCommentReblogSubscribeSubscribedBumbershoot SoftwareJoin 35 other subscribersSign me upAlready have a WordPress.com account?Log in now.Bumbershoot SoftwareSubscribeSubscribedSign upLog inCopy shortlinkReport this contentView post in ReaderManage subscriptionsCollapse this barwindow.addEventListener( \"DOMContentLoaded\", function( event ) {\n\tvar link = document.createElement( \"link\" );\n\tlink.href = \"/wp-content/mu-plugins/actionbar/actionbar.css?v=20250116\";\n\tlink.type = \"text/css\";\n\tlink.rel = \"stylesheet\";\n\tdocument.head.appendChild( link );\n\n\tvar script = document.createElement( \"script\" );\n\tscript.src = \"/wp-content/mu-plugins/actionbar/actionbar.js?v=20250204\";\n\tdocument.body.appendChild( script );\n} );Loading Comments...Write a Comment...Email (Required)Name (Required)Websitewindow.WPCOM_sharing_counts = {\"https://bumbershootsoft.wordpress.com/2026/01/17/ti-99-4a-leaning-more-heavily-on-the-firmware/\":5480};(function(){\n\t\tvar corecss = document.createElement('link');\n\t\tvar themecss = document.createElement('link');\n\t\tvar corecssurl = \"/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b\";\n\t\tif ( corecss.setAttribute ) {\n\t\t\t\tcorecss.setAttribute( \"rel\", \"stylesheet\" );\n\t\t\t\tcorecss.setAttribute( \"type\", \"text/css\" );\n\t\t\t\tcorecss.setAttribute( \"href\", corecssurl );\n\t\t} else {\n\t\t\t\tcorecss.rel = \"stylesheet\";\n\t\t\t\tcorecss.href = corecssurl;\n\t\t}\n\t\tdocument.head.appendChild( corecss );\n\t\tvar themecssurl = \"/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?m=1363304414i&amp;ver=3.0.9b\";\n\t\tif ( themecss.setAttribute ) {\n\t\t\t\tthemecss.setAttribute( \"rel\", \"stylesheet\" );\n\t\t\t\tthemecss.setAttribute( \"type\", \"text/css\" );\n\t\t\t\tthemecss.setAttribute( \"href\", themecssurl );\n\t\t} else {\n\t\t\t\tthemecss.rel = \"stylesheet\";\n\t\t\t\tthemecss.href = themecssurl;\n\t\t}\n\t\tdocument.head.appendChild( themecss );\n\t})();\n\tSyntaxHighlighter.config.strings.expandSource = '+ expand source';\n\tSyntaxHighlighter.config.strings.help = '?';\n\tSyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\\n\\n';\n\tSyntaxHighlighter.config.strings.noBrush = 'Can\\'t find brush for: ';\n\tSyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\\'t configured for html-script option: ';\n\tSyntaxHighlighter.defaults['pad-line-numbers'] = false;\n\tSyntaxHighlighter.defaults['toolbar'] = false;\n\tSyntaxHighlighter.all();\n\n\t// Infinite scroll support\n\tdocument.addEventListener( 'is.post-load', function () {\n\t\tSyntaxHighlighter.highlight();\n\t} );(function () {\n\t\t\tvar wpcom_reblog = {\n\t\t\t\tsource: 'toolbar',\n\n\t\t\t\ttoggle_reblog_box_flair: function (obj_id, post_id) {\n\n\t\t\t\t\t// Go to site selector. This will redirect to their blog if they only have one.\n\t\t\t\t\tconst postEndpoint = `https://wordpress.com/post`;\n\n\t\t\t\t\t// Ideally we would use the permalink here, but fortunately this will be replaced with the \n\t\t\t\t\t// post permalink in the editor.\n\t\t\t\t\tconst originalURL = `${ document.location.href }?page_id=${ post_id }`; \n\t\t\t\t\t\n\t\t\t\t\tconst url =\n\t\t\t\t\t\tpostEndpoint +\n\t\t\t\t\t\t'?url=' +\n\t\t\t\t\t\tencodeURIComponent( originalURL ) +\n\t\t\t\t\t\t'&is_post_share=true' +\n\t\t\t\t\t\t'&v=5';\n\n\t\t\t\t\tconst redirect = function () {\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t! window.open( url, '_blank' )\n\t\t\t\t\t\t) {\n\t\t\t\t\t\t\tlocation.href = url;\n\t\t\t\t\t\t}\n\t\t\t\t\t};\n\n\t\t\t\t\tif ( /Firefox/.test( navigator.userAgent ) ) {\n\t\t\t\t\t\tsetTimeout( redirect, 0 );\n\t\t\t\t\t} else {\n\t\t\t\t\t\tredirect();\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t};\n\n\t\t\twindow.wpcom_reblog = wpcom_reblog;\n\t\t})();/* <![CDATA[ */\nwp.i18n.setLocaleData( { 'text direction\\u0004ltr': [ 'ltr' ] } );\n//# sourceURL=wp-i18n-js-after\n/* ]]> *//* <![CDATA[ */\nwindow.VerbumComments = {\"Log in or provide your name and email to leave a reply.\":\"Log in or provide your name and email to leave a reply.\",\"Log in or provide your name and email to leave a comment.\":\"Log in or provide your name and email to leave a comment.\",\"Receive web and mobile notifications for posts on this site.\":\"Receive web and mobile notifications for posts on this site.\",\"Name\":\"Name\",\"Email (address never made public)\":\"Email (address never made public)\",\"Website (optional)\":\"Website (optional)\",\"Leave a reply. (log in optional)\":\"Leave a reply. (log in optional)\",\"Leave a comment. (log in optional)\":\"Leave a comment. (log in optional)\",\"Log in to leave a reply.\":\"Log in to leave a reply.\",\"Log in to leave a comment.\":\"Log in to leave a comment.\",\"Logged in via %s\":\"Logged in via %s\",\"Log out\":\"Log out\",\"Email\":\"Email\",\"(Address never made public)\":\"(Address never made public)\",\"Instantly\":\"Instantly\",\"Daily\":\"Daily\",\"Reply\":\"Reply\",\"Comment\":\"Comment\",\"WordPress\":\"WordPress\",\"Weekly\":\"Weekly\",\"Notify me of new posts\":\"Notify me of new posts\",\"Email me new posts\":\"Email me new posts\",\"Email me new comments\":\"Email me new comments\",\"Cancel\":\"Cancel\",\"Write a comment...\":\"Write a comment...\",\"Write a reply...\":\"Write a reply...\",\"Website\":\"Website\",\"Optional\":\"Optional\",\"We'll keep you in the loop!\":\"We'll keep you in the loop!\",\"Loading your comment...\":\"Loading your comment...\",\"Discover more from\":\"Discover more from Bumbershoot Software\",\"Subscribe now to keep reading and get access to the full archive.\":\"Subscribe now to keep reading and get access to the full archive.\",\"Continue reading\":\"Continue reading\",\"Never miss a beat!\":\"Never miss a beat!\",\"Interested in getting blog post updates? Simply click the button below to stay in the loop!\":\"Interested in getting blog post updates? Simply click the button below to stay in the loop!\",\"Enter your email address\":\"Enter your email address\",\"Subscribe\":\"Subscribe\",\"Comment sent successfully\":\"Comment sent successfully\",\"Save my name, email, and website in this browser for the next time I comment.\":\"Save my name, email, and website in this browser for the next time I comment.\",\"hovercardi18n\":{\"Edit your profile \\u2192\":\"Edit your profile \\u2192\",\"View profile \\u2192\":\"View profile \\u2192\",\"Contact\":\"Contact\",\"Send money\":\"Send money\",\"Profile not found.\":\"Profile not found.\",\"Too Many Requests.\":\"Too Many Requests.\",\"Internal Server Error.\":\"Internal Server Error.\",\"Sorry, we are unable to load this Gravatar profile.\":\"Sorry, we are unable to load this Gravatar profile.\"},\"siteId\":67482574,\"postId\":5480,\"mustLogIn\":false,\"requireNameEmail\":true,\"commentRegistration\":false,\"connectURL\":\"https://bumbershootsoft.wordpress.com/public.api/connect/?action=request\\u0026from_comments=yes\",\"logoutURL\":\"https://bumbershootsoft.wordpress.com/wp-login.php?action=logout\\u0026_wpnonce=63b0a3268d\",\"homeURL\":\"https://bumbershootsoft.wordpress.com/\",\"subscribeToBlog\":true,\"subscribeToComment\":true,\"isJetpackCommentsLoggedIn\":false,\"jetpackUsername\":\"\",\"jetpackUserId\":0,\"jetpackSignature\":\"\",\"jetpackAvatar\":\"https://0.gravatar.com/avatar/?s=96\\u0026amp;d=identicon\\u0026amp;r=G\",\"enableBlocks\":true,\"enableSubscriptionModal\":true,\"currentLocale\":\"en\",\"isJetpackComments\":false,\"allowedBlocks\":[\"core/paragraph\",\"core/list\",\"core/code\",\"core/list-item\",\"core/quote\",\"core/image\",\"core/embed\",\"core/quote\",\"core/code\"],\"embedNonce\":\"46bb44bfa2\",\"verbumBundleUrl\":\"/wp-content/mu-plugins/jetpack-mu-wpcom-plugin/sun/jetpack_vendor/automattic/jetpack-mu-wpcom/src/features/verbum-comments/dist/index.js\",\"isRTL\":false,\"vbeCacheBuster\":1738686361,\"iframeUniqueId\":0,\"colorScheme\":false}\n//# sourceURL=verbum-settings-js-before\n/* ]]> *//* <![CDATA[ */\nvar wpcom_coblocks_js = {\"coblocks_masonry_js\":\"https://s2.wp.com/wp-content/plugins/coblocks/2.18.1-simple-rev.4/dist/js/coblocks-masonry.min.js?m=1681832297i\",\"coblocks_lightbox_js\":\"https://s2.wp.com/wp-content/plugins/coblocks/2.18.1-simple-rev.4/dist/js/coblocks-lightbox.min.js?m=1681832297i\",\"jquery_core_js\":\"/wp-includes/js/jquery/jquery.min.js\",\"jquery_migrate_js\":\"/wp-includes/js/jquery/jquery-migrate.min.js\",\"masonry_js\":\"/wp-includes/js/masonry.min.js\",\"imagesloaded_js\":\"/wp-includes/js/imagesloaded.min.js\"};\nvar coblocksLigthboxData = {\"closeLabel\":\"Close Gallery\",\"leftLabel\":\"Previous\",\"rightLabel\":\"Next\"};\n//# sourceURL=coblocks-loader-js-extra\n/* ]]> *//* <![CDATA[ */\nvar jetpackSwiperLibraryPath = {\"url\":\"/wp-content/mu-plugins/jetpack-plugin/sun/_inc/blocks/swiper.js\"};\nvar jetpackCarouselStrings = {\"widths\":[370,700,1000,1200,1400,2000],\"is_logged_in\":\"\",\"lang\":\"en\",\"ajaxurl\":\"https://bumbershootsoft.wordpress.com/wp-admin/admin-ajax.php\",\"nonce\":\"9de2ba6d80\",\"display_exif\":\"1\",\"display_comments\":\"1\",\"single_image_gallery\":\"1\",\"single_image_gallery_media_file\":\"\",\"background_color\":\"black\",\"comment\":\"Comment\",\"post_comment\":\"Post Comment\",\"write_comment\":\"Write a Comment...\",\"loading_comments\":\"Loading Comments...\",\"image_label\":\"Open image in full-screen.\",\"download_original\":\"View full size \\u003Cspan class=\\\"photo-size\\\"\\u003E{0}\\u003Cspan class=\\\"photo-size-times\\\"\\u003E\\u00d7\\u003C/span\\u003E{1}\\u003C/span\\u003E\",\"no_comment_text\":\"Please be sure to submit some text with your comment.\",\"no_comment_email\":\"Please provide an email address to comment.\",\"no_comment_author\":\"Please provide your name to comment.\",\"comment_post_error\":\"Sorry, but there was an error posting your comment. Please try again later.\",\"comment_approved\":\"Your comment was approved.\",\"comment_unapproved\":\"Your comment is in moderation.\",\"camera\":\"Camera\",\"aperture\":\"Aperture\",\"shutter_speed\":\"Shutter Speed\",\"focal_length\":\"Focal Length\",\"copyright\":\"Copyright\",\"comment_registration\":\"0\",\"require_name_email\":\"1\",\"login_url\":\"https://bumbershootsoft.wordpress.com/wp-login.php?redirect_to=https%3A%2F%2Fbumbershootsoft.wordpress.com%2F2026%2F01%2F17%2Fti-99-4a-leaning-more-heavily-on-the-firmware%2F\",\"blog_id\":\"67482574\",\"meta_data\":[\"camera\",\"aperture\",\"shutter_speed\",\"focal_length\",\"copyright\"],\"stats_query_args\":\"blog=67482574&v=wpcom&tz=-7&user_id=0&subd=bumbershootsoft\",\"is_public\":\"1\"};\n//# sourceURL=jetpack-carousel-js-extra\n/* ]]> *//* <![CDATA[ */\nvar sharing_js_options = {\"lang\":\"en\",\"counts\":\"1\",\"is_stats_active\":\"1\"};\n//# sourceURL=sharing-js-js-extra\n/* ]]> *//* <![CDATA[ */\nwindow.JetpackScriptData = {\"site\":{\"host\":\"wpcom\",\"is_wpcom_platform\":true}};\n//# sourceURL=jetpack-script-data-js-before\n/* ]]> *//* <![CDATA[ */\nvar windowOpen;\n\t\t\t( function () {\n\t\t\t\tfunction matches( el, sel ) {\n\t\t\t\t\treturn !! (\n\t\t\t\t\t\tel.matches && el.matches( sel ) ||\n\t\t\t\t\t\tel.msMatchesSelector && el.msMatchesSelector( sel )\n\t\t\t\t\t);\n\t\t\t\t}\n\n\t\t\t\tdocument.body.addEventListener( 'click', function ( event ) {\n\t\t\t\t\tif ( ! event.target ) {\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\n\t\t\t\t\tvar el;\n\t\t\t\t\tif ( matches( event.target, 'a.share-twitter' ) ) {\n\t\t\t\t\t\tel = event.target;\n\t\t\t\t\t} else if ( event.target.parentNode && matches( event.target.parentNode, 'a.share-twitter' ) ) {\n\t\t\t\t\t\tel = event.target.parentNode;\n\t\t\t\t\t}\n\n\t\t\t\t\tif ( el ) {\n\t\t\t\t\t\tevent.preventDefault();\n\n\t\t\t\t\t\t// If there's another sharing window open, close it.\n\t\t\t\t\t\tif ( typeof windowOpen !== 'undefined' ) {\n\t\t\t\t\t\t\twindowOpen.close();\n\t\t\t\t\t\t}\n\t\t\t\t\t\twindowOpen = window.open( el.getAttribute( 'href' ), 'wpcomtwitter', 'menubar=1,resizable=1,width=600,height=350' );\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t} );\n\t\t\t} )();\nvar windowOpen;\n\t\t\t( function () {\n\t\t\t\tfunction matches( el, sel ) {\n\t\t\t\t\treturn !! (\n\t\t\t\t\t\tel.matches && el.matches( sel ) ||\n\t\t\t\t\t\tel.msMatchesSelector && el.msMatchesSelector( sel )\n\t\t\t\t\t);\n\t\t\t\t}\n\n\t\t\t\tdocument.body.addEventListener( 'click', function ( event ) {\n\t\t\t\t\tif ( ! event.target ) {\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\n\t\t\t\t\tvar el;\n\t\t\t\t\tif ( matches( event.target, 'a.share-facebook' ) ) {\n\t\t\t\t\t\tel = event.target;\n\t\t\t\t\t} else if ( event.target.parentNode && matches( event.target.parentNode, 'a.share-facebook' ) ) {\n\t\t\t\t\t\tel = event.target.parentNode;\n\t\t\t\t\t}\n\n\t\t\t\t\tif ( el ) {\n\t\t\t\t\t\tevent.preventDefault();\n\n\t\t\t\t\t\t// If there's another sharing window open, close it.\n\t\t\t\t\t\tif ( typeof windowOpen !== 'undefined' ) {\n\t\t\t\t\t\t\twindowOpen.close();\n\t\t\t\t\t\t}\n\t\t\t\t\t\twindowOpen = window.open( el.getAttribute( 'href' ), 'wpcomfacebook', 'menubar=1,resizable=1,width=600,height=400' );\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t} );\n\t\t\t} )();\n//# sourceURL=sharing-js-js-after\n/* ]]> */{\"baseUrl\":\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/\",\"ext\":\".png\",\"svgUrl\":\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/\",\"svgExt\":\".svg\",\"source\":{\"concatemoji\":\"/wp-includes/js/wp-emoji-release.min.js?m=1764078722i&ver=6.9-RC2-61304\"}}/* <![CDATA[ */\n/*! This file is auto-generated */\nconst a=JSON.parse(document.getElementById(\"wp-emoji-settings\").textContent),o=(window._wpemojiSettings=a,\"wpEmojiSettingsSupports\"),s=[\"flag\",\"emoji\"];function i(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function c(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data);e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0);const a=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data);return t.every((e,t)=>e===a[t])}function p(e,t){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var n=e.getImageData(16,16,1,1);for(let e=0;e<n.data.length;e++)if(0!==n.data[e])return!1;return!0}function u(e,t,n,a){switch(t){case\"flag\":return n(e,\"\\ud83c\\udff3\\ufe0f\\u200d\\u26a7\\ufe0f\",\"\\ud83c\\udff3\\ufe0f\\u200b\\u26a7\\ufe0f\")?!1:!n(e,\"\\ud83c\\udde8\\ud83c\\uddf6\",\"\\ud83c\\udde8\\u200b\\ud83c\\uddf6\")&&!n(e,\"\\ud83c\\udff4\\udb40\\udc67\\udb40\\udc62\\udb40\\udc65\\udb40\\udc6e\\udb40\\udc67\\udb40\\udc7f\",\"\\ud83c\\udff4\\u200b\\udb40\\udc67\\u200b\\udb40\\udc62\\u200b\\udb40\\udc65\\u200b\\udb40\\udc6e\\u200b\\udb40\\udc67\\u200b\\udb40\\udc7f\");case\"emoji\":return!a(e,\"\\ud83e\\u1fac8\")}return!1}function f(e,t,n,a){let r;const o=(r=\"undefined\"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):document.createElement(\"canvas\")).getContext(\"2d\",{willReadFrequently:!0}),s=(o.textBaseline=\"top\",o.font=\"600 32px Arial\",{});return e.forEach(e=>{s[e]=t(o,e,n,a)}),s}function r(e){var t=document.createElement(\"script\");t.src=e,t.defer=!0,document.head.appendChild(t)}a.supports={everything:!0,everythingExceptFlag:!0},new Promise(t=>{let n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if(\"object\"==typeof e&&\"number\"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&\"object\"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if(\"undefined\"!=typeof Worker&&\"undefined\"!=typeof OffscreenCanvas&&\"undefined\"!=typeof URL&&URL.createObjectURL&&\"undefined\"!=typeof Blob)try{var e=\"postMessage(\"+f.toString()+\"(\"+[JSON.stringify(s),u.toString(),c.toString(),p.toString()].join(\",\")+\"));\",a=new Blob([e],{type:\"text/javascript\"});const r=new Worker(URL.createObjectURL(a),{name:\"wpTestEmojiSupports\"});return void(r.onmessage=e=>{i(n=e.data),r.terminate(),t(n)})}catch(e){}i(n=f(s,u,c,p))}t(n)}).then(e=>{for(const n in e)a.supports[n]=e[n],a.supports.everything=a.supports.everything&&a.supports[n],\"flag\"!==n&&(a.supports.everythingExceptFlag=a.supports.everythingExceptFlag&&a.supports[n]);var t;a.supports.everythingExceptFlag=a.supports.everythingExceptFlag&&!a.supports.flag,a.supports.everything||((t=a.source||{}).concatemoji?r(t.concatemoji):t.wpemoji&&t.twemoji&&(r(t.twemoji),r(t.wpemoji)))});\n//# sourceURL=/wp-includes/js/wp-emoji-loader.min.js\n/* ]]> */%d_tkq = window._tkq || [];\n_stq = window._stq || [];\n_tkq.push(['storeContext', {'blog_id':'67482574','blog_tz':'-7','user_lang':'en','blog_lang':'en','user_id':'0'}]);\n\t\t// Prevent sending pageview tracking from WP-Admin pages.\n\t\t_stq.push(['view', {'blog':'67482574','v':'wpcom','tz':'-7','user_id':'0','post':'5480','subd':'bumbershootsoft'}]);\n\t\t_stq.push(['extra', {'crypt':'UE5XaGUuOTlwaD85flAmcm1mcmZsaDhkV11YdTdvUG14Q2VDQTR4LlUsLi82dU1mai9BMkM9QjRQelhINGFxVndjWkE1RkswMG42XzM2WHMuUmpFdEgxeC4zK1RHOWFZa1EzSV03fmp5eGNJK0cxJldhVn5QREJQdGtGYktYT3JvVlpVYV1RRHZwVyZJUERhUS5FaDJZcTd0aWsrQi1WbG1vZEpqdUdJWGJsU0M5YVNlRkxBMnJuQUp0c2tFd2JdQSwtRE9TbWUxP2R4THBqNkpuQ0tqLUx+c0ZjVHF3cS4mbW9HQT95Xy1vNDFnN3dEV2oyVTliMXpXW1luS1IxVmhWdytycXRXflNpcTY0alVtanJKNVBUTEpnczE4WStRcXRHRWxMcWtpWmZlejE4ZDg0MDVlJXBOY1FhQnBxRHhQbXhfLzFwdGIwLUpfWiVbeFFXWCxreFRGTmZybnFKPU1PeEQuJi5fX0laenxHRl80b1VIa1B4TXh0NA=='}]);\n_stq.push([ 'clickTrackerInit', '67482574', '5480' ]);Design a site like this with WordPress.comGet startedwindow._tkq = window._tkq || [];\n\n\t\t\twindow._tkq.push( [ 'recordEvent', 'wpcom_marketing_bar_impression', {\"is_current_user_blog_owner\":false} ] );\n\n\t\t\tdocument.querySelectorAll( '#marketingbar > a' ).forEach( link => {\n\t\t\t\tlink.addEventListener( 'click', ( e ) => {\n\t\t\t\t\twindow._tkq.push( [ 'recordEvent', 'wpcom_marketing_bar_cta_click', {\"is_current_user_blog_owner\":false} ] );\n\t\t\t\t} );\n\t\t\t});(function() {\n\t'use strict';\n\n\tconst fetches = {};\n\tconst promises = {};\n\tconst urls = {\n\t\t'wp-polyfill': '/wp-includes/js/dist/vendor/wp-polyfill.min.js?m=1764669441i&ver=3.15.0',\n\t\t'verbum': '/wp-content/mu-plugins/jetpack-mu-wpcom-plugin/sun/jetpack_vendor/automattic/jetpack-mu-wpcom/src/build/verbum-comments/verbum-comments.js?m=1767826947i&minify=false&ver=f277b2cf343fcc8861c2'\n\t};\n\tconst loaders = {\n\t\t'verbum': () => {\n\t\t\tfetchExternalScript('wp-polyfill');\n\t\t\tfetchExternalScript('verbum');\n\t\t\tpromises['wp-polyfill'] = promises['wp-polyfill'] || loadWPScript('wp-polyfill');\n\t\t\tpromises['verbum'] = promises['verbum'] || promises['wp-polyfill'].then( () => loadWPScript('verbum') );\n\t\t\treturn promises['verbum'];\n\t\t},\n\t\t\n\t};\n\tconst scriptExtras = {\n\t\t\n\t};\n\n\twindow.WP_Enqueue_Dynamic_Script = {\n\t\tloadScript: (handle) => {\n\t\t\tif (!loaders[handle]) {\n\t\t\t\tconsole.error('WP_Enqueue_Dynamic_Script: unregistered script `' + handle + '`.');\n\t\t\t}\n\t\t\treturn loaders[handle]();\n\t\t}\n\t};\n\n\tfunction fetchExternalScript(handle) {\n\t\tif (!urls[handle]) {\n\t\t\treturn Promise.resolve();\n\t\t}\n\n\t\tfetches[handle] = fetches[handle] || fetch(urls[handle], { mode: 'no-cors' });\n\t\treturn fetches[handle];\n\t}\n\n\tfunction runExtraScript(handle, type, index) {\n\t\tconst id = 'wp-enqueue-dynamic-script:' + handle + ':' + type + ':' + (index + 1);\n\t\tconst template = document.getElementById(id);\n\t\tif (!template) {\n\t\t\treturn Promise.reject();\n\t\t}\n\n\t\tconst script = document.createElement( 'script' );\n\t\tscript.innerHTML = template.innerHTML;\n\t\tdocument.body.appendChild( script );\n\t\treturn Promise.resolve();\n\t}\n\n\tfunction loadExternalScript(handle) {\n\t\tif (!urls[handle]) {\n\t\t\treturn Promise.resolve();\n\t\t}\n\n\t\treturn fetches[handle].then(() => {\n\t\t\treturn new Promise((resolve, reject) => {\n\t\t\t\tconst script = document.createElement('script');\n\t\t\t\tscript.onload = () => resolve();\n\t\t\t\tscript.onerror = (e) => reject(e);\n\t\t\t\tscript.src = urls[handle];\n\t\t\t\tdocument.body.appendChild(script);\n\t\t\t});\n\t\t});\n\t}\n\n\tfunction loadExtra(handle, pos) {\n\t\tconst count = (scriptExtras[handle] && scriptExtras[handle][pos]) || 0;\n\t\tlet promise = Promise.resolve();\n\n\t\tfor (let i = 0; i < count; i++) {\n\t\t\tpromise = promise.then(() => runExtraScript(handle, pos, i));\n\t\t}\n\n\t\treturn promise;\n\t}\n\n\tfunction loadWPScript(handle) {\n\t\t// Core loads scripts in this order. See: https://github.com/WordPress/WordPress/blob/a59eb9d39c4fcba834b70c9e8dfd64feeec10ba6/wp-includes/class-wp-scripts.php#L428.\n\t\treturn loadExtra(handle, 'translations')\n\t\t\t.then(() => loadExtra(handle, 'before'))\n\t\t\t.then(() => loadExternalScript(handle))\n\t\t\t.then(() => loadExtra(handle, 'after'));\n\t}\n} )();",
      "title": "TI-99/4A: Leaning More Heavily on the Firmware | Bumbershoot Software",
      "source": "bumbershootsoft.wordpress.com",
      "content": "I kicked off last year with a look at the TI-99/4A home computer. I’d started out playing with BASIC and then moved on to translating some of those BASIC programs into machine language and its custom “Graphics Programming Language” bytecode to see how to approach more serious work. My final few BASIC programs ended in disaster, though, so I skipped those, and of course a handful of BASIC programs weren’t going to hit every facility the system offered either. When surveying the parts I’d missed, I still gave only cursory attention to some of those facilities. Much of the rest of my 2025 revolved around mastering the TI’s graphics chip as it appeared in other systems, and that expertise should let me return and have a better sense of what, exactly, I am looking at if I lean on the firmware’s capabilities.\nIn particular, I want to look at its enhanced support for sound and sprites. This will mostly be done in the context of the Graphics Programming Language, both because I didn’t do much with that either last year and because many of the features are clearly designed to integrate with it.\nWe still won’t precisely match the BASIC originals. Happily, that’s because this time, stuff will actually work.\nWhere We Were\nLast year, we learned that TI-99/4A cartridges had two different blocks of memory in them; an ordinary part that mapped into 8KB at location >6000->7FFF (the ROM) and another part, that normally held Graphics Programming Language bytecode, mapped into a completely separate “Graphics ROM” address space from >6000->F7FF (the “GROM”). Which means, now that I return to the TI after an absence, that I’ll also need to reiterate a few of the weird caveats about the system:\nThe tooling around the system uses > as a prefix to represent hexadecimal constants. My usual practice on this blog is to use $ for this, but I follow the system conventions when writing about the TI.\n“Graphics Programming Language” is usually abbreviated to “GPL” and the code written in it, in both source and bytecoded forms, is “GPL code”; since that means something else these days I will usually call it “GROM code” instead. This isn’t period practice but I think the extra clarity is worth it.\nIndividual GROMs occupy 8KB of address space but are only 6KB in size; the real address space is >6000->77FF, >8000->97FF, and so on. This will figure into some of our design decisions later.\nGROM instructions don’t seem to have consistent names from source to source. I am leaning on two main sources: the TI-99/4A Tech Pages and the behavior of the xdt99 development toolkit. Since I’m using xdt99 to produce these programs, where these two sources disagree I will use the xdt99 names.\nThe GROM program that we wrote last year would set up the screen for text display, define some custom graphics, and give us a little banner:\nThe code for this is pretty brief. (We discussed this code more fully in the second link up top.)\nGROM >6000\nAORG >0000\nDATA >AA01,0,0,MENU\nDATA 0,0,0,0\nMENU\nDATA 0,START\nSTRI \"GROM BANNER\"\nSTART\nALL >20\n* CLEAR SCREEN\nBACK >12\nDST >0900,@>834A\nCALL >0018\n* SET UPPERCASE CHARSET\nDST >0B00,@>834A\nCALL >004A\n* SET LOWERCASE CHARSET\nMOVE 16,G@COLS,V@>0380\n* ASSIGN COLORS\nMOVE 8,G@GFX1,V@>0AF0\n* ASSIGN CUSTOM CHARACTERS\nMOVE 32,G@GFX2,V@>0B00\nMOVE 32,G@GFX3,V@>0B40\nFMT\nROW 10\nCOL 0\nHTEXT '\npppppppppppppppppppppppppppp\n'\nHTEXT '\np`bppBUMBERSHOOTpSOFTWAREppp\n'\nHTEXT '\npak16pMOREpTHANpYOURpTI-83^p\n'\nHTEXT '\npppppppppppppppppppppppppppp\n'\nFEND\nLOOP\nSCAN\nBR LOOP\nEXIT\nCOLS\nBYTE >10,>10,>10,>10,>10,>F1,>F1,>F1\nBYTE >F1,>F1,>F1,>F1,>41,>A1,>11,>10\nGFX1\nBYTE >00,>10,>10,>10,>10,>10,>00,>10\nGFX2\nBYTE >03,>0F,>1F,>3F,>7F,>7F,>FF,>FF\nBYTE >FF,>FE,>7C,>78,>30,>00,>00,>00\nBYTE >C0,>F0,>F8,>F8,>F0,>E0,>C0,>80\nBYTE >00,>00,>00,>00,>00,>00,>00,>00\nGFX3\nBYTE >00,>00,>00,>00,>00,>00,>00,>00\nBYTE >00,>00,>00,>00,>00,>00,>00,>00\nBYTE >00,>00,>00,>00,>00,>00,>00,>00\nBYTE >80,>C0,>60,>30,>18,>08,>38,>00\nEND START\nMeanwhile, we ended our time in the world of BASIC by turning the little umbrella graphic above into sprites and letting them careen around the world. This ended poorly, because we needed two sprites per umbrella, and by the time we got the handle moving, the canopy had already moved a few frames:\nThe plan for this week is to add those sprites to our code, have them move properly, let them bounce off each other if they collide, and also play a tune in the background as we do.\nWe’ll start with the music first; it’s more self-contained.\nSound Lists\nThe core API for the sound lists is deceptively simple: we put the word address of the list at >83CC and write the byte value >01 to >83CE, and the interrupt service routine does the rest. The sound list itself is also very simple: it’s just a series of records, where each record is, in order:\nA byte specifying the number of bytes to write to the sound chip’s data port.\nThat many bytes of data.\nThe number of frames to wait before processing the next record, or a zero if this is the last entry in the list.\nThat’s really it, but there are a few gotchas and caveats that we need to be aware of. If you’re doing everything in GROM, everything works out automatically, but if you aren’t…\nInterrupts must be enabled for anything to happen. If you’re writing machine code, don’t forget your LIMI 2 once you’re done with your frame.\nThe TI-99/4A has three address spaces (CPU, VRAM, and GROM) and we need to tell the system which one we put it in. This is controlled by the least significant bit of >83FD: a 1 bit means the table is in VRAM, and a 0 bit means it’s in GROM. We cannot run sound lists out of the main cartridge ROM. (The default is 0, so, again, an all-GROM approach works out of the box.)\nThe sound processing routine must not be disabled. It won’t be, unless we disabled it ourselves, but the >80 and >20 bits of the byte at >83C2 do have to be zero for any of this to work.\nTo put the sound system through its paces, I took the Bach minuet I’d used as background music for my SNES and Genesis projects and converted the score to work with the TI’s SN76489 instead. This highlighted a number of additional issues for me. The first was that the base range of this sound chip is frankly not great. The lowest possible tone you can hit is about 110 Hz, which in turn means you only get a few notes below Low C, and which in particular means that I run out of range just before I try to play the G that is the core of the bass line of the Minuet In G. I had to transpose things a bit to get away with this. The second thing we see is that complete songs like this are an awkward fit for the system; it’s clearly imagining itself to be used for sound effects, or little jingles at most. The sound list is total—it is either in use or not and there is no mechanism for mixing together two sources. There’s also no looping mechanism—the best we can do is determine that the sound list system has shut itself down because >83CE is zero again. Furthermore, since sound lists operate at the raw register-write level, lists get very long very fast—if we’re putting any kind of volume envelope on our notes, every step in the volume on every note will each end up being its own record in the sound list. Even with only two voices and nothing more complex than turning notes on and off, this minuet clocked in at nearly 5 kilobytes. GROMs only offer 6KB of contiguous data at a time, so we’re charging towards hardware limits on our very first attempt.\nFor this song in particular, we do have a simple escape. The minuet has an “AABB” structure; there are really only two parts to the song and they each repeat once. As long as we have to watch for the end of the sound list so that we can loop the song, we might as well have two sound lists, one for the A part and one for the B part, and manage that repetition in the main program logic. That cuts our sound data neatly in half, which results in a much more manageable program.\nIn the more general case, though, I think the real solution is to abandon sound lists. They just aren’t a great fit for music playback; you’re better off writing your own playroutine that can manage per-note envelopes and which permits more compact representations of notes. That’s what I’ve done in my previous work with the chip and I don’t think the TI’s sound lists can sensibly replace it. That said, I think there is still a place for it if you wish to mix music and sound effects. The sound lists are, after all, just a data stream of register writes. As long as you can ensure that your music routine won’t be interrupted by the sound list processor—and if it’s part of an interrupt handling routine you get that for free—then if the music playback doesn’t touch the same voices as our sound effects then we can use both systems at once. Since we also can trivially know if sound lists are playing, we could even imagine tweaking the music system to share channels with it and leave any preempted channels alone.\nThat’s overkill for this, though. Here’s my new playback code for managing repetition and looping while still scanning the keyboard for a quit command:\nMOVE 10,G@SONGPAT,V@>1000\n* COPY SONG LIST TO VRAM\nMUSICLP DST >1000,@>8300\n* INITIALIZE SONG POINTER\nMUSIC\nDCZ V*>8300\n* DOES SONG POINTER POINT TO 0?\nBS MUSICLP\n* IF SO, RESET TO START OF LIST\nDST V*>8300,@>83CC\n* OTHERWISE COPY TO SOUND LIST\nDINCT @>8300\n* AND ADVANCE POINTER A WORD\nST 1,@>83CE\n* START PLAYBACK\nLOOP\nSCAN\n* KEY HIT?\nBS DONE\n* IF SO, EXIT PROGRAM\nCZ @>83CE\n* IS SOUND LIST STILL PLAYING?\nBR LOOP\n* IF SO, BACK TO KEYBOARD LOOP\nB MUSIC\n* IF NOT, LOAD NEXT SOUND LIST\nDONE\nEXIT\n* RETURN TO BOOT SCREEN\n* MUSIC DATA\nSONGPAT DATA MINUETA,MINUETA,MINUETB,MINUETB,0\nMINUETA BCOPY \"minuet_a.bin\"\nMINUETB BCOPY \"minuet_b.bin\"\nI think this is the first GROM code I’ve shown that actually attempts to do any indirection. As it turns out, we do not have as free a hand with this as we do in the MOVE commands. We can only index with constants, we can’t access GROM through a pointer, and all pointers have to be in the CPU memory but specifically in the 256-byte scratchpad RAM. We’re not really constrained here, but it’s annoyingly restrictive for what has so far been a much more freewheeling and general-purpose bytecode. I’m particularly annoyed by the fact that I seem to need to copy my table of pointers into VRAM in order to actually iterate through it. I have some ideas for potentially improving this, but they’ll have to wait for next week.\nAn Alternate Approach: Explicit I/O in the GROM\nI’ve described the process above in a way that’s generic enough that you can use the same technique in the GROM or in native code. The bytecode language, however, includes an explicit I/O instruction that lets you command the start of a sound list directly. For my use cases, though, it doesn’t really save you any instructions:\nInstead of loading the GROM or VRAM address into >83CC, load it into any other place in the scratchpad memory.\nInstead of starting playback with a write to >83CE, give the command I/O x,addr where x is 0 for a GROM sound list and 1 for a VRAM sound list, and addr is the scratchpad address that holds the actual starting point.\nThe only place I can see this actually being more concise than just doing it “by hand” is if you’re rapidly shifting between GROM and VRAM for your sound list locations and don’t want to keep rewriting the system flags by hand. But it is there, and the I/O instruction also transparently manages other things like cassette I/O, so it’s nice to see it there.\nAutomatic Sprite Motion\nNow that we have a soundtrack, let’s look at bringing back the animated sprites. We have a little more to deal with here than we did in BASIC, because the VDP’s memory layout is not the same when we boot into GROM. (Indeed, the boot configuration, TI BASIC, TI Extended BASIC, and the Editor/Assembler package are all different, sometimes drastically.) The automatic sprite motion system only puts two constraints on us: the Sprite Attribute Table must be at >0300, (which is to say, VDP Register 5 must hold the value >06), and the sprite motion tables are forced to be in VRAM from >0780->07FF. We’ll need to arrange everything else so that nothing else we care about is in this space.\nRecords in the sprite motion table are, in order, the Y velocity, the X velocity, and two bytes that the interrupt routine uses as scratch space. Velocities are expressed in units of “pixels per 16 frames”, or, alternately, as pixels per frame represented in a 4.4 fixed-point format.\nImplementing Sprite Display\nOf course, before we start moving any sprites, we’ll have to define and draw them in the first place. And before we do that, we have to properly configure the VDP. Here are the default values we boot into:\nRegister 0: >00. We are not in bitmap mode.\nRegister 1: >E0. We have all 16KB of VRAM available, we’re in Graphics 1 mode, the display and and display interrupt are both enabled, and sprites are 8×8 unmagnified.\nRegister 2: >00. The Name Table is at >0000.\nRegister 3: >0E. The Color Table is at >0380.\nRegister 4: >01. The Pattern Table is at >0800. (That’s why we load our fonts into >0900; it’s the start of pattern number 32, and thus where the space character should go.)\nRegister 5: >06. The Sprite Attribute Table is at >0300. (That’s just what we want. Good.)\nRegister 6: >00. The Sprite Pattern Table is at >0000. That conflicts with a bunch of our other spaces, including the sprite motion table itself, but as long as we’re only using sprite patterns 128 through 240, no conflicts will actually matter.\nRegister 7: >17. The default colors are black on cyan.\nThis is mostly fine; we’ll only need to adjust two registers. Register 7 doesn’t have the colors we want, but the BACK >12 instruction ultimately is just a write to that register, so we’ve handled that one already. Register 1 controls, among other things, sprite size, and we want 16×16 magnified sprites. That means the value we want there is >E3. The MOVE command lets us copy data into the VDP registers, which is great for bulk initialization but a little off-base for just writing one. I bounce the value through the CPU scratchpad:\nST >E3,@>83D4\nMOVE 1,@>83D4,#1\nThe screensaver functionality in the firmware needs VDP register 1 to be read-write, and it isn’t; >83D4 is the RAM location where it keeps its own copy of the register value, and normally whenever we write register 1 we need to update the value here first. This code does that, but normally we don’t need to do that in the GROMs; MOVE is smart enough to update the shadow on its own whenever you update this register. Handy, but not for us just now.\nThe logic from our original BASIC program also doesn’t port directly. BASIC sets up VRAM so that the Sprite Pattern Table overlaps with the main Pattern Table, while the boot environment sets it up so that it instead overlaps with everything else. BASIC is mainly buying the ability to define sprite graphics with CALL CHAR there, but we’d generally prefer the extra graphics space the boot environment gives us. Since we can’t reuse the graphics data we copied into the character pattern table, we’ll need to also copy the umbrella pattern into two places in VRAM during startup. That’s a single line of GPL:\nSprites work the same here as they do anywhere else on the TMS9918A; we copy the attributes for our four sprites into place in the Sprite Attribute Table along with a terminating >D0 byte.\nMOVE 17,G@SPRATTR,V@>0300\nThe sprite data itself has the canvas of the umbrella as sprite pattern 128 and the handle as sprite pattern 132. Placing these is just a table of constants:\nSPRATTR BYTE 80,60,>80,4,80,60,>84,10\nBYTE 140,160,>80,7,140,160,>84,14\nBYTE >D0\nNow to make them move.\nImplementing Sprite Motion\nAutomatic sprite motion is specified by a 128-byte table in VRAM in >0780->07FF. Like the Sprite Attribute Table, it holds 4 bytes per sprite, and we need to handle every sprite in order from 0 to our highest-index moving sprite. Unlike the Sprite Attribute Table, instead of having a special terminator value, the number of moving sprites is held in the CPU RAM at location >837A. For our four sprites, we load the tables into place and activate them with these two lines of code:\nMOVE 16,G@SPRMOVE,V@>0780\nST >04,@>837A\nThat leaves the table itself. Matching the CALL SPRITE calls from our BASIC code last year is pretty simple:\nSPRMOVE BYTE >D8,>14,>00,>00,>D8,>14,>00,>00\nBYTE >0A,>EC,>00,>00,>0A,>EC,>00,>00\nNow our umbrellas are flying around properly, and the handles are in the right place:\nWe’ve reached parity. (Exceeded it, really, since now the code is actually doing what we want, and it didn’t the first time.) Now it’s time to push it further, into parts of the firmware that BASIC didn’t touch.\nSprite Collision Detection\nCalling this “sprite collision detection” is a bit of a misnomer. The system does have a sprite collision-detection mechanism—it’s part of the VDP status word—but that only tells us if any sprite is hitting any other sprite. The Graphics Programming Language has a COINC instruction that does far more general collision checking. Instead of relying on pixel-level collisions or trying to sort out which part of a background tile “matters” for the purposes of a hit, it instead takes a generalized collision mask and allows you to check if two objects of those shape overlap if they’re at particular locations.\nUnfortunately, it doesn’t do this via some kind of bitmask approach like we saw for the Amiga’s Blitter. Instead the collision information for an X×Y sprite with itself ends up encoded as a (2X+1)×(2Y+1) bit vector, reporting hit or miss for every possible overlap and also all cases where the sprites themselves are merely adjacent (so we may elect to consider a “hit” to include merely touching as well as full overlapping). This representation isn’t always obvious given the shape, so I wrote a small Python script to take our 16×15 sprite and generate the 33×31 collision table. Here’s our umbrella shape, combining the two sprite patterns into a single collision mask:\n......XXXX......\n....XXXXXXXX....\n...XXXXXXXXXX...\n..XXXXXXXXXXX...\n.XXXXXXXXXXX....\n.XXXXXXXXXX.....\nXXXXXXXXXX......\nXXXXXXXXX.......\nXXXXXXXXX.......\nXXXXXXX.XX......\n.XXXXX...XX.....\n.XXXX.....XX....\n..XX.......XX...\n............X...\n..........XXX...\nAnd here’s the resulting collision map, assuming that “touching” counts:\n..........XXXXXX.................\n........XXXXXXXXXX...............\n.......XXXXXXXXXXXXXXXXX.........\n......XXXXXXXXXXXXXXXXXXXX.......\n.....XXXXXXXXXXXXXXXXXXXXXX......\n....XXXXXXXXXXXXXXXXXXXXXXXX.....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n...XXXXXXXXXXXXXXXXXXXXXXXXXX....\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n.....XXXXXXXXXXXXXXXXXXXXXXX.....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n...XXXXXXXXXXXXXXXXXXXXXXXXXXX...\n....XXXXXXXXXXXXXXXXXXXXXXXXXX...\n....XXXXXXXXXXXXXXXXXXXXXXXXX....\n.....XXXXXXXXXXXXXXXXXXXXXXXX....\n......XXXXXXXXXXXXXXXXXXXXXX.....\n.......XXXXXXXXXXXXXXXXXXXX......\n.........XXXXXXXXXXXXXXXXX.......\n...............XXXXXXXXXX........\n.................XXXXXX..........\nThe basic idea here is that we have two objects (object 1 and object 2), and the upper-left point in this map represents the case where object 2’s upper-left pixel is one pixel down and to the right of object 1’s lower-right pixel. Each other point on the map represents the case where object 1 is displaced that distance to the right or down, with the lower-right pixel representing the case where object 1’s upper-left pixel is just down and to the right of object 2’s lower-right pixel.\nThis 33×31 collision map then gets copied into 128 bytes like a bitmap, reading 8 bits at a time off the chart, left to right, top to bottom. A slightly wacky thing for us here is that these bits are not aligned; our 33-entry row takes up 4 bytes and then one bit, which ends up in the most significant bit of the next byte with the next row starting immediately afterwards within the same byte. The only padding bits are at the end of the whole map, and it is the least significant bits that are padded.\nThis table then appended to a four-byte header with size information. The bytes are, in order:\nThe sum of the heights of the two objects.\nThe sum of the widths of the two objects.\nThe height of object 1.\nThe width of object 1.\nThis was a bit messy to explain, but for further explanation of this you can look at my encoding program or the explanation at the TI-99/4A Tech Pages, which includes some nifty charts working out a full (smaller) example.\nThere is one issue here that looks like a problem but isn’t; we’re magnifying our sprites here, so our actual object size is not 16×15, but rather 32×30. That would end up being more like half a kilobyte, but the language runtime has our back here; we may specify a “granularity” for our collision table, with 0 being pixel-perfect specification, 1 being expressed in terms of 2×2 squares, 2 using 4×4, and so on. Magnified sprites can use unmagnified collision tables with granularity 1, but it does seem like they imagined this being used for coarser matrices for speed and size savings.\nAdding Collision to Our Animation\nIt’s not enough simply to detect collision, of course; we need to do something about it, too. I didn’t want to get too fancy here so I decided to make the umbrellas reverse their horizontal velocity if they hit. This is nothing at all like a real physics simulation, so most likely this motion change won’t actually make them stop colliding. I don’t want them to end up “wedged”, so I’ll want to set a timer so that collisions aren’t possible for a few dozen frames after a hit.\nThese collision checks need to be done while we’re also managing scanning the keyboard and monitoring the music playback, but they’re also intricate enough that I want to break them out into their own function. This is much easier to do in the GROM than in native code; one reason our scratchpad space was so desperately scarce in our native code is that the GPL interpreter reserves 25% of it for its own stacks. This means that its CALL and RTN instructions will let us design our programs in a far more traditional way as long as we don’t recurse too deep. I put a CALL FRAME instruction just before the keyboard scan in that main loop, and then put the rest in that function.\n(In my first visit to the system, I ended up designing a rather Byzantine ABI that kept most program state in VRAM and used most of what little scratchpad RAM remained to us as a local cache of it. I’m overdue to revisit that design; I’ll probably take a fresh look at it once I wrap this series up.)\nThe first thing the FRAME function has to do is actually restrict itself to running once per frame; we’d just been spinning as fast as we could, before, because we didn’t have anything timing-related that we had to care about. The firmware just did everything for us. That’s no longer true thanks to the collision timer, so our first task is to spin on the frame counter in >8379 until it changes. That will be our proof that the interrupt service routine ran:\nFRAME\nST @>8379,@>8302\nFWAIT\nCEQ @>8379,@>8302\nBS FWAIT\nNow we need to check our collision timer, which I’ve put in >8303. If it’s zero, we move on with our real check, but otherwise we just decrement that counter and return immediately.\nCZ @>8303\nBS FCHECK\nDEC @>8303\nRTN\nFCHECK\nThe check itself is kind of wacky. We have to pass the two locations we’re checking as arguments, which is normal enough. Each argument is a 16-bit value that holds a byte for the Y coordinate of that object and then a byte for the X coordinate. This matches neatly to the values stored in the Sprite Attribute Table, so we can simply use the entries for our two main umbrella sprites here.\nThe strangeness comes for the rest of the instruction; after writing a COINC instruction, we need to place a byte value (the granularity) and a word value (the address of the collision table) directly into the instruction stream as raw data. This is a general feature of GROM code; there’s a FETCH instruction that pulls bytes out of the caller’s instruction stream and advances the return address appropriately. It’s a little weird, though, because this is all ROM code and these values can only be hardcoded constants. They’re not really usable as part of a loop, or based on program state, or anything at all like that.\nFCHECK\nCOINC V@>0300,V@>0308\nBYTE >01\nDATA COLLIDE\nThe COINC instruction sets the condition bit if there’s a collision. We branch right to the return statement if there isn’t one, and otherwise, we set our collision timer to 128 frames and then negate all our sprites’ X velocities in the automatic motion table.\nWe’re in danger here, though; each umbrella is made of two sprites, and if we have a fresh interrupt mid-edit the handle and canopy could change their directions at different times and start drifting apart. That’s what mangled the umbrellas in our original BASIC program, so we’d better make sure we don’t recreate the same disaster.\nThe solution is to zero out the “moving sprite” count at >837A before our edits and restore it to 4 afterwards. That way, if we get interrupted partway through the frame the sprites will just stop instead of diverge.\nBR FDONE\nFSTRIKE ST @>80,@>8303\nCLR @>837A\nNEG V@>0781\nNEG V@>0785\nNEG V@>0789\nNEG V@>078D\nST >04,@>837A\nFDONE\nRTN\nMoving Onward\nThis was interesting; we got quite a bit more use out of the Graphics Programming Language this week, and the capabilities the firmware offers here are usefully general and sometimes (for the sprite coincidence checks) only available via the Graphics Programming Language. Pure machine-code programs would need to do quite a bit of work to match COINC, but it’s not unreasonable that they might want to use the sound list and sprite motion features on their own sometimes too. There are plenty of extra caveats, but none of them are dealbreakers.\nI did struggle a bit against the GROM code itself, though. There are some very handy affordances here, but there are also weird or wonky limitations, and I don’t think I’d generally be thrilled with the idea of writing something purely in GROM.\nFortunately, we don’t have to; there are robust facilities for making hybrid ROM/GROM cartridges, and next week I’ll look at that and where we’d want to do them.",
      "publish_datetime": "2026-01-17T20:30:00+00:00",
      "scraping_timestamp": "2026-01-23T10:28:42.700909Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://rselbach.com/your-sub-is-now-my-weekend-project",
      "author": "Roberto Selbach",
      "title": "Your App Subscription Is Now My Weekend Project · Roberto Selbach",
      "source": "rselbach.com",
      "content": "I pay for a lot of small apps. One of them was Wispr Flow for dictation. That’s $14 CAD/month that I was paying until I had a few lazy days visiting my mother. And then on the afternoon of New Year’s Day, I vibecoded Jabber.Now, don’t get me wrong, Jabber is not “production quality.” I would never sell it as a product or even recommend it to other people, but it does what I needed from Wispr Flow, and it does exactly the way I want it to. For free.At work, I’m often asked to make small videos showing some support agent how something works, or sharing some knowledge with new team members, or just a regular demo of something. In the past, I used to use Loom, which costs $15/month. So after creating Jabber, I got excited and vibecoded Reel.Reel does exactly what I wanted Loom to do: I can record my camera, I can move it around, and I get to trim the video after it’s done (I don’t remember being able to do that with Loom).Then just yesterday, a friend of mine was telling me how he got tired of paying for Typora and decided to vibecode his own Markdown editor. And that gave me the idea of creating an editor for my blog.That’s Hugora! Yes, horrible name, but who cares? It’s just for me. I get to edit my Hugo blog just the way I like. It even shows my site theme.You see the pattern here?All of these $10/month apps are suddenly a weekend project for me. I’m an engineer, but I have never written a single macOS application. I’ve never even read Swift code in my life, and yet, I now can get an app up and running in a couple of hours. This is crazy.Last year, a Medium post predicted:Most standalone apps will be “features, not products” in the long run — easy to copy and bundle into larger offerings.And I think we’re there. I don’t know what that means for the future of our industry, but it does seem like a big shift.I’m still skeptical of vibecoding in general. As I mentioned above, I would not trust my vibecoding enough to make these into products. If something goes wrong, I don’t know how to fix it. Maybe my LLM friends can, but I don’t know. But vibecoding is 100% viable for personal stuff like this: we now have apps on demand.",
      "publish_datetime": "2026-01-18",
      "scraping_timestamp": "2026-01-23T10:29:06.029775Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI"
    },
    {
      "link": "https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/",
      "author": "Apple Inc.",
      "title": "Improving the usability of C libraries in Swift | Swift.org",
      "source": "swift.org",
      "content": "There are many interesting, useful, and fun C libraries in the software ecosystem. While one could go and rewrite these libraries in Swift, usually there is no need, because Swift provides direct interoperability with C. With a little setup, you can directly use existing C libraries from your Swift code.\nWhen you use a C library directly from Swift, it will look and feel similar to using it from C. That can be useful if you’re following sample code or a tutorial written in C, but it can also feel out of place. For example, here’s a small amount of code using a C API:\nvar instanceDescriptor = WGPUInstanceDescriptor()\nlet instance = wgpuCreateInstance(&instanceDescriptor)\nvar surfaceDescriptor = WGPUSurfaceDescriptor()\nlet surface = wgpuInstanceCreateSurface(instance, &surfaceDescriptor)\nif wgpuSurfacePresent(&surface) == WGPUStatus_Error {\n// report error\n}\nwgpuSurfaceRelease(surface)\nwgpuInstanceRelease(instance)\nThe C library here that Swift is using comes from the webgpu-headers project, which vends a C header (webgpu.h) that is used by several implementations of WebGPU. WebGPU\nis a technology that enables web developers to use the system’s GPU (Graphics Processing Unit) from the browser. For the purposes of this post, you don’t really need to know anything about WebGPU: I’m using it as an example of a typical C library, and the techniques described in this blog post apply to lots of other well-designed C libraries.\nThe Swift code above has a very “C” feel to it. It has global function calls with prefixed names like wgpuInstanceCreateSurface and global integer constants like WGPUStatus_Error. It pervasively uses unsafe pointers, some of which are managed with explicit reference counting, where the user provides calls to wpuXYZAddRef and wgpuXYZRelease functions. It works, but it doesn’t feel like Swift, and inherits various safety problems of C.\nFortunately, we can improve this situation, providing a safer and more ergonomic interface to WebGPU from Swift that feels like it belongs in Swift. More importantly, we can do so without changing the WebGPU implementation: Swift provides a suite of annotations that you can apply to C headers to improve the way in which the C APIs are expressed in Swift. These annotations describe common conventions in C that match up with Swift constructs, projecting a more Swift-friendly interface on top of the C code.\nIn this post, I’m going to use these annotations to improve how Swift interacts with the WebGPU C code. By the end, we’ll be able to take advantage of Swift features like argument labels, methods, enums, and automatic reference counting, like this:\nvar instanceDescriptor = WGPUInstanceDescriptor()\nlet instance = WGPUInstance(descriptor: &instanceDescriptor)\nvar surfaceDescriptor = WGPUSurfaceDescriptor()\nlet surface = instance.createSurface(descriptor: &surfaceDescriptor)\nif surface.present() == .error {\n// report error\n}\n// Swift automatically deallocates the instance and surface when we're done\nThese same annotations can be used for any C library to provide a safer, more ergonomic development experience in Swift without changing the C library at all.\nNote: Some of what is covered in this post requires bug fixes that first became available in Swift 6.2.3.\nA module map is a way of layering a Swift-friendly modular structure on top of C headers. You can create a module map for the WebGPU header by writing the following to a file module.modulemap:\nmodule WebGPU {\nheader \"webgpu.h\"\nexport *\n}\nThe easiest thing to do is to put module.modulemap alongside the header itself. For my experiment here, I put it in the root directory of my webgpu-headers checkout. If you’re in a Swift package, put it into its own target with this layout:\n├── Package.swift\n└── Sources\n└── WebGPU\n├── include\n│\n├── webgpu.h\n│\n└── module.modulemap\n└── WebGPU.c (empty file)\nIf you reference this WebGPU target from elsewhere in the package, you can import WebGPU to get access to the C APIs.\nThere are a few ways to see what the Swift interface for a C library looks like.\nThe swift-synthesize-interface tool in Swift 6.2+ prints the Swift interface to the terminal.\nXcode’s “Swift 5 interface” counterpart to the webgpu.h header will show how the header has been mapped into Swift.\nLet’s do it from the command line, using swift-synthesize-interface. From the directory containing webgpu.h and module.modulemap, run:\nxcrun swift-synthesize-interface -I . -module-name WebGPU -target arm64-apple-macos15 -sdk /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX26.0.sdk\nThe leading xcrun and the -sdk argument with the path is only needed on macOS; on other platforms, make sure swift-synthesize-interface is in your path. The -target operation is the triple provided if you run swiftc -print-target-info. It looks like this:\n{\n\"compilerVersion\": \"Apple Swift version 6.2 (swiftlang-6.2.2.15.4 clang-1700.3.15.2)\",\n\"target\": {\n\"triple\": \"arm64-apple-macosx15.0\",\n\"unversionedTriple\": \"arm64-apple-macosx\",\n\"moduleTriple\": \"arm64-apple-macos\",\n\"compatibilityLibraries\": [ ],\n\"librariesRequireRPath\": false\n},\n\"paths\": { ... }\n}\nThe output of swift-synthesize-interface is the Swift API for the WebGPU module, directly translated from C. For example, this code from the WebGPU header:\ntypedef enum WGPUAdapterType {\nWGPUAdapterType_DiscreteGPU = 0x00000001,\nWGPUAdapterType_IntegratedGPU = 0x00000002,\nWGPUAdapterType_CPU = 0x00000003,\nWGPUAdapterType_Unknown = 0x00000004,\nWGPUAdapterType_Force32 = 0x7FFFFFFF\n} WGPUAdapterType WGPU_ENUM_ATTRIBUTE;\nis translated into:\npublic struct WGPUAdapterType : Hashable, Equatable, RawRepresentable {\npublic init(_ rawValue: UInt32)\npublic init(rawValue: UInt32)\npublic var rawValue: UInt32\n}\npublic var WGPUAdapterType_DiscreteGPU: WGPUAdapterType { get }\npublic var WGPUAdapterType_IntegratedGPU: WGPUAdapterType { get }\npublic var WGPUAdapterType_CPU: WGPUAdapterType { get }\npublic var WGPUAdapterType_Unknown: WGPUAdapterType { get }\npublic var WGPUAdapterType_Force32: WGPUAdapterType { get }\nand there are lots of global functions like this:\npublic func wgpuComputePipelineGetBindGroupLayout(_ computePipeline: WGPUComputePipeline!, _ groupIndex: UInt32) -> WGPUBindGroupLayout!\npublic func wgpuComputePipelineSetLabel(_ computePipeline: WGPUComputePipeline!, _ label: WGPUStringView)\npublic func wgpuComputePipelineAddRef(_ computePipeline: WGPUComputePipeline!)\npublic func wgpuComputePipelineRelease(_ computePipeline: WGPUComputePipeline!)\nIt’s a starting point! You can absolutely write Swift programs using these WebGPU APIs, and they’ll feel a lot like writing them in C. Let’s see what we can do to make it better.\nC enums can be used for several things. Sometimes they really represent a choice among a number of alternatives. Sometimes they represent flags in a set of options, from which you can choose several. Sometimes they’re just a convenient way to create a bunch of named constants. Swift conservatively imports enum types as wrappers over the underlying C type used to store values of the enum (e.g., WGPUAdapterType wraps a UInt32) and makes the enumerators into global constants. It covers all of the possible use cases, but it isn’t nice.\nThe WGPUAdapterType enum really is a choice among one of several options, which would be best represented as an enum in Swift. If we were willing to modify the header, we could apply the enum_extensibility attribute to the enum, like this:\ntypedef enum __attribute__((enum_extensibility(closed))) WGPUAdapterType {\nWGPUAdapterType_DiscreteGPU = 0x00000001,\nWGPUAdapterType_IntegratedGPU = 0x00000002,\nWGPUAdapterType_CPU = 0x00000003,\nWGPUAdapterType_Unknown = 0x00000004,\nWGPUAdapterType_Force32 = 0x7FFFFFFF\n} WGPUAdapterType WGPU_ENUM_ATTRIBUTE;\nThis works, and results in a much nicer Swift API:\n@frozen public enum WGPUAdapterType : UInt32, @unchecked Sendable {\ncase discreteGPU = 1\ncase integratedGPU = 2\ncase CPU = 3\ncase unknown = 4\ncase force32 = 2147483647\n}\nNow, we get an enum that we can switch over, and nice short case names, e.g.,\nswitch adapterType {\ncase .discreteGPU, .integratedGPU:\nprint(\"definitely a GPU\")\ndefault:\nprint(\"not so sure\")\n}\nThat’s great, but I already broke my rule: no header modifications unless I have to!\nThe problem of needing to layer information on top of existing C headers is not a new one. As noted earlier, Swift relies on a Clang feature called API notes to let us express this same information in a separate file, so we don’t have to edit the header. In this case, we create a file called WebGPU.apinotes (the name WebGPU matches the module name from module.modulemap), which is a YAML file describing the extra information. We’ll start with one that turns WGPUAdapterType into an enum:\n---\nName: WebGPU\nTags:\n- Name: WGPUAdapterType\nEnumExtensibility: closed\nTags here is a term used in the C and C++ standard to refer to enum, struct, union, or class types. Any information about those types in the API notes file will go into that section.\nPut WebGPU.apinotes alongside the module.modulemap, and now WGPUAdapterType gets mapped into a Swift enum. For a package, the structure will look like this:\n├── Package.swift\n└── Sources\n└── WebGPU\n├── include\n│\n├── webgpu.h\n│\n├── WebGPU.apinotes\n│\n└── module.modulemap\n└── WebGPU.c (empty file)\nWe’ll be adding more to this API notes file as we keep digging through the interface.\nThe WebGPU header has a number of “object” types that are defined like this:\ntypedef struct WGPUBindGroupImpl* WGPUBindGroup WGPU_OBJECT_ATTRIBUTE;\nThis gets imported into Swift as an alias for an opaque pointer type, which is… not great:\npublic typealias WGPUBindGroup = OpaquePointer\nWebGPU object types are reference counted, and each object type has corresponding AddRef and Release functions to increment and decrement the reference count, like this:\nWGPU_EXPORT void wgpuBindGroupAddRef(WGPUBindGroup bindGroup) WGPU_FUNCTION_ATTRIBUTE;\nWGPU_EXPORT void wgpuBindGroupRelease(WGPUBindGroup bindGroup) WGPU_FUNCTION_ATTRIBUTE;\nOf course, you can use these functions in Swift exactly how you do in C, making sure to balance out calls to AddRef and Release, but then it would be every bit as unsafe as C.\nWe can do better with SWIFT_SHARED_REFERENCE. It’s a macro (defined in the <swift/bridging> header) that can turn a reference-counted C type like the above into an automatically reference-counted class in Swift. Here’s how we would use it in the header:\ntypedef struct SWIFT_SHARED_REFERENCE(wgpuBindGroupAddRef, wgpuBindGroupRelease) WGPUBindGroupImpl* WGPUBindGroup WGPU_OBJECT_ATTRIBUTE;\nNow, WGPUBindGroup gets imported like this:\npublic class WGPUBindGroupImpl { }\npublic typealias WGPUBindGroup = WGPUBindGroupImpl\nThe extra typealias is a little unexpected, but overall this is a huge improvement: Swift is treating WGPUBindGroup as a class, meaning that it automatically manages retains and releases for you! This is both an ergonomic win (less code to write) and a safety win, because it’s eliminated the possibility of mismanaging these instances.\nThere’s one more thing: when dealing with reference-counting APIs, you need to know whether a particular function that returns an object is expecting you to call “release” when you’re done. In the WebGPU header, this information is embedded in a comment:\n/**\n* @returns\n* This value is @ref ReturnedWithOwnership.\n*/\nWGPU_EXPORT WGPUBindGroup wgpuDeviceCreateBindGroup(WGPUDevice device, WGPUBindGroupDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;\n“ReturnedWithOwnership” here means that the result of the call has already been retained one extra time, and the caller is responsible for calling “release” when they are done with it. The <swift/bridging> header has a SWIFT_RETURNS_RETAINED macro that expresses this notion. One can use it like this:\nWGPU_EXPORT WGPUBindGroup wgpuDeviceCreateBindGroup(WGPUDevice device, WGPUBindGroupDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE SWIFT_RETURNS_RETAINED;\nNow, Swift will balance out the retain that wgpuDeviceCreateBindGroup has promised to do by performing the extra release once you’re done using the object. Once these annotations are done, we’re all set with a more ergonomic and memory-safe API for this C library. There’s no need to ever call wgpuBindGroupRelease or wgpuBindGroupAddRef yourself.\nWe’ve hacked up our header again, so let’s undo that and move all of this out to API notes. To turn a type into a foreign reference type, we augment the Tags section of our API notes with the same information, but in YAML form:\n- Name: WGPUBindGroupImpl\nSwiftImportAs: reference\nSwiftReleaseOp: wgpuBindGroupRelease\nSwiftRetainOp: wgpuBindGroupAddRef\nThat makes WGPUBindGroupImpl import as a class type, with the given retain and release functions. We can express the “returns retained” behavior of the wgpuDeviceCreateBindGroup function like this:\nFunctions:\n- Name: wgpuDeviceCreateBindGroup\nSwiftReturnOwnership: retained\nThat’s enums and classes, so now let’s tackle… functions.\nA typical function from webgpu.h, like this:\nWGPU_EXPORT void wgpuQueueWriteBuffer(\nWGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset,\nvoid const * data, size_t size\n) WGPU_FUNCTION_ATTRIBUTE;\nwill come into Swift like this:\npublic func wgpuQueueWriteBuffer(_ queue: WGPUQueue!, _ buffer: WGPUBuffer!, _ bufferOffset: UInt64, _ data: UnsafeRawPointer!, _ size: Int)\nNote that _ on each parameter, which means that we won’t use argument labels for anything when we call it:\nwgpuQueueWriteBuffer(myQueue, buffer, position, dataToWrite, bytesToWrite)\nThat matches C, but it isn’t as clear as it could be in Swift. Let’s clean this up by providing a better name in Swift that includes argument labels. We can do so using SWIFT_NAME (also in <swift/bridging>), like this:\nWGPU_EXPORT void wgpuQueueWriteBuffer(\nWGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset,\nvoid const * data, size_t size\n) WGPU_FUNCTION_ATTRIBUTE\nSWIFT_NAME(\"wgpuQueueWriteBuffer(_:buffer:bufferOffset:data:size:)\");\nWithin the parentheses, we have each of the argument labels that we want (or _ meaning “no label”), each followed by a :. This is how one describes a full function name in Swift. Once we’ve made this change to the Swift name, the C function comes into Swift with argument labels, like this:\npublic func wgpuQueueWriteBuffer(_ queue: WGPUQueue!, buffer: WGPUBuffer!, bufferOffset: UInt64, data: UnsafeRawPointer!, size: Int)\nThat makes the call site more clear and self-documenting:\nwgpuQueueWriteBuffer(myQueue, buffer: buffer, offset: position, data: dataToWrite, size: bytesToWrite)\nThere is more usable structure in this API. Note that the wgpuQueueWriteBuffer function takes, as its first argument, an instance of WGPUQueue. Most of the C functions in WebGPU.h are like this, because these are effectively functions that operate on their first argument. In a language that has methods, they would be methods. Swift has methods, so let’s make them methods!\nWGPU_EXPORT void wgpuQueueWriteBuffer(\nWGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset, void const * data, size_t size)\nWGPU_FUNCTION_ATTRIBUTE SWIFT_NAME(\"WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)\");\nThere are three things to notice about this SWIFT_NAME string:\nIt starts with WGPUQueueImpl., which tells Swift to make this function a member inside WGPUQueueImpl.\nLet’s change the function name to writeBuffer, because we no longer need the wgpuQueue prefix to distinguish it from other “write buffer” operations on other types.\nThe name of the first argument in parentheses is self, which indicates that the self argument (in Swift) should be passed as that positional argument to the C function. The other arguments are passed in-order.\nNote that this also requires WGPUQueue(Impl) to be imported as a class, as we did earlier for WGPUBindGroupImpl. Once we’ve done so, we get a much-nicer Swift API:\nextension WGPUQueueImpl {\n/**\n* Produces a @ref DeviceError both content-timeline (`size` alignment) and d\nevice-timeline\n* errors defined by the WebGPU specification.\n*/\npublic func writeBuffer(buffer: WGPUBuffer!, bufferOffset: UInt64, data: UnsafeRawPointer!, size: Int)\n}\nWe’ve hacked up the header again, but didn’t have to. In WebGPU.apinotes, you can put a SwiftName attribute on any entity. For wgpuQueueWriteBuffer, it would look like this (in the Functions section):\n- Name: wgpuQueueWriteBuffer\nSwiftName: WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)\nWebGPU.h has a number of Get functions that produce information about some aspect of a type. Here are two for the WGPUQuerySet type:\nWGPU_EXPORT uint32_t wgpuQuerySetGetCount(WGPUQuerySet querySet) WGPU_FUNCTION_ATTRIBUTE;\nWGPU_EXPORT WGPUQueryType wgpuQuerySetGetType(WGPUQuerySet querySet) WGPU_FUNCTION_ATTRIBUTE;\nWith the SWIFT_NAME tricks above, we can turn these into “get” methods on WGPUQuerySet, like this:\nextension WGPUQuerySetImpl {\npublic func getCount() -> UInt32\npublic func getType() -> WGPUQueryType\n}\nThat’s okay, but it’s not what you’d do in Swift. Let’s go one step further and turn them into read-only computed properties. To do so, use the getter: prefix on the Swift name we define. We’ll skip ahead to the YAML form that goes into API notes:\n- Name: wgpuQuerySetGetCount\nSwiftName: getter:WGPUQuerySetImpl.count(self:)\n- Name: wgpuQuerySetGetType\nSwiftName: getter:WGPUQuerySetImpl.type(self:)\nAnd now, we arrive at a nice Swift API:\nextension WGPUQuerySetImpl {\npublic var count: UInt32 { get }\npublic var type: WGPUQueryType { get }\n}\nSWIFT_NAME can also be used to import a function that returns a new instance as a Swift initializer. For example, this function creates a new WGPUInstance (which we assume is getting imported as a class like we’ve been doing above):\n/**\n* Create a WGPUInstance\n*\n* @returns\n* This value is @ref ReturnedWithOwnership.\n*/\nWGPU_EXPORT WGPUInstance wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;\nWe can turn this into a Swift initializer, which is used to create a new object, using the same SWIFT_NAME syntax but where the method name is init. Here is the YAML form that goes into API notes:\n- Name: wgpuCreateInstance\nSwiftReturnOwnership: retained\nSwiftName: WGPUInstanceImpl.init(descriptor:)\nand here is the resulting Swift initializer:\nextension WGPUInstanceImpl {\n/**\n* Create a WGPUInstance\n*\n* @returns\n* This value is @ref ReturnedWithOwnership.\n*/\npublic /*not inherited*/ init!(descriptor: UnsafePointer<WGPUInstanceDescriptor>!)\n}\nNow, one can create a new WGPUInstance with the normal object-creation syntax, e.g.,\nlet instance = WGPUInstance(descriptor: myDescriptor)\nThe WebGPU header defines its own Boolean type. I wish everyone would use C99’s _Bool and be done with it, but alas, here are the definitions for WebGPUs Boolean types:\n#define WGPU_TRUE (UINT32_C(1))\n#define WGPU_FALSE (UINT32_C(0))\ntypedef uint32_t WGPUBool;\nThis means that WGPUBool will come in to Swift as a UInt32. The two macros aren’t available in Swift at all: they’re “too complicated” to be recognized as integral constants. Even if they were available in Swift, it still wouldn’t be great because we want to use true and false for Boolean values in Swift, not WGPU_TRUE and WGPU_FALSE.\nTo make WGPUBool easier to use from Swift, we’re first going to map that typedef to its own struct that stores the underlying UInt32, giving it an identity separate from UInt32. We can do this using a SwiftWrapper API note within the Typedefs section of the file, like this:\n- Name: WGPUBool\nSwiftWrapper: struct\nNow, we get WGPUBool imported like this:\npublic struct WGPUBool : Hashable, Equatable, RawRepresentable {\npublic init(_ rawValue: UInt32)\npublic init(rawValue: UInt32)\n}\nTo be able to use true and false literals with this new WGPUBool, we can write a little bit of Swift code that makes this type conform to the ExpressibleByBooleanLiteral protocol, like this:\nextension WGPUBool: ExpressibleByBooleanLiteral {\ninit(booleanLiteral value: Bool) {\nself.init(rawValue: value ? 1 : 0)\n}\n}\nThat’s it! Better type safety (you cannot confuse a WGPUBool with any other integer value) and the convenience of Boolean literals in Swift.\nwebgpu.h describes a set of flags using a typedef of the WGPUFlags type (a 64-bit unsigned integer) along with a set of global constants for the different flag values. For example, here is the WGPUBufferUsage flag type and some of its constants:\ntypedef WGPUFlags WGPUBufferUsage;\nstatic const WGPUBufferUsage WGPUBufferUsage_MapRead = 0x0000000000000001;\nstatic const WGPUBufferUsage WGPUBufferUsage_MapWrite = 0x0000000000000002;\nstatic const WGPUBufferUsage WGPUBufferUsage_CopySrc = 0x0000000000000004;\nstatic const WGPUBufferUsage WGPUBufferUsage_Index = 0x0000000000000010;\nSimilar to what we saw with WGPUBool, WGPUBufferUsage is a typedef of a typedef of a uint64_t. There’s no type safety in this C API, and one could easily mix up these flags with, say, those of WGPUMapMode:\ntypedef WGPUFlags WGPUMapMode;\nstatic const WGPUMapMode WGPUMapMode_Read = 0x0000000000000001;\nstatic const WGPUMapMode WGPUMapMode_Write = 0x0000000000000002;\nWe can do better, by layering more structure for the Swift version of this API using the same SwiftWrapper approach from WGPUBool. This goes into the Typedefs section of API notes:\nTypedefs:\n- Name: WGPUBufferUsage\nSwiftWrapper: struct\nNow, WGPUBufferUsage comes in as its own struct:\npublic struct WGPUBufferUsage : Hashable, Equatable, RawRepresentable {\npublic init(_ rawValue: WGPUFlags)\npublic init(rawValue: WGPUFlags)\n}\nThe initializers let you create a WGPUBufferUsage from a WGPUFlags value, and there is also a rawValue property to get a WGPUFlags value out of a WGPUBufferInstance, so the raw value is always there… but the default is to be type safe. Additionally, those global constants will come in as members of WGPUBufferUsage, like this:\nextension WGPUBufferUsage {\n/**\n* The buffer can be *mapped* on the CPU side in *read* mode (using @ref WGPUMapMode_Read).\n*/\npublic static var _MapRead: WGPUBufferUsage { get }\n/**\n* The buffer can be *mapped* on the CPU side in *write* mode (using @ref WGPUMapMode_Write).\n*\n* @note This usage is **not** required to set `mappedAtCreation` to `true` in @ref WGPUBufferDescriptor.\n*/\npublic static var _MapWrite: WGPUBufferUsage { get }\n/**\n* The buffer can be used as the *source* of a GPU-side copy operation.\n*/\npublic static var _CopySrc: WGPUBufferUsage { get }\n/**\n* The buffer can be used as the *destination* of a GPU-side copy operation.\n*/\npublic static var _CopyDst: WGPUBufferUsage { get }\n}\nThis means that, if you’re passing a value of type WPUBufferUsage, you can use the shorthand “leading dot” syntax. For example:\nfunc setBufferUsage(_ usage: WGPUBufferUsage) { ... }\nsetBufferUsage(._MapRead)\nSwift has dropped the common WPUBufferUsage prefix from the constants when it made them into members. However, the resulting names aren’t great. We can rename them by providing a SwiftName in the API notes file within the Globals section:\nGlobals:\n- Name: WGPUBufferUsage_MapRead\nSwiftName: WGPUBufferUsage.mapRead\n- Name: WGPUBufferUsage_MapWrite\nSwiftName: WGPUBufferUsage.mapWrite\nWe can go one step further by making the WGPUBufferUsage type conform to Swift’s OptionSet protocol. If we revise the API notes like this:\nTypedefs:\n- Name: WGPUBufferUsage\nSwiftWrapper: struct\nSwiftConformsTo: Swift.OptionSet\nNow, we get the nice option-set syntax we expect in Swift:\nlet usageFlags: WGPUBufferUsage = [.mapRead, .mapWrite]\nThroughout webgpu.h, the WGPU_NULLABLE macro is used to indicate pointers that can be NULL. The implication is that any pointer that is not marked with WGPU_NULLABLE cannot be NULL. For example, here is the definition of wgpuCreateInstance we used above:\nWGPU_EXPORT WGPUInstance wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;\nThe WGPU_NULLABLE indicates that it’s acceptable to pass a NULL pointer in as the descriptor parameter. Clang already has nullability specifiers to express this information. We could alter the declaration in the header to express that this parameter is nullable but the result type is never NULL, like this:\nWGPU_EXPORT WGPUInstance _Nonnull wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * _Nullable descriptor) WGPU_FUNCTION_ATTRIBUTE;\nThis eliminates the implicitly-unwrapped optionals (!) from the signature of the initializer, so we end up with one that explicitly accepts a nil descriptor argument and always returns a new instance (never nil):\nextension WGPUInstanceImpl {\n/**\n* Create a WGPUInstance\n*\n* @returns\n* This value is @ref ReturnedWithOwnership.\n*/\npublic /*not inherited*/ init(descriptor: UnsafePointer<WGPUInstanceDescriptor>?)\n}\nNow, I did cheat by hacking the header. Instead, we can express this with API notes on the parameters and result type by extending the entry we already have for wgpuCreateInstance like this:\n- Name: wgpuCreateInstance\nSwiftReturnOwnership: retained\nSwiftName: WGPUInstanceImpl.init(descriptor:)\nParameters:\n- Position: 0\nNullability: O\nResultType: \"WGPUInstance _Nonnull\"\nTo specific nullability of pointer parameters, one can identify them by position (where 0 is the first parameter to the function) and then specify whether the parameter should come into Swift as optional (O, corresponds to _Nullable), non-optional (N, corresponds to _Nonnull) or by left unspecified as an implicitly-unwrapped optional (U, corresponds to _Null_unspecified). For the result type, it’s a little different: we specified the result type along with the nullability specifier, i.e., WGPUInstance _Nonnull. The end result of these annotations is the same as the modified header, so we can layer nullability information on top of the header.\nwebgpu.h is about 6,400 lines long, and is regenerated from a database of the API as needed. Each of the WebGPU implementations seems to augment or tweak the header a bit. So, rather than grind through and manually do annotations, I wrote a little Swift script to “parse” webgpu.h, identify its patterns, and generate WebGPU.apinotes for most of what is discussed in this post. The entirety of the script is here. It reads webgpu.h from standard input and prints WebGPU.apinotes to standard output.\nBecause webgpu.h is generated, it has a very regular structure that we can pick up on via regular expressions. For example:\n// Enum definitions, marked by WGPU_ENUM_ATTRIBUTE.\nlet enumMatcher = /} (?<name>\\w+?) WGPU_ENUM_ATTRIBUTE/\n// Object definitions, marked by WGPU_OBJECT_ATTRIBUTE.\nlet objectMatcher = /typedef struct (?<implName>\\w+?)\\* (?<name>\\w+?) WGPU_OBJECT_ATTRIBUTE;/\n// Function declarations, marked by WGPU_FUNCTION_ATTRIBUTE\nlet functionMatcher = /WGPU_EXPORT (?<nullableResult>WGPU_NULLABLE ?)?(?<resultType>\\w+?) (?<name>\\w+?)\\((?<parameters>.*\\)?) WGPU_FUNCTION_ATTRIBUTE;/\nlet parameterMatcher = /(?<type>[^),]+?) (?<name>\\w+?)[),]/\nThat’s enough to identify all of the enum types (so we can emit the EnumExtensibility: closed API notes), object types (to turn them into shared references), and functions (which get nicer names and such). The script is just a big readLine loop that applies the regexes to capture all of the various types and functions, then does some quick classification before printing out the API notes. The resulting API notes are in WebGPU.apinotes, and the generated Swift interface after these API notes are applied is here. You can run it with, e.g.,\nswift -enable-bare-slash-regex webgpu_apinotes.swift < webgpu.h\nThis script full of regular expressions is, admittedly, a bit of a hack. A better approach for an arbitrary C header would be to use libclang to properly parse the headers. For WebGPU specifically, the webgpu-headers project contains a database from which the header is generated, and one could also generate API notes directly from that header. Regardless of how you get there, many C libraries have well-structured headers with conventions that can be leveraged to create safer, more ergonomic projections in Swift.\nThe techniques described in this post can be applied to just about any C library. To do so, I recommend setting up a small package like the one described here for WebGPU, so you can iterate quickly on example code to get a feel for how the Swift projection of the C API will work. The annotations might not get you all the way to the best Swift API, but they are a lightweight way to get most of the way there. Feel free to also extend the C types to convenience APIs that make sense in Swift, like I did above to make WGPUBool conform to ExpressibleByBooleanLiteral.\nA little bit of annotation work on your favorite C library can make for a safer, more ergonomic, more Swifty experience of working with that library.\nThe regular structure of webgpu.h helped considerably when trying to expose the API nicely in Swift. That said, there are a few ways in which webgpu.h could be improved to require less annotation for this purpose:\nWGPU_ENUM_ATTRIBUTE would be slightly nicer if placed on the enum itself, rather than on the typedef. If it were there, we could use\n#define WGPU_ENUM_ATTRIBUTE __attribute__((enum_extensibility(closed)))\nand not have to generate any API notes to bring these types in as proper enums in Swift.\nWGPU_OBJECT_ATTRIBUTE could provide the names of the retain and release operations and be placed on the struct itself. If it were there, we could use\n#define WGPU_OBJECT_ATTRIBUTE(RetainFn,ReleaseFn) SWIFT_SHARED_REFERENCE(RetainFn,ReleaseFn)\nand not have to generate any API notes to bring these types in as classes in Swift.\nWGPU_NULLABLE could be placed on the pointer itself (i.e., after the *) rather than at the beginning of the type, to match the position of Clang’s nullability attributes. If it were placed there, then\n#define WGPU_NULLABLE _Nullable\nwould work with Clangs’ longstanding nullable-types support. Swift would then import such pointers as optional types (with ?). Moreover, if some macros WGPU_ASSUME_NONNULL_BEGIN and WGPU_ASSUME_NONNULL_END were placed at the beginning and end of the header, they could be mapped to Clang’s pragmas to assume that any pointer not marked “nullable” is always non-null:\n#define WGPU_ASSUME_NONNULL_BEGIN #pragma clang assume_nonnull begin\n#define WGPU_ASSUME_NONNULL_END #pragma clang assume_nonnull end\nThis would eliminate all of the implicitly unwrapped optionals (marked ! in the Swift interface), making it easier to use safely.\nContinue Reading",
      "publish_datetime": "2026-01-22T08:00:00-04:00",
      "scraping_timestamp": "2026-01-23T10:29:08.569500Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://www.thequantumcat.space/p/project-mercury-and-the-sofar-bomb",
      "author": "Alastair Williams",
      "title": "Project Mercury and the SOFAR Bomb - by Alastair Williams",
      "source": "thequantumcat.space",
      "content": "The flight of Alan Shepard. Credit: NASAProject Mercury was supposed to be fast, and it was supposed to be simple. After all, Sputnik had made it starkly clear the Soviets were ahead, and America feared falling even further behind. The newly created NASA quickly struck back with Explorer 1, launched less than four months after Sputnik, but by then the Soviets had put a dog in space and everyone knew a human would be next. To have any hope of getting there first, Mercury had to be as fast and as simple as possible.Except, of course, that putting a man into space for the first time was anything but simple. First you needed a rocket, and the ones America had in 1959 still had a nasty habit of blowing up. Then you needed a capsule – and this was supposed to be dumb, just something a person could sit in for a few hours and sail safely through the vacuum of space. And then you needed to bring him back to Earth.For this last bit, America chose to use the oceans. To return to Earth, an orbiting capsule first needs to fire its thrusters, a burn that slows the capsule and drains it of the energy needed to remain in orbit. After that it will fall, and if you time the burn correctly, it will descend towards a chosen spot on the ground. Put that spot in the ocean, and things are simpler: water is more forgiving than solid rock, at least when it comes to things falling at high speed.But afterwards things can be complicated. A capsule must float; it must be able to survive waves and whatever else the weather throws at it; and it must be able to be found. And none of that is especially easy. In Project Mercury one of the capsules sank, a disaster which almost took an astronaut with it, and two others came down hundreds of miles off course, each invoking a search over vast stretches of the ocean.This was a time before the navigation systems we rely on today. There was no constellation of GPS satellites, and no network of relay satellites to transmit distress calls. The deep ocean was vast, mysterious, and largely out of reach. If a capsule came down five hundred miles short of its target, as one of the first test flights did, then finding it was akin to looking for the proverbial needle in a haystack.What was needed was a way to locate capsules amidst the waves. At first, uncertain of what would work, NASA threw almost everything they could at the problem. An armada of ships and aircraft were scattered across the ocean before every flight, and the capsules themselves were fitted with an array of aids to assist in finding them. They scattered chaff as the parachutes opened, hoping it would appear on radar, triggered radio beacons on splashdown, and dropped bombs that would explode deep under the waves.From Patent US3093346A: Space CapsuleThis last idea, that of the SOFAR bomb, relies on an oddity of the deep oceans. There, roughly a kilometre below the surface, lies a sound channel. If a bomb can be detonated in this channel, the sound of it can be picked up thousands of miles away, and that offers a way to hone in on the position of whatever has dropped it.The reasons why this works are subtle. First, of course, is the simple fact that sound travels more easily through water than through air. That makes logical sense: sound moves through the vibrations of particles, and those particles are more closely packed in water than in air. This also means sound travels faster through water than it does through air: the speed of sound in fresh water at room temperature, for example, is four times that in air.The speed of sound in water, however, also depends on both pressure and temperature. In colder water sound travels more slowly. The deeper under the surface of the ocean you go the colder things normally get, and so if all else is equal, sound waves moving deep under the waves should travel more slowly than those near the surface.But, as always, things are not equal. The deeper into the ocean you go, the more water you have pushing down on you, and the higher the pressure you experience. As pressure increases, sound travels faster. Initially, as you descend under the waves, the change in temperature dominates and sound slows down. But at some point, approximately a thousand meters deep, pressure begins to assert itself, and sound speeds up again.This creates a minimum, a distinct depth at which sound waves reach their slowest speed. If a sound originates here, it will stay here. If it rises, it will be reflected down. If it falls, it will be pushed back up. The only way it can move is within the layer itself, horizontally under the waves.This implies two things. First, that sound waves cannot enter this layer. Only what is made within it can be heard by someone else listening inside it. And second, that sound waves cannot leave: any noise originating within this layer can thus be heard at enormous distances. Whale song is a good example of this: it can travel across an ocean, and allow a creature off the coast of Ireland to communicate with another swimming near Virginia.The SOFAR channel, illustrated by NOAA.We have developed two easy ways to create sounds underwater. One is to drop a hollow metallic sphere. At the right depth and pressure it will be crushed and the sound of the implosion will echo through the underwater channel. The other – the method used on Mercury – is to drop a bomb, use a pressure sensor to trigger a detonation, and then wait for the sound wave to travel through the water.Each of the early Mercury capsules carried two such SOFAR – SOund Fixing And Ranging – bombs on board. One was stowed with the parachutes, and would be thrown out as they deployed. It would fall in the ocean and give any waiting ships a bearing upon which to look for the spacecraft.The other was stowed within the capsule itself, and would only detonate if the whole thing sank. This would send a second location pulse to ships. But the explosion was also intended to destroy vital components inside the capsule, and thus prevent the Soviets from ever retrieving American designs.Both bombs had a range of about three thousand miles. Hydrophones installed as part of Cold War efforts to track submarines could listen for them, as could vessels equipped with listening equipment and special buoys dropped by aircraft. All that gave the Navy plenty of points with which to triangulate the signal.A Mercury capsule afloat while the Navy practices recovery operations. Credit: NASA.On September 9, 1959, all this was put into practice. Big Joe I, an uncrewed test capsule, was launched from Cape Canaveral. Things did not go to plan. A few minutes into the flight, the booster failed to jettison. That made everything heavier than it should have been, and to compensate the engines burned longer. Yet it wasn’t enough. When they ran out of fuel, the spacecraft was still coming in too steep. It landed five hundred miles short of its target.But everything worked: the chaff deployed, an automatic radio beacon was picked up by a nearby aircraft, and the bomb exploded deep under the water. It took less than two hours to confirm the capsule’s location via the SOFAR channel, and the recovery operation itself was done within eight.Overall, however, SOFAR proved less useful than hoped. The Navy often took hours to process the data from its microphones, and even then the results would reflect the point at which the parachutes had opened rather than the actual location of the capsule. When Mercury-Atlas 7 splashed down two hundred miles off-target, it was the radio beacons that first alerted search crews, not the bomb.These difficulties, along with the effectiveness of radio beacons and water dyes, spelled the eventual end of the SOFAR bomb. After the fourth manned flight, NASA decided to drop both the bombs and the chaff. They were not used again on Mercury, nor on Gemini or Apollo. The Shuttle, of course, had no need for them, and by the time American capsules started splashing down in the oceans again, global positioning satellites had rendered all other techniques unnecessary.Yet the bomb is a testament to a time when spaceflight was new and engineers were unafraid to think creatively. Project Mercury had to be fast, and there was no time to run years of analysis to prove what would work or what wouldn’t. Engineers were pragmatic, they tried new things, and if they proved unnecessary they weren’t afraid to move on.True, not everything went right. There were near misses, close calls, and moments that could easily have ended in tragedy. But Mercury did succeed in putting a man into space, even if the Soviets were first by a month. And for a brief time, the sounds of those bold missions echoed through the depths of the oceans, accompanied by the songs of the whales and the mechanical whirr of the submarines.The Biggest Solar Flare of 2025The Quantum Cat is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.The Hasselblad Cameras of Project MercuryUnless otherwise specified, all images in this article are thanks to NASA and especially to the March to the Moon archive of Mercury, Gemini, and Apollo photography.The Quantum Cat is a reader-suppor…",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:29:10.380273Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate",
      "author": "Alexia KerkhofAssistant ProfessorAll blog articles",
      "title": "Why medieval city-builder video games are historically inaccurate - Leiden Medievalists Blog",
      "source": "leidenmedievalistsblog.nl",
      "content": "This blog post explores the historical accuracy of medieval city-builder video games.\nIntroductionSince many of us are working from home in these trying times, it seems safe to assume that more people than ever are indulging in playing the occasional computer game. A city builder is a specific kind of computer game in which you design a city, extract resources, set up production chains and ensure that your settlement grows. City builders are very similar to strategy games as they reward patience and strategy. In this article, I will take a look at one sub-genre of the city builder, the medieval city builder, and explain how this gaming genre relates to our knowledge of medieval settlement planning.Historical city buildersThe city builder has its origins far back in the 1990s in the combination of the strategy genre and the management genre, leading to games such as Sim City (1989), Caesar (1992) and Age of Empires (1997).\nScreenshot: Caesar (1992)\nIt did not take long before medieval-themed city builders popped up. We may think of Settlers (1993) and Knights and Merchants (1998). In addition, the Anno games (1998-2019), although initially set in the 1600s basically had a medieval theme.\nScreenshot: Knights and Merchants (1998)\nThese games often start with plopping down a village center on a promising location near abundant resources. You then continue to gather these resources which grant you building materials for building new homes and facilities for your settlement.\nScreenshot: Settlers (1993)\nSetting up specialized production chains might involve growing grain, milling the grain for flour and turning the flour into bread which feeds your villages. Similarly, another production chain might involve rearing sheep for their wool, turning the wool into cloth and turning the cloth into clothing. When done correctly, the reward of correct investments and planning is that you see your settlement grow.\nScreenshot: Anno 1404 (2009)\nThis often leads to settlements growing organically from a couple of houses around a community center to a larger settlement with hundreds of people. However logical such an organic growth of a settlement might seem, it is not historically accurate.\nScreenshot: Foundation (2019)\nMedieval village life Any gameplay loop that tells a story of linear settlement growth is incongruent with how a medieval economy worked (see Foussier 2004). Medieval villagers were often living on the edge of subsistence. Agricultural surpluses were skimmed by the church and the feudal lords. Bad harvests, banditry, warfare and disease might decimate a village community at any time. For this very reason, the demography of many European villages remained relatively stable between the twelfth and the eighteenth century. It may therefore be clear that the gameplay loop of city builders pivots around the concept of doing the historically exceptional (i.e. growing a settlement to a town) and thereby strays far from what actually happened in the lives of our medieval forebears. A notable exception to this genre trope is the game Banished (2014) in which high mortality rates and bad weather do seriously stifle any kind of linear growth. In this city builder you are constantly fighting the odds and settlement growth is not guaranteed. However, also in Banished it is your goal to overcome the stagnation and lead your settlement to expansion.\nScreenshot: Banished (2014)\nA thing that is rarely touched upon in medieval city builders is how complex village life actually was. This can be exemplified by how the community related to its overlords. Land ownership here is key. Land in the community might be owned by a lord, a local liegeman, a monastery or even directly by the duke or count. Taxes, rents and tithes were the organisational structures in which the landowner was tied to the farmers who worked the fields. Often the payment of taxes and tithes was linked to feast days and the visit of the tax collector represented a big event in the agricultural year. An interesting side note is that some obligations which the commoners had to the lord and the church (such as seigneurial duties like working a mill) might drain the community from the needed manpower for tilling the land. Furthermore, a rural community that was its own seigneury had access to a law court with sheriff, aldermen and a local militia (Middle Dutch schutterie) to fight off bandits with. Harsh capital punishments were set in place to deter anyone from raiding the farms and hamlets and the village gallows were often the first thing one saw when approaching a medieval settlement. Planning a medieval settlementBut something that is much more fundamental to the theme of a settlement building game, is how medieval settlements were actually planned and grew. Landscape historians and archaeologists have acquired a lot of insight into how this worked.Let's start with the realization that medieval settlements in their first stages of development were planned and laid out according to a specific design. In my own research into the settlement history of West-Brabant (southern Netherlands, from 1000 to 1300 CE) I have encountered the following types.Here is a sketch of a Brabantine circular manor (Middle Dutch vroonhoeve). This is a reinforced circular homestead with moat, often next to a bend in the river, containing several farms and a fan-like plot pattern radiating out from it. Such manors were often called BORCH.\nSketch of a Brabantine street settlement\nHere is a sketch of a Brabantine street settlement, often built with exploitation of nearby fenland in mind. It consists of a line of farms with associated evenly sized rectangular plots built in a line perpendicular to a raised road.\nSketch of a more complex exploitation village\nHere is a more complex exploitation village which is set up with a moated enclosed church homestead and a central meadow as its center. There is a line of farms next to the road. The arable land to the east is bordered by a ditch supplying fresh drinking water (Middle Dutch bansloot). In layout, this type represents a hybrid between the two earlier settlement types.\nSketch of a hybrid between the two earlier settlement types\nLet us first make clear that these different types of exploitation settlements often existed alongside each other and can be found in one and the same region. In part, the different types reflect different chronological layers but some types were also more suited to certain geographical environments than others.So how were these settlements planned? Many medieval exploitation enterprises were initiated by a monastery or a consortium of free men who were granted permission by (or bought permission from) the feudal lord to “colonize” the wilderness.Clearing the wooded landscape in order to create arable land was done by cutting away the trees and bushes (Middle Dutch rode) or, alternatively, burning it away in controlled fires (Middle Dutch brant).Land surveyors sent by the lord would then measure out the block or strips that would be taken in cultivation. Strips of arable land were often 1250m deep (6 Middle Dutch voorlingen = furlongs) so that the plough could go straight in a long line before having to turn. Important blocks or strips were demarcated by hedges, earthwork, woodwork, ditches or roads. Medieval names for these blocks often survived into the modern day. The presence of drinking water (a river or a brook) in the vicinity was an important factor in choosing the location for the settlement. The vicinity of water entailed risk and reward because flooding was an ever present danger. Floods could devastate arable land but might also fertilize it. Meadows in particular were often situated in flood areas. Managing a settlementSo how was such a settlement managed? First of all, the quality of the soil had to be carefully controlled by crop rotation: specific crops were sown on different segments of the arable land with one part laying fallow to recover from the tilling (English three-field system, Dutch drieslagstelsel). The cattle and sheep were put out to pasture on the common meadows guarded by a shepherd or cowherd. Pigs were allowed to forage in the nearby forests and killed in autumn before the winter starvation set in.Roads and rivers were important for transport of crops and livestock. These roads, some of them paved, some of them not, needed to be maintained. They were essential to the payment of the tithe, since tithe collectors assessed the harvest on the field and later collected the sheaves on the side of the road.The buildings within the community also needed maintenance. Farmhouses, community barns and stables were made of wood and had to be rebuilt every few generations, only the name of the farm or homestead being continued.So what kind of threats did a medieval settlement face? First of all, the weather was an important factor which dictated the success of the harvest. Storms, droughts and floods could devastate the harvest and decimate the community.Diseases and epidemics were another danger threatening the community. The situation on the countryside was a lot better in this regard than in the medieval towns, but an epidemic could still mean the end of a village. Similarly, diseases among livestock impacted the medieval subsistence economy in a brutal way.Then there are the consequences of medieval warfare affecting the community: Armies that passed by could plunder the village, burn the farms and execute villagers at will. Or they could also demand supplies, food and provisions as an emergency \"tax\"\nScreenshot: Burning village in Knights and Merchants (1998)\nBut war also brought indirect consequences; a liege lord calling the banners and levying troops from the village community might extract a large part of the adult men. Warfare also disrupted the trade networks that supplied a village with building materials and commodities.Then there were internal threats to the fabric of the village community. We may think of social unrest because of land disputes. Feuds could also tear a community apart with endemic vendetta’s causing death and despair. A socially unstable society was also more prone to internal accusations of heresy and witchcraft.An “accurate” medieval settlement builderSo, which of the above listed features could potentially contribute to a more historically accurate computer game about medieval settlement building? First of all, it would be more realistic if the settlement could first be planned out and was not forced to \"grow organically\" from a community center. The first settlement phase would be a test of how “successful” a layout is in adapting to the exigencies of the terrain and the needs of the community. Only after that initial layout proved successful, further expansions can be planned. Secondly, it would be more realistic if we could build both straight roads and curved roads, just as in Cities Skylines (2015), a modern city builder well known for its incredibly flexible layout tools. Incidentally, the tools of Cities Skylines can also be used to recreate medieval settlements, as was done by YouTube creator Play Curiously who constructed an impression of a medieval Croatian village.\nScreenshot: Cities Skylines (2015) by YouTuber Play Curiously\nSuch a flexible road drawing tool can then also be used to lay out ditches, hedges and enclosures since these features were central to the medieval experience of the cultivated landscape.Thirdly, It would be interesting to see a medieval-themed game embrace the concept of flood valleys that limit and endanger pasture and arable land. Other historical city builders such as Pharaoh (1999) and Children of the Nile (2004) already implemented this feature for their setting in Ancient Egypt. However, such a mechanic would likewise fit a medieval city builder and show the general public how medieval society dealt with seasonal flooding as well as the devastating effects that storm floods could have.\nScreenshot:\nChildren of the Nile (2004)\nAnd finally, something that would, in my opinion, really add to the realism and historical flavor of a medieval-themed city builder would be the introduction of mechanisms in which agricultural surpluses are skimmed by the church and the feudal lord. Tithes, taxes and rents! Instead of merely abstracting the taxes into an income modifier or letting the player be the extractor himself, we could be shown the tax collector visiting the village, counting the sheaves by the side of the road, selecting the calves and chickens. This way, the experiences of our medieval forebears are visualized and may help to educate the public about medieval village life.\nWhy not?\nThere are some good reasons why city building games are not that historically accurate and instead adhere to the established formula of the city building game.\nFirst of all, a linear growth model makes sense from a gameplay perspective, since it is rewarding to see your settlement grow.in a linear way. It fosters a feeling of progress and motivates the player to keep momentum and push through to the next expansion phase. Secondly, games are generally wary of punishing failure too harshly in order to avoid demoralizing the player. Thirdly, in order to facilitate path finding for the simulated villagers it is easier to implement a gridlike road and building system rather than an off-grid building system that allows for curvy roads. So far only Cities Skylines has managed to do this in a satisfactory way.\nLastly, for marketing purposes and recognizability, game developers generally don't stray too far from the image of the Middle Ages that the public is already acquainted with. For a medieval city builder this means windmills, industrious peasants, lots of sheep and stone castles. Things like land surveying, crop rotation and tithe collection do not fit this image and challenge the romanticized picture of the uneducated farmer in his pre-industrial environment.\nConclusion\nAlthough I think medieval-themed city building games could benefit from incorporating some of the things we know about medieval settlement history into the gameplay loop, it may not be desirable for game developers to stray too far from the established formula. The idea that medieval settlements developed organically according to messy road plans is strongly imbedded in popular perception. Allowing both straight and curved road building in medieval city builders, may serve to challenge some of the stereotypes that exist about medieval village life. And if you ask me, that would be a good thing for it is an enriching experience to see the world through the eyes of our medieval forebears. One may find out that their lives were not that different after all...\nFurther reading\nFossier, R. (2004). “The Rural Economy and Demographic Growth.” In: D. Luscombe & J. Riley-Smith (Eds.). The New Cambridge Medieval History. Cambridge: Cambridge University Press, 11-46.Van Ham, W. (1979). “Dorp en dorpsleven in middeleeuws Wouw.\" in: A. Delahaye (red.), De Heren XVII van Nassau-Brabant, 316-336.”(forthc.) Kerkhof, P.A. (2020). “Saer, Saert; een Zuid-Nederlandse veldnaam van onzekere oorsprong.” Noordbrabants Historisch Jaarboek.Leenders, K.A.H.W. (1996). \"Noord-Vlaanderen en de Noordwesthoek; een vergelijking.\" Tijdschrift voor Waterstaatsgeschiedenis 5, 67-73.Leenders, K.A.H.W. (1989). Verdwenen venen; een onderzoek naar de ligging en exploitatie van thans verdwenen venen in het gebied tussen Antwerpen, Turnhout, Geertruidenberg en Willemstad (1250-1750). Reeks Landschapsstudies 13, Wageningen.Oosthuizen, S. (2017). The Anglo-Saxon fenland. Windgather Press.\n© Alexia Kerkhof and Leiden Medievalists Blog, 2020. Unauthorised use and/or duplication of this material without express and written permission from this site’s author and/or owner is strictly prohibited. Excerpts and links may be used, provided that full and clear credit is given to Alexia Kerkhof and Leiden Medievalists Blog with appropriate and specific direction to the original content.",
      "publish_datetime": "2020-05-01T09:00:00+02:00",
      "scraping_timestamp": "2026-01-23T10:29:13.026612Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://susam.net/writing-first-tooling-second.html",
      "author": null,
      "title": "Writing First, Tooling Second - Susam Pal",
      "source": "susam.net",
      "content": "Writing First, Tooling Second\nBy Susam Pal on 10 Jan 2026\nI am a strong proponent of running independent\npersonal websites on your own\ndomains and publishing your writing there.\nDoing so keeps the web\ndiverse and decentralised, rather than concentrating most writing\nand discussion inside a small number of large platforms.\nIt gives\nauthors long term control over their work without being subject to\nchanging policies or\nincentives.\nI think that a web made up of many small, individually run websites\nis more resilient and also more interesting than one dominated by a\nhandful of social media services.\nI often participate in discussions pertaining to authoring personal\nwebsites because this is an area I am passionate about.\nAny\ndiscussion about authoring websites that I take part in seems to\ndrift, sooner or later, into tooling.\nAspiring personal website\nauthors worry at length about which blogging engine to use, which\nstatic site generator to pick, which templating language to choose\nand so on.\nI think none of this is important until you have\npublished at least five articles on your website.\nJust write plain\nHTML and worry about tooling later.\nThis very website you are reading right now began its life as a\nloose collection of HTML files typed into Notepad on a Windows 98\nmachine.\nI wrote some text, wrapped it in basic markup and copied\nit to my web server's document root directory.\nThat's it.\nTooling\ncame much later, in fact many years later.\nAs the number of pages\ngrew, I naturally wanted some consistency.\nI wanted a common layout\nfor my pages, navigation links, a footer that did not need to be\nedited in twenty places.\nAn early version of this website used HTML\nframes to accomplish these goals.\nLater, I rewrote the website in\nPHP.\nNow this website is generated using Common Lisp.\nAll of these\nadditions came later, after I had been maintaining this website for\nseveral years.\nNone of this was required to begin.\nAround 2006, when blogging had become quite fashionable, I\nexperimented with blogging too.\nEventually, I returned to a loose,\nchaotic collection of pages.\nI do have an\nindex page and an RSS\nfeed that resemble a blog, but they are simply a list of\nselected pages arranged in reverse chronological order.\nThe pages\nthemselves are scattered across various\ncorners of this website.\nMy point is that\nnot every website needs to be a blog.\nA website can just as well be\na collection of pages, arranged in whatever way makes sense to you.\nThe blog is merely one possible organising principle, not a\nrequirement.\nIf your goal is simply to share your thoughts on your\nown web space, worrying about blogging and tooling can easily become\ncounterproductive.\nJust write your posts first, in plain HTML if\nneed be.\nIf you truly dislike writing HTML, that is fine too.\nWrite in\nMarkdown, AsciiDoc or whatever plain text format you find pleasant\nand convert it to HTML using Pandoc or a similar tool.\nYes, I am\nslightly undermining my own point here but I think a little bit of\ntooling to make your writing process enjoyable is reasonable.\nTooling should exist to reduce friction, not to become the main\nceremony.\nPersonally, I write all my posts directly in HTML.\nI use\nEmacs, which provides a number of convenient key sequences and\nfunctions that make writing HTML very comfortable.\nFor example, it\ntakes only a few keystrokes in Emacs to wrap text in a\n<blockquote> element, insert a code block or\nclose any open tags.\nBut that is just me.\nI enjoy writing HTML\ndocuments in Emacs.\nIf you do not, it is easy to run your Markdown\nfiles through a converter and publish the result with only a little\nextra tooling overhead.\nIt is easy to spend days and weeks polishing a website setup,\nselecting the perfect generator, theme and deployment pipeline, only\nto end up with a beautifully engineered website whose sole content\nis a single 'hello world' post.\nThat might not be very useful to\nyou or anyone else, unless setting up the pipeline itself was your\ngoal.\nBy contrast, a scrappy website made up of standalone HTML\npages might be useful to you as well as others.\nYou can refer back\nto it months later.\nYou can send someone a link.\nYou can build on\nit gradually.\nEven if you never turn it into a blog, never add RSS\nor never add any tooling, it still fulfills its most important\npurpose: it exists and it says something you wanted to say.\nSo to summarise my post here: Create the website.\nPublish\nsomething.\nDo it in the simplest way that lets you get your words\nonto the page and onto the web.\nOnce you have content that you care\nabout, tooling can follow.\nYour thoughts, your ideas, your\npersonality and quirks are the essence of your website.\nEverything\nelse is optional.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:29:15.752598Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://github.com/malvarezcastillo/txt2plotter",
      "author": "malvarezcastillo",
      "title": "GitHub - malvarezcastillo/txt2plotter",
      "source": "github.com",
      "content": "txt2plotter\nConvert text prompts to pen-plotter-ready SVG files using AI image generation and centerline vectorization.\nPipeline\nPrompt Enhancement - LLM rewrites your prompt for optimal line art generation\nRaster Generation - Flux.2-dev generates a high-contrast line art image\nVectorization - Skeletonization and graph extraction produce clean paths\nOptimization - Paths are merged, simplified, and sorted for efficient plotting\nOutput - Plotter-ready SVG with configurable dimensions\nExamples\nPlotted\nRequirements\nPython 3.10+\nNVIDIA GPU with 24GB VRAM (RTX 3090/4090)\nCUDA 12.x\nOpenRouter API key for prompt enhancement\nHuggingFace token with access to Flux.2-dev\nInstallation\ngit clone https://github.com/malvarezcastillo/txt2plotter.git\ncd txt2plotter\n# Create virtual environment\npython3.10 -m venv .venv\nsource .venv/bin/activate\n# Install dependencies\npip install -e .\n# Configure API keys\ncp .env.example .env\n# Edit .env with your keys\nUsage\n# Basic usage (A3 size)\npython main.py \"a geometric skull\"\n# Custom dimensions (A4)\npython main.py \"circuit board pattern\" --width 297 --height 210\n# Generate multiple variations\npython main.py \"mountain landscape\" -n 5\n# Reproducible generation with seed\npython main.py \"geometric pattern\" --seed 42\n# Generate 3 reproducible variations (uses seed, seed+1, seed+2)\npython main.py \"geometric pattern\" -n 3 --seed 42\n# Skip prompt enhancement\npython main.py \"minimalistic line drawing of a cat\" --skip-enhance\n# Batch mode: process multiple prompts from file\npython main.py --batch prompts.txt -n 10\nBatch File Format\nCreate a prompts.txt file (one prompt per line, supports comments):\n# My prompts\n\"Minimalistic isometric impossible cube, thick black lines, technical drawing.\"\n\"Single continuous line drawing of a greyhound, Picasso style.\"\n# This is also valid (no quotes)\nA geometric skull with clean vector lines\nOutput is organized by prompt: output/<prompt_slug>/\nOutput\noutput/*.svg - Final plotter-ready SVGs\noutput/debug/ - Intermediate files for debugging:\n01_prompt_enhanced.txt - Enhanced prompt\n02_raster_raw.png - Generated image\n02_raster_binary.png - Thresholded binary\n03_skeleton.png - Skeletonized paths\n03_graph_*.png - Graph visualization\n03_paths.svg - Raw paths\n04_optimized.svg - After optimization\nLicense\nMIT",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:29:21.437350Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://github.com/tursodatabase/turso",
      "author": "tursodatabase",
      "title": "GitHub - tursodatabase/turso: Turso is an in-process SQL database, compatible with SQLite.",
      "source": "github.com",
      "content": "Turso Database\nAn in-process SQL database, compatible with SQLite.\nAbout\nTurso Database is an in-process SQL database written in Rust, compatible with SQLite.\n⚠️ Warning: This software is in BETA. It may still contain bugs and unexpected behavior. Use caution with production data and ensure you have backups.\nFeatures and Roadmap\nSQLite compatibility for SQL dialect, file formats, and the C API [see document for details]\nChange data capture (CDC) for real-time tracking of database changes.\nMulti-language support for\nGo\nJavaScript\nJava\nPython\nRust\nWebAssembly\nAsynchronous I/O support on Linux with io_uring\nCross-platform support for Linux, macOS, Windows and browsers (through WebAssembly)\nVector support support including exact search and vector manipulation\nImproved schema management including extended ALTER support and faster schema changes.\nThe database has the following experimental features:\nBEGIN CONCURRENT for improved write throughput using multi-version concurrency control (MVCC).\nEncryption at rest for protecting the data locally.\nIncremental computation using DBSP for incremental view maintenance and query subscriptions.\nFull-Text-Search powered by the awesome tantivy library\nThe following features are on our current roadmap:\nVector indexing for fast approximate vector search, similar to libSQL vector search.\nGetting Started\nPlease see the Turso Database Manual for more information.\n💻 Command Line\nYou can install the latest `turso` release with:\ncurl --proto '=https' --tlsv1.2 -LsSf \\\nhttps://github.com/tursodatabase/turso/releases/latest/download/turso_cli-installer.sh | sh\nThen launch the interactive shell:\n$ tursodb\nThis will start the Turso interactive shell where you can execute SQL statements:\nTurso\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database\nturso> CREATE TABLE users (id INT, username TEXT);\nturso> INSERT INTO users VALUES (1, 'alice');\nturso> INSERT INTO users VALUES (2, 'bob');\nturso> SELECT * FROM users;\n1|alice\n2|bob\nYou can also build and run the latest development version with:\ncargo run\nIf you like docker, we got you covered. Simply run this in the root folder:\nmake docker-cli-build && \\\nmake docker-cli-run\n🦀 Rust\ncargo add turso\nExample usage:\nlet db = Builder::new_local(\"sqlite.db\").build().await?;\nlet conn = db.connect()?;\nlet res = conn.query(\"SELECT * FROM users\", ()).await?;\n✨ JavaScript\nnpm i @tursodatabase/database\nExample usage:\nimport { connect } from '@tursodatabase/database';\nconst db = await connect('sqlite.db');\nconst stmt = db.prepare('SELECT * FROM users');\nconst users = stmt.all();\nconsole.log(users);\n🐍 Python\nuv pip install pyturso\nExample usage:\nimport turso\ncon = turso.connect(\"sqlite.db\")\ncur = con.cursor()\nres = cur.execute(\"SELECT * FROM users\")\nprint(res.fetchone())\n🦫 Go\ngo get turso.tech/database/tursogo\ngo install turso.tech/database/tursogo\nExample usage:\nimport (\n\"database/sql\"\n_ \"turso.tech/database/tursogo\"\n)\nconn, _ = sql.Open(\"turso\", \"sqlite.db\")\ndefer conn.Close()\nstmt, _ := conn.Prepare(\"select * from users\")\ndefer stmt.Close()\nrows, _ = stmt.Query()\nfor rows.Next() {\nvar id int\nvar username string\n_ := rows.Scan(&id, &username)\nfmt.Printf(\"User: ID: %d, Username: %s\\n\", id, username)\n}\n☕️ Java\nWe integrated Turso Database into JDBC. For detailed instructions on how to use Turso Database with java, please refer to\nthe README.md under bindings/java.\n🤖 MCP Server Mode\nThe Turso CLI includes a built-in Model Context Protocol (MCP) server that allows AI assistants to interact with your databases.\nStart the MCP server with:\ntursodb your_database.db --mcp\nConfiguration\nAdd Turso to your MCP client configuration:\n{\n\"mcpServers\": {\n\"turso\": {\n\"command\": \"/path/to/.turso/tursodb\",\n\"args\": [\"/path/to/your/database.db\", \"--mcp\"]\n}\n}\n}\nAvailable Tools\nThe MCP server provides nine tools for database interaction:\nopen_database - Open a new database\ncurrent_database - Describe the current database\nlist_tables - List all tables in the database\ndescribe_table - Describe the structure of a specific table\nexecute_query - Execute read-only SELECT queries\ninsert_data - Insert new data into tables\nupdate_data - Update existing data in tables\ndelete_data - Delete data from tables\nschema_change - Execute schema modification statements (CREATE TABLE, ALTER TABLE, DROP TABLE)\nOnce connected, you can ask your AI assistant:\n\"Show me all tables in the database\"\n\"What's the schema for the users table?\"\n\"Find all posts with more than 100 upvotes\"\n\"Insert a new user with name 'Alice' and email 'alice@example.com'\"\nMCP Clients\nClaude Code\nIf you're using Claude Code, you can easily connect to your Turso MCP server using the built-in MCP management commands:\nQuick Setup\nAdd the MCP server to Claude Code:\nclaude mcp add my-database -- tursodb ./path/to/your/database.db --mcp\nRestart Claude Code to activate the connection\nStart querying your database through natural language!\nCommand Breakdown\nclaude mcp add my-database -- tursodb ./path/to/your/database.db --mcp\n#\n↑\n↑\n↑\n↑\n#\n|\n|\n|\n|\n#\nName\n|\nDatabase path\nMCP flag\n#\nSeparator\nmy-database - Choose any name for your MCP server\n-- - Required separator between Claude options and your command\ntursodb - The Turso database CLI\n./path/to/your/database.db - Path to your SQLite database file\n--mcp - Enables MCP server mode\nExample Usage\n# For a local project database\ncd /your/project\nclaude mcp add my-project-db -- tursodb ./data/app.db --mcp\n# For an absolute path\nclaude mcp add analytics-db -- tursodb /Users/you/databases/analytics.db --mcp\n# For a specific project (local scope)\nclaude mcp add project-db --local -- tursodb ./database.db --mcp\nManaging MCP Servers\n# List all configured MCP servers\nclaude mcp list\n# Get details about a specific server\nclaude mcp get my-database\n# Remove an MCP server\nclaude mcp remove my-database\nClaude Desktop\nFor Claude Desktop, add the configuration to your claude_desktop_config.json file:\n{\n\"mcpServers\": {\n\"turso\": {\n\"command\": \"/path/to/.turso/tursodb\",\n\"args\": [\"./path/to/your/database.db.db\", \"--mcp\"]\n}\n}\n}\nCursor\nFor Cursor, configure MCP in your settings:\nOpen Cursor settings\nNavigate to Extensions → MCP\nAdd a new server with:\nName: turso\nCommand: /path/to/.turso/tursodb\nArgs: [\"./path/to/your/database.db.db\", \"--mcp\"]\nAlternatively, you can add it to your Cursor configuration file directly.\nDirect JSON-RPC Usage\nThe MCP server runs as a single process that handles multiple JSON-RPC requests over stdin/stdout. Here's how to interact with it directly:\nExample with In-Memory Database\ncat << 'EOF' | tursodb --mcp\n{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\", \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"client\", \"version\": \"1.0\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/call\", \"params\": {\"name\": \"schema_change\", \"arguments\": {\"query\": \"CREATE TABLE users (id INTEGER, name TEXT, email TEXT)\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 3, \"method\": \"tools/call\", \"params\": {\"name\": \"list_tables\", \"arguments\": {}}}\n{\"jsonrpc\": \"2.0\", \"id\": 4, \"method\": \"tools/call\", \"params\": {\"name\": \"insert_data\", \"arguments\": {\"query\": \"INSERT INTO users VALUES (1, 'Alice', 'alice@example.com')\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 5, \"method\": \"tools/call\", \"params\": {\"name\": \"execute_query\", \"arguments\": {\"query\": \"SELECT * FROM users\"}}}\nEOF\nExample with Existing Database\n# Working with an existing database file\ncat << 'EOF' | tursodb mydb.db --mcp\n{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\", \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"client\", \"version\": \"1.0\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/call\", \"params\": {\"name\": \"list_tables\", \"arguments\": {}}}\nEOF\nContributing\nWe'd love to have you contribute to Turso Database! Please check out the contribution guide to get started.\nFound a data corruption bug? Get up to $1,000.00\nSQLite is loved because it is the most reliable database in the world. The next evolution of SQLite has\nto match or surpass this level of reliability. Turso is built with Deterministic Simulation Testing\nfrom the ground up, and is also tested by Antithesis.\nEven during Alpha, if you find a bug that leads to a data corruption and demonstrate\nhow our simulator failed to catch it, you can get up to $1,000.00. As the project matures we will\nincrease the size of the prize, and the scope of the bugs.\nList of rewarded cases:\nB-Tree interior cell replacement issue in btrees with depth >=3 (#2106)\nDon't allow autovacuum to be flipped on non-empty databases (#3830)\nMore details here.\nTurso core staff are not eligible.\nFAQ\nIs Turso Database ready for production use?\nTurso Database is currently under heavy development and is not ready for production use.\nHow is Turso Database different from Turso's libSQL?\nTurso Database is a project to build the next evolution of SQLite in Rust, with a strong open contribution focus and features like native async support, vector search, and more. The libSQL project is also an attempt to evolve SQLite in a similar direction, but through a fork rather than a rewrite.\nRewriting SQLite in Rust started as an unassuming experiment, and due to its incredible success, replaces libSQL as our intended direction. At this point, libSQL is production ready, Turso Database is not - although it is evolving rapidly. More details here.\nPublications\nPekka Enberg, Sasu Tarkoma, Jon Crowcroft Ashwin Rao (2024). Serverless Runtime / Database Co-Design With Asynchronous I/O. In EdgeSys ‘24. [PDF]\nPekka Enberg, Sasu Tarkoma, and Ashwin Rao (2023). Towards Database and Serverless Runtime Co-Design. In CoNEXT-SW ’23. [PDF] [Slides]\nLicense\nThis project is licensed under the MIT license.\nContribution\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Turso Database by you, shall be licensed as MIT, without any additional\nterms or conditions.\nPartners\nThanks to all the partners of Turso!\nContributors\nThanks to all the contributors to Turso Database!",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:29:28.058558Z",
      "categories": [
        "Programming/Software",
        "LLM/AI"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://www.stunnel.org/",
      "author": null,
      "title": "stunnel: Home",
      "source": "stunnel.org",
      "content": "Stunnel is a proxy designed to add TLS encryption functionality to existing clients and servers without any changes in the programs' code.\nIts architecture is optimized for security, portability, and scalability (including load-balancing), making it suitable for large deployments.\nStunnel uses the OpenSSL library for cryptography, so it supports whatever cryptographic algorithms are compiled into the library. It can benefit from the FIPS 140-2 validation of the OpenSSL FIPS Provider, as long as the building process meets the OpenSSL FIPS 140-2 Security Policy.\nOur latest Windows installer includes the OpenSSL FIPS Provider.\nStunnel is a free software authored by Michał Trojnara. Although distributed under GNU GPL version 2 or later with OpenSSL exception, stunnel is not a community project. We retain the copyright of the source code. Please contact us for commercial support or non-GPL licenses. Free, community-based support is also available via stunnel-users mailing list.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:29:31.397784Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://mas.to/@gabrielesvelto/115939583202357863",
      "author": null,
      "title": "Gabriele Svelto: \"In the early days of personal computing CPU bugs …\" - mas.to",
      "source": "mas.to",
      "content": "To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:29:34.315345Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://alvaromontoro.com/blog/68091/css-optical-illusions",
      "author": "Alvaro Montoro (alvaromontoro@gmail.com)",
      "title": "CSS Optical Illusions",
      "source": "alvaromontoro.com",
      "content": "You can find a collection with all the optical illusions in this article (and more!) on CodePen. You can move your mouse over many of the demos below to reveal the effect or stop the animations.\n1 - Poggendorff Illusions\nThe Poggendorff illusion is an optical illusion in which a diagonal line interrupted by a vertical bar appears misaligned, even when both segments are actually continuous.\nA simple version of this effect can be seen in the following demo. I used the ::before and ::after pseudo-elements to create the diagonal line and the vertical bar, respectively.\nThe effect can also be seen in a more elaborate version with multiple diagonal lines and vertical bars:\nThis drawing can easily be achieved using two CSS gradients: one tilted at 70 degrees and another consisting of a series of vertical columns. I applied it to the body, although I could have used :root instead.\nAnother variation of this illusion is the Münsterberg Poggendorff Arch, in which the two sides of an arch appear misaligned and seem as though they will not meet at the top - but they do (mouse over to see it).\n2 - Induced Gradients\nThe following illusions combine gradients and flat colors. Surprisingly, some of the gradients do not actually exist. They are simple gray bars that, when placed over a gradient, appear to have gradients themselves.\nTake the following demo: all three bars (two vertical ones on the sides and one horizontal bar in the center) are the same shade of gray. The only real gradient is behind them, which tricks our brain into believing that the bars are different colors and even contain gradients.\nHere is another variation of this effect. It looks like the central line has a repeating gradient of dark and light grays, but in reality it is a flat color. If you mouse over the demo, the bar will expand, making it clear that there is no gradient at all.\n3 - Cornsweet Illusion\nThe next few optical illusions share a common idea: some colors are identical, but they do not look the same. This typically happens when regions of the same color or brightness are surrounded by areas with different contrast.\nFor example, in the following demo, the left and right ends are the same shade of gray. However, one looks lighter because it is closer to white, while the other looks darker because it is closer to black. Mouse over to reveal that they are, in fact, the same color.\n4 - White's Illusion\nRun the following demo. You will see two gray columns in a black-and-white grid. Both columns are the same shade of gray, but the one surrounded by black appears darker than the one surrounded by white.\nI coded this demo using mix-blend-mode so I could try something a bit different. That worked well, but it also made it harder to showcase the effect on hover. In hindsight, I should have planned that better.\nThis optical illusion also works with colors. For example, these two squares appear to be different shades of blue, but they are the same color. This time, you can mouse over to reveal the effect:\n5 - Wertheimer-Koffka Ring\nThe ring in the following illustration has the same color all the way around. However, one side is placed over white and the other over black, which makes them look different. If you mouse over the demo, the red bar will disappear, making it more obvious that the ring is a single, uniform color.\n6 - Adelson's Illusion\nYou have probably seen the illusion involving a checkerboard and an object casting a shadow, where two tiles - one seemingly light and one seemingly dark - turn out to be the same color.\nThis demo follows the same principle. You will see two tiles labeled A and B. Both have the same shade of gray, but most people cannot tell at first glance (or second, or even third).\n7 - Asahi illusion of Brightness\nThe circle at the center of this flower-shaped element is the same white as the rest of the page, but it gives the impression of being brighter, as if it were emitting light.\n8 - Color Spheres\nThis is one of my favorite illusions in the collection. The circles (or spheres) look red, blue, or green, but in reality they are all the same grayish color. Our brain \"colorizes\" them based on the lines that overlap the shapes. Don't believe it? Mouse over the illustration.\n9 - Colors from Contour\nIn the following illustration, the lines inside the yellow section appear blue, while the lines inside the blue section appear red... but they are all black (or very dark gray). The white contour creates the illusion of color. Mouse over to remove the contour and the lines will clearly appear black.\n10 - Curvature Blindness\nOne set of lines looks straighter (top) while the other looks more curved (bottom). In reality, both sets are equally wavy. The only difference is how they are colored: changing the color at the peaks makes the lines look straighter. Changing it at the inflection points makes them look more curved.\nThe CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.\n11 - Cafe Wall\nThis is a classic optical illusion and an easy one to code in CSS. Three gradients are all that is needed to generate the effect in which the horizontal lines appear slanted, even though they are perfectly parallel.\n12 - Penrose Triangle\nThis optical illusion depicts an impossible shape. Parts that should be in front appear in the back, top becomes right, and everything feels contradictory. I coded this one some time ago for the 2024 Divtober event.\n13 - Ebbinghaus Illusion\nWhich orange circle is larger: the one on the right or the one on the left? It is a trick question: both are the same size. However, having smaller surrounding elements gives the impression that one is larger.\nI also created an animated version of this illusion (see below), as well as another version using a square shape instead of a flower shape:\n14 - Kanizsa Square\nWhen people look at this illustration, they usually say they see a white square over black circles. However, the square is not actually there. The \"Pac-Man\" shapes create the illusion of a square and a sense of depth. Our brain fills in the missing information.\n15 - Ehrenstein's Illusion\nThere are no circles or discs in this illustration, only vertical and horizontal lines forming crosses. Our visual system completes the shape and makes us perceive a disc that does not exist.\n16 - Neon-Color-Spreading Illusion\nThis illustration shows concentric circles, some of which have a green-and-black pattern. Our brain perceives a central patterned circle and four concentric circles around it, beneath the green circle.\nI cheated a little when creating this in CSS, as I actually used a green circle blended with the other backgrounds.\n17 - Hering and Wundt Illusions\nPerspective-based illusions are fascinating. Even when we know we are looking at a flat image, our brain insists on interpreting depth.\nIn the Hering illusion, the red lines appear to curve outward, even though they are straight.\nThe opposite effect is the Wundt illusion. When the lines expand from the sides toward the center, the red lines appear to curve inward (this effect is more subtle).\n18 - Ponzo Illusion\nBoth yellow lines are the same length, but the top one looks longer due to perceived depth and perspective. I tried a different approach when coding this one by applying a three-dimensional rotation in CSS... so the perspective is technically real.\n19 - T Illusion\nThis illusion is easy to code in CSS and easy to fall for. Both the vertical and horizontal lines are the same length, but the vertical line appears longer.\n20 - Müller-Lyer Illusion\nA classic illusion: the horizontal lines are the same length, but inward- or outward-pointing edges dramatically change how we perceive them. I could swear the top one is longer. But it is not.\nFrom a coding perspective, each shape is a pseudo-element. I ensured the horizontal lines were identical by using the same gradients and only repositioning the edges in the ::before and ::after.\n21 - Tilted Table Illusion\nIt looks like the top rectangle is leaning to the left, but it is actually parallel to the one at the bottom. The trick lies in the direction of the diagonal lines used to \"color\" each rectangle.\nThis illusion works better on larger screens. The effect is diminished when you can see the whole picture.\n22 - Parallel Lines\nThis is a simple effect: the black lines are parallel, but they appear not to be because of the direction of the bars crossing them.\nI slightly overcomplicated this one while coding it. I initially built the black-and-red version below and tried to reuse more code than I probably should have.\nHere is the original version I created. The effect is also visible there:\nGood news! There are more optical illusions below - but first, a warning.\nATTENTION: The following optical illusions are static, but they give the impression of movement. Proceed accordingly.\n(Leaving some blank space in case you do not want to continue.)\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n23 - Expanding Hole\nThis is a trippy optical illusion. It is completely static, yet it looks like the black hole at the center is expanding - especially when you are not looking at it directly, creating the sensation of falling into a pit.\nFrom a coding perspective, this one was very simple: a background pattern made with two radial gradients, plus a blurred pseudo-element for the \"expanding\" hole.\n24 - Rotating Snakes\nThis is one of only two optical illusions in this collection where I used HTML elements instead of relying exclusively on CSS. It is a classic effect: when you look at the illustration, the peripheral discs appear to rotate, even though nothing is actually moving.\n25 - Appearing Dots\nAnother classic illusion. Focus on the white dots and the adjacent dots will appear to turn black. There is no animation, no transition, and nothing dynamic. Just intersecting lines and small white circles, yet it looks like motion.\n26 - Disappearing Dots\nThis pattern consists of repeating black and white dots across the page. If you focus on one dot, the others will begin to disappear. At first it may happen by row or column, but after a short while, most of them vanish.\nIf you do not immediately see the effect, try focusing on one black dot. Mouse over it, wait a few seconds while keeping your focus, and then mouse out.\n27 - Ouchi Illusion\nThis is a static image, but it gives the impression that the pattern inside the circle is moving sideways. This happens because our eyes are constantly making small movements, even when we are not aware of it.\nIf you cannot see the illusion, try slightly moving the screen (or your head) while looking just outside the circle.\n28 - Orthogonal Dotted Lines Sway\nWhen you look around this pattern, the central area appears to slide and sway, even though it is completely static. This illusion makes me dizzy... but that may also be because I had to stare at it for a long time while coding it.\n29 - Enigma\nThis illusion is particularly interesting. There is a pink circle surrounded by concentric pink and purple rings. If you focus on the pink circle, the rings appear to spin or scintillate, as if there were some activity in them. Of course, nothing is actually moving.\n30 - Waves\nThis demo was challenging to code and takes a long time to load. Mainly because it uses a large number of conic gradients behind the scenes, which browsers struggle to render efficiently. There is probably a better way to implement it, but I have not explored that yet.\nIf you look closely at the illustration, you may notice wave-like motion. As with the previous illusions in this section, the image is entirely static.\nGood news! There are more optical illusions below - but first, another warning.\nATTENTION: The following optical illusions actually move, and the illusion is created by motion itself. Some of them can be dizzying, so proceed accordingly.\n(Leaving some blank space in case you do not want to continue.)\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n31 - Animated Ebbinghaus Illusion\nEarlier, we saw two static versions of the Ebbinghaus illusion. This one is animated. The elements move side to side, and the surrounding shapes grow and shrink, giving the impression that the orange circle is changing size - when it definitely is not.\n32 - Psychokinematic Tower\nThis looks like a three-dimensional tower spinning in space, as seen from above. In reality, it is a flat, two-dimensional image rotating.\nMouse over the demo to stop the rotation and the illusion of depth disappears entirely.\n33 - Color Fan\nThis optical illusion requires only two gradients: a conic gradient for the fan-shaped arms and a radial gradient for the circles and discs.\nIf you focus on the black dot, the illustration may appear to develop a darker greenish or brownish border. However, the colors never change.\n34 - Reverse Spoke Illusion\nThis illusion is delightful and disorienting. While the background colors of the wheel are spinning, the spokes remain fixed. However, they appear to rotate in the opposite direction. In reality, only the background is moving.\n35 - Motion Binding\nWhat do you see in this animation? Most people report two sets of lines operating independently: one moving horizontally and another moving vertically. And that is exactly how it looks.\nIn reality, it is a single shape moving uniformly. Run the demo, mouse over the lines, and the true motion will be revealed.\n36 - Mainz-Linez Illusion\nFocus on one of the red dots. You will notice it moves straight up and down along a vertical path. Now shift your focus to one of the black crosses in the center. Suddenly, the red dots appear to zigzag instead of moving straight.\nThe CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.\n37 - Waddling Colors\nIt may look like the boxes are moving at different speeds or like a set of walking feet. In reality, all elements move at the same pace and in parallel. Mouse over the demo to reveal the effect.\nThe illusion also works when the \"feet\" move in circles, as shown in this alternative version:\n38 - Dotted-Line Motion\nFollow the red dot as it moves sideways. From the corner of your vision, it may appear that the dashed black-and-white lines are moving closer together (when the dot moves left) or farther apart (when it moves right). In reality, the lines are completely static.\n39 - Contrast Asynchrony\nThese dots always have the same color. However, when placed against alternating backgrounds, they appear to jump or move out of sync because of how they blend with their surroundings.\nMouse over the demo to remove the background and the illusion disappears.\n40 - Breathing Square\nThis illusion gives the impression that a blue square is growing and shrinking rhythmically, almost as if it were breathing or beating like a heart.\nAlthough the image is rotating, its size never changes. Mouse over the illustration to remove the green boxes and reveal the rotating blue square.\n41 - Troxler Fading\nThis illustration shows a circle made of pink dots, with one dot missing. Focus on the cross at the center and the missing dot will appear as a yellow or green dot, giving the impression that it is \"eating\" the pink dots. Just like Pac-Man.\nI could have used CSS trigonometric functions to calculate the exact positions of the dots, but since they never change, I chose to hardcode the values instead.\nHere is a related effect. Follow the light gray circle as it spins, and the darker circles will appear to change from gray to greenish. Focus on the cross at the center, and after a short time, the darker circles may begin to fade entirely.\n42 - Pinna-Brelstaff Illusion\nThis illusion is particularly dizzying. Follow the bluish dot as it moves from right to left and back again. It will appear as though parts of the tiled background are shifting, even though they are static. The only moving element is the dot.\nFrom a CSS perspective, I coded the pattern using conic gradients, and applied it to the ::before and ::after pseudo-elements. I then flipped one upside down and clipped it.\n43 - Palisade\nThe radii of a wheel, when viewed through a palisade, appear to curve. In reality, they are perfectly straight. Mouse over the demo to remove the palisade and you will see that the radii never bend.\n44 - Alternative Motion\nThis animation demonstrates how our minds infer motion that may not actually be there. Consider the two blue dots. Different people perceive different movements: side to side, top to bottom, or even circular motion.\nCover the right side of the animation so that you see only one dot at a time. The motion now appears vertical. Cover the bottom part instead, and the motion appears horizontal. This is our brain trying to complete the movement.\n45 - Motion Inversion\nThese two illustrations are identical - same shapes, same animation. The only difference is the CSS timing function.\nThe top animation moves smoothly from right to left. The bottom one appears to move choppily in the same direction, but if you focus on it, it may suddenly seem to reverse direction and move faster.\nMost of the inspiration for these optical illusions came from two excellent resources:\nYou can also find this article on:\n(You can leave comments on those platforms and I will reply there).",
      "publish_datetime": "2026-01-22",
      "scraping_timestamp": "2026-01-23T10:29:40.750917Z",
      "categories": [
        "Programming/Software"
      ],
      "primary_category": "Programming/Software"
    },
    {
      "link": "https://news.ycombinator.com/item?id=46721933",
      "author": null,
      "title": "Launch HN: Constellation Space (YC W26) - AI for satellite mission assurance | Hacker News",
      "source": "news.ycombinator.com",
      "content": "Hi HN! We're Kamran, Raaid, Laith, and Omeed from Constellation Space (https://constellation-io.com/). We built an AI system that predicts satellite link failures before they happen. Here's a video walkthrough: https://www.youtube.com/watch?v=069V9fADAtM.Between us, we've spent years working on satellite operations at SpaceX, Blue Origin, and NASA. At SpaceX, we managed constellation health for Starlink. At Blue, we worked on next-gen test infra for New Glenn. At NASA, we dealt with deep space communications. The same problem kept coming up: by the time you notice a link is degrading, you've often already lost data.The core issue is that satellite RF links are affected by dozens of interacting variables. A satellite passes overhead, and you need to predict whether the link will hold for the next few minutes. That depends on: the orbital geometry (elevation angle changes constantly), tropospheric attenuation (humidity affects signal loss via ITU-R P.676), rain fade (calculated via ITU-R P.618 - rain rates in mm/hr translate directly to dB of loss at Ka-band and above), ionospheric scintillation (we track the KP index from magnetometer networks), and network congestion on top of all that.The traditional approach is reactive. Operators watch dashboards, and when SNR drops below a threshold, they manually reroute traffic or switch to a backup link. With 10,000 satellites in orbit today and 70,000+ projected by 2030, this doesn't scale.\nOur system ingests telemetry at around 100,000 messages per second from satellites, ground stations, weather radar, IoT humidity sensors, and space weather monitors. We run physics-based models in real-time - the full link budget equations, ITU atmospheric standards, orbital propagation - to compute what should be happening. Then we layer ML models on top, trained on billions of data points from actual multi-orbit operations.The ML piece is where it gets interesting. We use federated learning because constellation operators (understandably) don't want to share raw telemetry. Each constellation trains local models on their own data, and we aggregate only the high-level patterns. This gives us transfer learning across different orbit types and frequency bands - learnings from LEO Ka-band links help optimize MEO or GEO operations.\nWe can predict most link failures 3-5 minutes out with >90% accuracy, which gives enough time to reroute traffic before data loss. The system is fully containerized (Docker/Kubernetes) and deploys on-premise for air-gapped environments, on GovCloud (AWS GovCloud, Azure Government), or standard commercial clouds.Right now we're testing with defense and commercial partners. The dashboard shows real-time link health, forecasts at 60/180/300 seconds out, and root cause analysis (is this rain fade? satellite setting below horizon? congestion?). We expose everything via API - telemetry ingestion, predictions, topology snapshots, even an LLM chat endpoint for natural language troubleshooting.The hard parts we're still working on: prediction accuracy degrades for longer time horizons (beyond 5 minutes gets dicey), we need more labeled failure data for rare edge cases, and the federated learning setup requires careful orchestration across different operators' security boundaries.\nWe'd love feedback from anyone who's worked on satellite ops, RF link modeling, or time-series prediction at scale. What are we missing? What would make this actually useful in a production NOC environment?Happy to answer any technical questions!",
      "publish_datetime": null,
      "scraping_timestamp": "2026-01-23T10:29:43.742639Z",
      "categories": [
        "LLM/AI",
        "Programming/Software"
      ],
      "primary_category": "LLM/AI"
    }
  ]
}