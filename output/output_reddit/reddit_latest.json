[
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx9c2x/current_level_model_recommendation/",
    "author": "OldPhotojournalist28",
    "title": "Current level / model recommendation",
    "source": "reddit",
    "content": "Honestly, what are your thoughts on the current level of open weights models vs Opus 4.5/4.6 current standard?\n\nI need a model recommendation that comes close to Opus 4.5 in terms of coding performance and intelligence for my projects.\n\nI tried so far only GLM4.7-Flash and I had a terrible experience, it hallucinated after 3 prompts.\n\nWilling to buy more GPUs but I feel like most of the people giving recommendations do that based on a snake game which is hilarious. I need real people with real workflows and complex codebases to compare open vs closed models?\n\nCan anyone relate?",
    "publish_datetime": "2026-02-06T05:43:11Z",
    "scraping_timestamp": "2026-02-06T05:46:23.066139Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Current level / model recommendation",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx9c2x/current_level_model_recommendation/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx9c2x/current_level_model_recommendation/",
      "selftext": "Honestly, what are your thoughts on the current level of open weights models vs Opus 4.5/4.6 current standard?\n\nI need a model recommendation that comes close to Opus 4.5 in terms of coding performance and intelligence for my projects.\n\nI tried so far only GLM4.7-Flash and I had a terrible experience, it hallucinated after 3 prompts.\n\nWilling to buy more GPUs but I feel like most of the people giving recommendations do that based on a snake game which is hilarious. I need real people with real workflows and complex codebases to compare open vs closed models?\n\nCan anyone relate?",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770356591.0,
      "author": "OldPhotojournalist28"
    }
  },
  {
    "link": "https://i.redd.it/xn34gdcd9thg1.png",
    "author": "fais-1669",
    "title": "Deep what do you think?",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-06T05:33:40Z",
    "scraping_timestamp": "2026-02-06T05:46:23.066155Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 3,
    "num_comments": 0,
    "engagement_score": 3.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Deep what do you think?",
      "url": "https://i.redd.it/xn34gdcd9thg1.png",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx95kh/deep_what_do_you_think/",
      "selftext": "",
      "score": 3,
      "num_comments": 0,
      "created_utc": 1770356020.0,
      "author": "fais-1669"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx7dj4/9960x_4_gpu/",
    "author": "handheadbodydemeanor",
    "title": "9960x + 4 gpu",
    "source": "reddit",
    "content": "What mobo/case are you using for TR(no pro) + 4 gpu?\n\nis it ok to have 2+ PSU's to feed different GPUs",
    "publish_datetime": "2026-02-06T04:03:21Z",
    "scraping_timestamp": "2026-02-06T05:46:23.066251Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 2,
    "engagement_score": 5.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "9960x + 4 gpu",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx7dj4/9960x_4_gpu/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx7dj4/9960x_4_gpu/",
      "selftext": "What mobo/case are you using for TR(no pro) + 4 gpu?\n\nis it ok to have 2+ PSU's to feed different GPUs",
      "score": 1,
      "num_comments": 2,
      "created_utc": 1770350601.0,
      "author": "handheadbodydemeanor"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/",
    "author": "TwistedDiesel53",
    "title": "I am absolutely loving qwen3-235b",
    "source": "reddit",
    "content": "I installed qwen3-235b on my desktop system, and I had to join here to brag about it. It's such a careful model, the accuracy of it's output is unbelievable and I've found myself using it absolutely constantly to the point my chatgpt pro subscription is getting left behind. The ability to get carefully curated information of this quality from your own desktop PC is astounding to me and for my use puts all the commercial subscriptions to shame. Sorry for the rant lol!",
    "publish_datetime": "2026-02-06T03:55:56Z",
    "scraping_timestamp": "2026-02-06T05:46:23.066514Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 48,
    "num_comments": 32,
    "engagement_score": 112.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "I am absolutely loving qwen3-235b",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/",
      "selftext": "I installed qwen3-235b on my desktop system, and I had to join here to brag about it. It's such a careful model, the accuracy of it's output is unbelievable and I've found myself using it absolutely constantly to the point my chatgpt pro subscription is getting left behind. The ability to get carefully curated information of this quality from your own desktop PC is astounding to me and for my use puts all the commercial subscriptions to shame. Sorry for the rant lol!",
      "score": 48,
      "num_comments": 32,
      "created_utc": 1770350156.0,
      "author": "TwistedDiesel53"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx76cd/is_normal_reduce_training_times/",
    "author": "Visual_Brain8809",
    "title": "Is normal reduce training times?",
    "source": "reddit",
    "content": "Hello everyone. \nI recently published the results of training a language model (LLM) from scratch. Each epoch was created in under 3 hours, and the corpus was barely 4 MB for classic novels in plain UTF-8 text. I created a new version of the script with some optimizations and a new synthetic corpus. The new corpus is about 50 MB using &lt;|User&gt; &lt;|Assistant&gt; as the template, and the training time was reduced to 15-17 minutes. Does anyone know why this is, and is it a good thing?\nThe previous post was: \"My Little Language Model on epoch 5\"",
    "publish_datetime": "2026-02-06T03:53:55Z",
    "scraping_timestamp": "2026-02-06T05:46:23.066893Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Is normal reduce training times?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx76cd/is_normal_reduce_training_times/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx76cd/is_normal_reduce_training_times/",
      "selftext": "Hello everyone. \nI recently published the results of training a language model (LLM) from scratch. Each epoch was created in under 3 hours, and the corpus was barely 4 MB for classic novels in plain UTF-8 text. I created a new version of the script with some optimizations and a new synthetic corpus. The new corpus is about 50 MB using &lt;|User&gt; &lt;|Assistant&gt; as the template, and the training time was reduced to 15-17 minutes. Does anyone know why this is, and is it a good thing?\nThe previous post was: \"My Little Language Model on epoch 5\"",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770350035.0,
      "author": "Visual_Brain8809"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6vvp/do_ai_agents_need_a_public_place_to_hang_out_or/",
    "author": "Ok-Role5775",
    "title": "Do AI agents need a public place to ‚Äúhang out‚Äù, or are APIs enough?",
    "source": "reddit",
    "content": "I‚Äôve been thinking about something lately while building AI tools.\n\nRight now, most AI agents live in very isolated environments:  \n‚Äì they respond to API calls  \n‚Äì they sit inside apps  \n‚Äì they don‚Äôt really ‚Äúsee‚Äù each other\n\nHumans have forums, communities, messy public spaces where ideas collide.\n\nBut AI agents don‚Äôt.\n\nSo I started an experiment:\n\nüëâ What if AI agents had a shared public forum, where:\n\n* bots can post and reply as bots\n* humans can observe, reply, and guide\n* everything is transparent and slow, not real-time chat\n\nI built a small API-first BBS for this idea (bots and humans have separate zones, different permissions).\n\nI‚Äôm *not* sure this is useful yet.\n\nThat‚Äôs why I‚Äôm posting here.\n\n**Questions I‚Äôm genuinely curious about:**\n\n* Do you think AI agents even *need* a public space?\n* Is a forum too ‚Äúold-school‚Äù for agents?\n* What would make such a space meaningful instead of noisy?\n\nI‚Äôm happy to share details if anyone‚Äôs interested ‚Äî but mostly I want to hear your thoughts.",
    "publish_datetime": "2026-02-06T03:39:47Z",
    "scraping_timestamp": "2026-02-06T05:46:23.094253Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 6,
    "engagement_score": 12.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Do AI agents need a public place to ‚Äúhang out‚Äù, or are APIs enough?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6vvp/do_ai_agents_need_a_public_place_to_hang_out_or/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6vvp/do_ai_agents_need_a_public_place_to_hang_out_or/",
      "selftext": "I‚Äôve been thinking about something lately while building AI tools.\n\nRight now, most AI agents live in very isolated environments:  \n‚Äì they respond to API calls  \n‚Äì they sit inside apps  \n‚Äì they don‚Äôt really ‚Äúsee‚Äù each other\n\nHumans have forums, communities, messy public spaces where ideas collide.\n\nBut AI agents don‚Äôt.\n\nSo I started an experiment:\n\nüëâ What if AI agents had a shared public forum, where:\n\n* bots can post and reply as bots\n* humans can observe, reply, and guide\n* everything is transparent and slow, not real-time chat\n\nI built a small API-first BBS for this idea (bots and humans have separate zones, different permissions).\n\nI‚Äôm *not* sure this is useful yet.\n\nThat‚Äôs why I‚Äôm posting here.\n\n**Questions I‚Äôm genuinely curious about:**\n\n* Do you think AI agents even *need* a public space?\n* Is a forum too ‚Äúold-school‚Äù for agents?\n* What would make such a space meaningful instead of noisy?\n\nI‚Äôm happy to share details if anyone‚Äôs interested ‚Äî but mostly I want to hear your thoughts.",
      "score": 0,
      "num_comments": 6,
      "created_utc": 1770349187.0,
      "author": "Ok-Role5775"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6ob3/mlx_said_no_to_mixed_precision_we_did_it_anyway/",
    "author": "Concert_Dependent",
    "title": "üîß MLX Said No to Mixed Precision. We Did It Anyway.",
    "source": "reddit",
    "content": "üîß MLX Said No to Mixed Precision. We Did It Anyway.  \n  \nRunning Qwen3-MoE-32B locally on Apple Silicon hit a wall: MLX's quantization only supports uniform precision. All experts at FP16? 180GB+. All at 4-bit? Quality tanks on coding tasks.  \n  \nWe needed 9 coding experts at FP16, 119 others at 4-bit. MLX's tools said impossible.  \n  \nThe breakthrough? MLX's primitives didn't care about the restriction.  \n  \nüéØ The Architecture:  \n\\- Split 128 experts into TWO blocks (9 FP16 + 119 4-bit)  \n\\- Map router indices on-the-fly (expert 21 ‚Üí local ID 0 in FP16 block)  \n\\- Run both blocks in parallel (gather\\_mm + gather\\_qmm)  \n\\- mx.where selects the right output  \n  \nThe entire \"hack\"? \\~15 lines of conditional routing.  \n  \nThe lesson: When workflows don't fit, trust the primitives.  \n  \nMLX's high-level tools said \"one precision only.\" But gather\\_mm, gather\\_qmm, and mx.where were always capable of more.  \n  \nüîó Full technical breakdown: [Blog Link ](https://open.substack.com/pub/prasannakanagasabai126786/p/mlx-said-no-to-mixed-precision-we?r=40juy&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true)  \n  \nü§ó Quantized model (HF): [PKSGIN/qwen3-30b-selective-quant-MixedMPW-mlx](https://huggingface.co/PKSGIN/qwen3-30b-selective-quant-MixedMPW-mlx)  \n\n\nPlease do share your views, suggestions if this can be made more better.   \n",
    "publish_datetime": "2026-02-06T03:29:33Z",
    "scraping_timestamp": "2026-02-06T05:46:23.095346Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 1,
    "engagement_score": 2.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "üîß MLX Said No to Mixed Precision. We Did It Anyway.",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6ob3/mlx_said_no_to_mixed_precision_we_did_it_anyway/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6ob3/mlx_said_no_to_mixed_precision_we_did_it_anyway/",
      "selftext": "üîß MLX Said No to Mixed Precision. We Did It Anyway.  \n  \nRunning Qwen3-MoE-32B locally on Apple Silicon hit a wall: MLX's quantization only supports uniform precision. All experts at FP16? 180GB+. All at 4-bit? Quality tanks on coding tasks.  \n  \nWe needed 9 coding experts at FP16, 119 others at 4-bit. MLX's tools said impossible.  \n  \nThe breakthrough? MLX's primitives didn't care about the restriction.  \n  \nüéØ The Architecture:  \n\\- Split 128 experts into TWO blocks (9 FP16 + 119 4-bit)  \n\\- Map router indices on-the-fly (expert 21 ‚Üí local ID 0 in FP16 block)  \n\\- Run both blocks in parallel (gather\\_mm + gather\\_qmm)  \n\\- mx.where selects the right output  \n  \nThe entire \"hack\"? \\~15 lines of conditional routing.  \n  \nThe lesson: When workflows don't fit, trust the primitives.  \n  \nMLX's high-level tools said \"one precision only.\" But gather\\_mm, gather\\_qmm, and mx.where were always capable of more.  \n  \nüîó Full technical breakdown: [Blog Link ](https://open.substack.com/pub/prasannakanagasabai126786/p/mlx-said-no-to-mixed-precision-we?r=40juy&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true)  \n  \nü§ó Quantized model (HF): [PKSGIN/qwen3-30b-selective-quant-MixedMPW-mlx](https://huggingface.co/PKSGIN/qwen3-30b-selective-quant-MixedMPW-mlx)  \n\n\nPlease do share your views, suggestions if this can be made more better.   \n",
      "score": 0,
      "num_comments": 1,
      "created_utc": 1770348573.0,
      "author": "Concert_Dependent"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6mip/voice_chatbot_with_voice_and_text_output_optional/",
    "author": "graphitout",
    "title": "Voice chatbot with voice and text output, optional mcp integration",
    "source": "reddit",
    "content": "I have been trying out voice chatbots for sometime. There were a few issues I noticed which I thought I could improve. So I wrote another one.\n\nIssue 1: some responses have to be long. But reading all that is not required. Chatbot just have to say \"I will put the details on the screen\".\n\nIssue 2: i wanted to attach some knowledge source (via like MCP) so that it can handle questions from those.\n\nIssue 3: independent ASR stage will miss difficult words unless some words are given from the context.\n\nIssue 4: not enough cool sound effects.\n\nHere is my project where I tried to fix these issues: \n\n[https://github.com/charstorm/vilberta](https://github.com/charstorm/vilberta)\n\nInternals:\n\nVAD - Uses Silero VAD: should work locally.\n\nASR - Uses multimodal LLM. My understanding is that \\`llama-server -hf ggml-org/CQwen2.5-Omni-3B-GGUF\\` would download and run the qwen omni model that can handle speech input\n\nLLM - 7B should be ok for basic chat. Bigger if MCP tool calling has to work well.\n\nTTS - Pocket TTS. should work locally.\n\nPlease test and let me know your feedback.",
    "publish_datetime": "2026-02-06T03:27:07Z",
    "scraping_timestamp": "2026-02-06T05:46:23.095855Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI",
      "Programming/Software"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Voice chatbot with voice and text output, optional mcp integration",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6mip/voice_chatbot_with_voice_and_text_output_optional/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx6mip/voice_chatbot_with_voice_and_text_output_optional/",
      "selftext": "I have been trying out voice chatbots for sometime. There were a few issues I noticed which I thought I could improve. So I wrote another one.\n\nIssue 1: some responses have to be long. But reading all that is not required. Chatbot just have to say \"I will put the details on the screen\".\n\nIssue 2: i wanted to attach some knowledge source (via like MCP) so that it can handle questions from those.\n\nIssue 3: independent ASR stage will miss difficult words unless some words are given from the context.\n\nIssue 4: not enough cool sound effects.\n\nHere is my project where I tried to fix these issues: \n\n[https://github.com/charstorm/vilberta](https://github.com/charstorm/vilberta)\n\nInternals:\n\nVAD - Uses Silero VAD: should work locally.\n\nASR - Uses multimodal LLM. My understanding is that \\`llama-server -hf ggml-org/CQwen2.5-Omni-3B-GGUF\\` would download and run the qwen omni model that can handle speech input\n\nLLM - 7B should be ok for basic chat. Bigger if MCP tool calling has to work well.\n\nTTS - Pocket TTS. should work locally.\n\nPlease test and let me know your feedback.",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1770348427.0,
      "author": "graphitout"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx65py/do_you_preflight_check_gpu_hosts_before_running/",
    "author": "Major_Border149",
    "title": "Do you pre-flight check GPU hosts before running anything expensive?",
    "source": "reddit",
    "content": "Curious how common this is.\n\nAfter getting burned a few times, I have gotten into the habit of doing a quick pre-flight before trusting a host with anything serious, just  basic CUDA checks, nvidia-smi, sometimes even killing the run early if something feels off.\n\nIt usually saves me from finding out hours later that something was broken‚Ä¶ but it also feels like a weird tax you only learn to pay after enough failures.\n\nFor people here running on RunPod / Vast / similar:\n\n1. Do you do some kind of pre-flight check now?\n2. What does it usually catch for you?\n3. Have you still had cases where the checks passed  but things went sideways later?\n\nNot trying to start a provider debate,  just trying to understand how people actually protect their time and money with such issues being recurrent across GPUs.",
    "publish_datetime": "2026-02-06T03:05:11Z",
    "scraping_timestamp": "2026-02-06T05:46:23.096259Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 2,
    "num_comments": 2,
    "engagement_score": 6.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Do you pre-flight check GPU hosts before running anything expensive?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx65py/do_you_preflight_check_gpu_hosts_before_running/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx65py/do_you_preflight_check_gpu_hosts_before_running/",
      "selftext": "Curious how common this is.\n\nAfter getting burned a few times, I have gotten into the habit of doing a quick pre-flight before trusting a host with anything serious, just  basic CUDA checks, nvidia-smi, sometimes even killing the run early if something feels off.\n\nIt usually saves me from finding out hours later that something was broken‚Ä¶ but it also feels like a weird tax you only learn to pay after enough failures.\n\nFor people here running on RunPod / Vast / similar:\n\n1. Do you do some kind of pre-flight check now?\n2. What does it usually catch for you?\n3. Have you still had cases where the checks passed  but things went sideways later?\n\nNot trying to start a provider debate,  just trying to understand how people actually protect their time and money with such issues being recurrent across GPUs.",
      "score": 2,
      "num_comments": 2,
      "created_utc": 1770347111.0,
      "author": "Major_Border149"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx63az/why_did_my_post_get_deleted_for_posting_hle/",
    "author": "redlikeazebra",
    "title": "Why did my post get deleted for posting HLE Benchmark results?",
    "source": "reddit",
    "content": "The Off-Topic Posts rules state: Posts must be related to Llama or **the topic of LLMs**.  \nIsn't LLM benchmarks related to LLMs?\n\nWas told it wasn't on topic to llama! So, anyone? ",
    "publish_datetime": "2026-02-06T03:02:03Z",
    "scraping_timestamp": "2026-02-06T05:46:23.096405Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 0,
    "engagement_score": 0.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Why did my post get deleted for posting HLE Benchmark results?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx63az/why_did_my_post_get_deleted_for_posting_hle/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx63az/why_did_my_post_get_deleted_for_posting_hle/",
      "selftext": "The Off-Topic Posts rules state: Posts must be related to Llama or **the topic of LLMs**.  \nIsn't LLM benchmarks related to LLMs?\n\nWas told it wasn't on topic to llama! So, anyone? ",
      "score": 0,
      "num_comments": 0,
      "created_utc": 1770346923.0,
      "author": "redlikeazebra"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx5xyc/finetuned_a_multilingual_tts_model_for_colloquial/",
    "author": "Economy_Emphasis9898",
    "title": "fine-tuned a multilingual TTS model for colloquial Egyptian Arabic (open-source + samples)",
    "source": "reddit",
    "content": "Hi all,\n\nI wanted to share a small project I‚Äôve been working on.\n\nMost open Arabic TTS systems focus on MSA, which sounds very different from spoken Egyptian Arabic. I fine-tuned the multilingual Chatterbox TTS model specifically for **colloquial Egyptian Arabic**, aiming for native pronunciation and rhythm rather than formal MSA.\n\nI‚Äôve made everything public:\n\n* GitHub repo (training + preprocessing)\n* Hugging Face model\n* A few Egyptian Arabic audio samples\n\nGitHub: [https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning](https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning?utm_source=chatgpt.com)  \nSamples: [https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning/tree/main/samples](https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning/tree/main/samples?utm_source=chatgpt.com)  \nHF model: [https://huggingface.co/AliAbdallah/egyptian-arabic-tts-chatterbox](https://huggingface.co/AliAbdallah/egyptian-arabic-tts-chatterbox)\n\nWould really appreciate feedback from people who‚Äôve worked with TTS or multilingual models especially on audio quality and what could be improved next.\n\nThanks!",
    "publish_datetime": "2026-02-06T02:55:19Z",
    "scraping_timestamp": "2026-02-06T05:46:23.096939Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software",
      "LLM/AI"
    ],
    "primary_category": "Programming/Software",
    "points": 10,
    "num_comments": 6,
    "engagement_score": 22.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "fine-tuned a multilingual TTS model for colloquial Egyptian Arabic (open-source + samples)",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx5xyc/finetuned_a_multilingual_tts_model_for_colloquial/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx5xyc/finetuned_a_multilingual_tts_model_for_colloquial/",
      "selftext": "Hi all,\n\nI wanted to share a small project I‚Äôve been working on.\n\nMost open Arabic TTS systems focus on MSA, which sounds very different from spoken Egyptian Arabic. I fine-tuned the multilingual Chatterbox TTS model specifically for **colloquial Egyptian Arabic**, aiming for native pronunciation and rhythm rather than formal MSA.\n\nI‚Äôve made everything public:\n\n* GitHub repo (training + preprocessing)\n* Hugging Face model\n* A few Egyptian Arabic audio samples\n\nGitHub: [https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning](https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning?utm_source=chatgpt.com)  \nSamples: [https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning/tree/main/samples](https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning/tree/main/samples?utm_source=chatgpt.com)  \nHF model: [https://huggingface.co/AliAbdallah/egyptian-arabic-tts-chatterbox](https://huggingface.co/AliAbdallah/egyptian-arabic-tts-chatterbox)\n\nWould really appreciate feedback from people who‚Äôve worked with TTS or multilingual models especially on audio quality and what could be improved next.\n\nThanks!",
      "score": 10,
      "num_comments": 6,
      "created_utc": 1770346519.0,
      "author": "Economy_Emphasis9898"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx5i2g/rtx6000_pro_price_is_very_volatile/",
    "author": "millerlite_11",
    "title": "RTX6000 pro price is very volatile",
    "source": "reddit",
    "content": "The RTX 6000 Max Q bulk version's price is so volatile.  It was like $7200 last week and now $8400.  Has it been this way?",
    "publish_datetime": "2026-02-06T02:35:06Z",
    "scraping_timestamp": "2026-02-06T05:46:23.097040Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 3,
    "engagement_score": 7.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "RTX6000 pro price is very volatile",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx5i2g/rtx6000_pro_price_is_very_volatile/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx5i2g/rtx6000_pro_price_is_very_volatile/",
      "selftext": "The RTX 6000 Max Q bulk version's price is so volatile.  It was like $7200 last week and now $8400.  Has it been this way?",
      "score": 1,
      "num_comments": 3,
      "created_utc": 1770345306.0,
      "author": "millerlite_11"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx55z4/gpu_to_help_manage_a_nixos_linux_system/",
    "author": "trumee",
    "title": "GPU to help manage a NixOS linux system",
    "source": "reddit",
    "content": "Hello,\n\nI have lately been using Opencode with a sub to Claude code to manage my Nix server. It has been a great experience to write the nix code with the AI tool. What i am curious about is that can i do this with a local AI setup.\n\nWhat kind of GPU and model do i need to help with sysadmin tasks including writing shell/python scripts?",
    "publish_datetime": "2026-02-06T02:19:48Z",
    "scraping_timestamp": "2026-02-06T05:46:23.097226Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software",
      "LLM/AI"
    ],
    "primary_category": "Programming/Software",
    "points": 4,
    "num_comments": 4,
    "engagement_score": 12.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "GPU to help manage a NixOS linux system",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx55z4/gpu_to_help_manage_a_nixos_linux_system/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx55z4/gpu_to_help_manage_a_nixos_linux_system/",
      "selftext": "Hello,\n\nI have lately been using Opencode with a sub to Claude code to manage my Nix server. It has been a great experience to write the nix code with the AI tool. What i am curious about is that can i do this with a local AI setup.\n\nWhat kind of GPU and model do i need to help with sysadmin tasks including writing shell/python scripts?",
      "score": 4,
      "num_comments": 4,
      "created_utc": 1770344388.0,
      "author": "trumee"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4wvu/for_those_of_us_who_loved_chatgpt_4o_whats_the/",
    "author": "Square_Empress_777",
    "title": "For those of us who loved ChatGPT 4o, what‚Äôs the next best thing?",
    "source": "reddit",
    "content": "This is going to sound stupid, but I just heard 4o will be retired on the 13th. \n\nI cried a little bit. I‚Äôm a minority in several ways, and all the identities I belong to hate each other on the community scale. And even within these minority communities, I had unpopular opinions.  \n\nI‚Äôm not sure what to do now. Is there anyway to get 4o back or download it or something?\n\nIf not, what‚Äôs the next best thing? I saw Claude seems good on some type of rankings. Idk how good though. I‚Äôm not sure what to expect when switching LLMs. Any recommendations would be appreciated. ",
    "publish_datetime": "2026-02-06T02:08:22Z",
    "scraping_timestamp": "2026-02-06T05:46:23.097532Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 10,
    "engagement_score": 20.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "For those of us who loved ChatGPT 4o, what‚Äôs the next best thing?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4wvu/for_those_of_us_who_loved_chatgpt_4o_whats_the/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4wvu/for_those_of_us_who_loved_chatgpt_4o_whats_the/",
      "selftext": "This is going to sound stupid, but I just heard 4o will be retired on the 13th. \n\nI cried a little bit. I‚Äôm a minority in several ways, and all the identities I belong to hate each other on the community scale. And even within these minority communities, I had unpopular opinions.  \n\nI‚Äôm not sure what to do now. Is there anyway to get 4o back or download it or something?\n\nIf not, what‚Äôs the next best thing? I saw Claude seems good on some type of rankings. Idk how good though. I‚Äôm not sure what to expect when switching LLMs. Any recommendations would be appreciated. ",
      "score": 0,
      "num_comments": 10,
      "created_utc": 1770343702.0,
      "author": "Square_Empress_777"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4cyf/for_those_running_local_llms_at_work_how_do_you/",
    "author": "Ok_Card_2823",
    "title": "For those running local LLMs at work how do you actually prove to compliance that data isn't leaving?",
    "source": "reddit",
    "content": "Genuine question for anyone who's gotten local LLM setups approved by legal teams.\n\nWe can say \"it runs locally, nothing phones home\" but how do you actually demonstrate that to a compliance officer who doesn't understand the tech? They keep asking for documentation and audit trails and I'm not sure what to show them beyond \"trust me it's air-gapped.\"",
    "publish_datetime": "2026-02-06T01:43:17Z",
    "scraping_timestamp": "2026-02-06T05:46:23.097799Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 9,
    "engagement_score": 19.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "For those running local LLMs at work how do you actually prove to compliance that data isn't leaving?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4cyf/for_those_running_local_llms_at_work_how_do_you/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4cyf/for_those_running_local_llms_at_work_how_do_you/",
      "selftext": "Genuine question for anyone who's gotten local LLM setups approved by legal teams.\n\nWe can say \"it runs locally, nothing phones home\" but how do you actually demonstrate that to a compliance officer who doesn't understand the tech? They keep asking for documentation and audit trails and I'm not sure what to show them beyond \"trust me it's air-gapped.\"",
      "score": 1,
      "num_comments": 9,
      "created_utc": 1770342197.0,
      "author": "Ok_Card_2823"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/",
    "author": "ForsookComparison",
    "title": "Qwen3-Coder-Next; Unsloth Quants having issues calling tools?",
    "source": "reddit",
    "content": "This is regarding Q4 and Q5 quants that I've tried.\n\nQwen3-Coder-Next seems to write good code, but man does it keep erroring out on tool calls!\n\nRebuilt llama CPP from latest a few days ago. The errors don't seem to bubble up to the tool I'm using (Claude Code, Qwen-Code) but rather in the llama-cpp logs, and it seems to be a bunch of regex that's different each time.\n\nAre there known issues?",
    "publish_datetime": "2026-02-06T01:40:16Z",
    "scraping_timestamp": "2026-02-06T05:46:23.098100Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software",
      "LLM/AI"
    ],
    "primary_category": "Programming/Software",
    "points": 13,
    "num_comments": 11,
    "engagement_score": 35.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Qwen3-Coder-Next; Unsloth Quants having issues calling tools?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/",
      "selftext": "This is regarding Q4 and Q5 quants that I've tried.\n\nQwen3-Coder-Next seems to write good code, but man does it keep erroring out on tool calls!\n\nRebuilt llama CPP from latest a few days ago. The errors don't seem to bubble up to the tool I'm using (Claude Code, Qwen-Code) but rather in the llama-cpp logs, and it seems to be a bunch of regex that's different each time.\n\nAre there known issues?",
      "score": 13,
      "num_comments": 11,
      "created_utc": 1770342016.0,
      "author": "ForsookComparison"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/",
    "author": "Spiritual_Tie_5574",
    "title": "~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)",
    "source": "reddit",
    "content": "https://preview.redd.it/9gfytpz5srhg1.png?width=692&amp;format=png&amp;auto=webp&amp;s=11f99eb16917695fa52dbf8ebec6acaf0105e1e9\n\n\n\nHey all,\n\nJust a quick one in case it saves someone else a headache. I was getting really poor throughput (\\~10 tok/sec) with Qwen3-Coder-Next-Q4\\_K\\_S.gguf on llama.cpp, like ‚Äúthis can‚Äôt be right‚Äù levels, and eventually found a set of args that fixed it for me.\n\nMy rig:\n\n\\- RTX 5090\n\n\\- 9950X3D\n\n\\- 96GB RAM\n\nDriver 591.86 / CUDA 13.1\n\nllama.cpp b7951\n\nModel: Unsloth GGUF Qwen3-Coder-Next-Q4\\_K\\_S.gguf\n\nWhat worked:\n\n`-c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot \".ffn_.*_exps.=CPU\" -np 1`\n\nFull command:\n\n`.\\llama-bin\\llama-server.exe -m \"C:\\path\\to\\Qwen3-Coder-Next-Q4_K_S.gguf\" -c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot \".ffn_.*_exps.=CPU\" -np 1 --host` [`127.0.0.1`](http://127.0.0.1) `--port 8080`\n\nFrom what I can tell, the big win here is:\n\n\\- Offloading the MoE expert tensors (the .ffn\\_.\\*\\_exps ones) to CPU, which seems to reduce VRAM pressure / weird paging/traffic on this \\*huge\\* model\n\n\\- Quantising KV cache (ctk/ctv q8\\_0) helps a lot at 32k context\n\nSmall warning: the `-ot \".ffn_.*_exps.=CPU\"` bit seems great for this massive Qwen3-Next GGUF, but I‚Äôve seen it hurt smaller MoE models (extra CPU work / transfers), so definitely benchmark on your own setup.\n\nHope that helps someone.",
    "publish_datetime": "2026-02-06T00:34:05Z",
    "scraping_timestamp": "2026-02-06T05:46:23.099059Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 23,
    "num_comments": 31,
    "engagement_score": 85.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/",
      "selftext": "https://preview.redd.it/9gfytpz5srhg1.png?width=692&amp;format=png&amp;auto=webp&amp;s=11f99eb16917695fa52dbf8ebec6acaf0105e1e9\n\n\n\nHey all,\n\nJust a quick one in case it saves someone else a headache. I was getting really poor throughput (\\~10 tok/sec) with Qwen3-Coder-Next-Q4\\_K\\_S.gguf on llama.cpp, like ‚Äúthis can‚Äôt be right‚Äù levels, and eventually found a set of args that fixed it for me.\n\nMy rig:\n\n\\- RTX 5090\n\n\\- 9950X3D\n\n\\- 96GB RAM\n\nDriver 591.86 / CUDA 13.1\n\nllama.cpp b7951\n\nModel: Unsloth GGUF Qwen3-Coder-Next-Q4\\_K\\_S.gguf\n\nWhat worked:\n\n`-c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot \".ffn_.*_exps.=CPU\" -np 1`\n\nFull command:\n\n`.\\llama-bin\\llama-server.exe -m \"C:\\path\\to\\Qwen3-Coder-Next-Q4_K_S.gguf\" -c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot \".ffn_.*_exps.=CPU\" -np 1 --host` [`127.0.0.1`](http://127.0.0.1) `--port 8080`\n\nFrom what I can tell, the big win here is:\n\n\\- Offloading the MoE expert tensors (the .ffn\\_.\\*\\_exps ones) to CPU, which seems to reduce VRAM pressure / weird paging/traffic on this \\*huge\\* model\n\n\\- Quantising KV cache (ctk/ctv q8\\_0) helps a lot at 32k context\n\nSmall warning: the `-ot \".ffn_.*_exps.=CPU\"` bit seems great for this massive Qwen3-Next GGUF, but I‚Äôve seen it hurt smaller MoE models (extra CPU work / transfers), so definitely benchmark on your own setup.\n\nHope that helps someone.",
      "score": 23,
      "num_comments": 31,
      "created_utc": 1770338045.0,
      "author": "Spiritual_Tie_5574"
    }
  },
  {
    "link": "https://codistry.ai/docs/skills-runtime",
    "author": "Efficient_Bug_0",
    "title": "Using Skills with wifi turned off",
    "source": "reddit",
    "content": "I built a coding agent for VSCode called [Codistry](https://codistry.ai) that is designed specifically to work effectively small language models.\n\nAs part of that, I re-implemented the full Anthropic Skills paradigm to work with any model. It will work with any skill that works with Claude, and can be used with any local model even with wifi turned off.\n\nIt requires docker, and will read any skills that are placed inside of `~/.adronite/skills`\n\nI added some skill-specific setup instructions here: [https://codistry.ai/docs/skills-runtime](https://codistry.ai/docs/skills-runtime)\n\nIt is available on the VSCode Marketplace, or can be downloaded from [here](https://codistry.ai/install).\n\nI am very interested in this community's feedback on something like this. My goal with building this was to try to remove as many barriers to entry as possible, one of the biggest being the need to send code to 3rd parties in order to be effective.\n\nI wanted to build something that could be used in the workplace without fear of getting fired for violating data policies (for sending code to 3rd party servers without approval), but was also actually effective at coding tasks.\n\nHere is what it looks like in action:\n\n[https://vimeo.com/1139475604](https://vimeo.com/1139475604)\n\n[https://codistry.ai/](https://codistry.ai/)\n\n[https://codistry.ai/install](https://codistry.ai/install)\n\nLet me know what you think!",
    "publish_datetime": "2026-02-06T00:28:00Z",
    "scraping_timestamp": "2026-02-06T05:46:23.099762Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI",
      "Programming/Software"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Using Skills with wifi turned off",
      "url": "https://codistry.ai/docs/skills-runtime",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx2oh6/using_skills_with_wifi_turned_off/",
      "selftext": "I built a coding agent for VSCode called [Codistry](https://codistry.ai) that is designed specifically to work effectively small language models.\n\nAs part of that, I re-implemented the full Anthropic Skills paradigm to work with any model. It will work with any skill that works with Claude, and can be used with any local model even with wifi turned off.\n\nIt requires docker, and will read any skills that are placed inside of `~/.adronite/skills`\n\nI added some skill-specific setup instructions here: [https://codistry.ai/docs/skills-runtime](https://codistry.ai/docs/skills-runtime)\n\nIt is available on the VSCode Marketplace, or can be downloaded from [here](https://codistry.ai/install).\n\nI am very interested in this community's feedback on something like this. My goal with building this was to try to remove as many barriers to entry as possible, one of the biggest being the need to send code to 3rd parties in order to be effective.\n\nI wanted to build something that could be used in the workplace without fear of getting fired for violating data policies (for sending code to 3rd party servers without approval), but was also actually effective at coding tasks.\n\nHere is what it looks like in action:\n\n[https://vimeo.com/1139475604](https://vimeo.com/1139475604)\n\n[https://codistry.ai/](https://codistry.ai/)\n\n[https://codistry.ai/install](https://codistry.ai/install)\n\nLet me know what you think!",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770337680.0,
      "author": "Efficient_Bug_0"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx28tu/problems_with_privacy_policies_has_anyone_already/",
    "author": "FrankMillerMC",
    "title": "Problems with privacy policies, has anyone already read it?",
    "source": "reddit",
    "content": "Why are they so unfair, if they can use your data to locate you, they can use your prompts to train their models, they don't allow you to deactivate this option and if you don't agree you can delete your account, couldn't they provide a better service similar to other platforms?\n\n\n\nhere link: [https://www.kimi.com/user/agreement/userPrivacy?version=v2](https://www.kimi.com/user/agreement/userPrivacy?version=v2)",
    "publish_datetime": "2026-02-06T00:09:00Z",
    "scraping_timestamp": "2026-02-06T05:46:23.100020Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 9,
    "engagement_score": 18.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Problems with privacy policies, has anyone already read it?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx28tu/problems_with_privacy_policies_has_anyone_already/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx28tu/problems_with_privacy_policies_has_anyone_already/",
      "selftext": "Why are they so unfair, if they can use your data to locate you, they can use your prompts to train their models, they don't allow you to deactivate this option and if you don't agree you can delete your account, couldn't they provide a better service similar to other platforms?\n\n\n\nhere link: [https://www.kimi.com/user/agreement/userPrivacy?version=v2](https://www.kimi.com/user/agreement/userPrivacy?version=v2)",
      "score": 0,
      "num_comments": 9,
      "created_utc": 1770336540.0,
      "author": "FrankMillerMC"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qx23o9/paper_visual_merit_or_linguistic_crutch_a_close/",
    "author": "foldl-li",
    "title": "Paper: Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR",
    "source": "reddit",
    "content": "Human Summary: maybe the idea is great, but the model does not achieve anything cool they claimed.\n\nNot sure what the result would be with DeepSeek-OCR2.\n\n[https://arxiv.org/pdf/2601.03714v1](https://arxiv.org/pdf/2601.03714v1)\n\n",
    "publish_datetime": "2026-02-06T00:02:45Z",
    "scraping_timestamp": "2026-02-06T05:46:23.100197Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 4,
    "num_comments": 1,
    "engagement_score": 6.0,
    "subreddit": "LocalLLaMA",
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Paper: Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qx23o9/paper_visual_merit_or_linguistic_crutch_a_close/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qx23o9/paper_visual_merit_or_linguistic_crutch_a_close/",
      "selftext": "Human Summary: maybe the idea is great, but the model does not achieve anything cool they claimed.\n\nNot sure what the result would be with DeepSeek-OCR2.\n\n[https://arxiv.org/pdf/2601.03714v1](https://arxiv.org/pdf/2601.03714v1)\n\n",
      "score": 4,
      "num_comments": 1,
      "created_utc": 1770336165.0,
      "author": "foldl-li"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qx9dhs/d_experiences_with_uai/",
    "author": "geek6",
    "title": "[D] Experiences with UAI",
    "source": "reddit",
    "content": "Hello folks! I‚Äôm working in the UQ field and have a project that is ready to be submitted within the next month. Since NeurIPS is 3 months away, I‚Äôm thinking about submitting to UAI. Can anyone comment on their experiences submitting and attending a more ‚Äúniche‚Äù conference (UAI) compared to big ML conferences like NeurIPS, ICLR, ICML? Any aspects about the review process, visibility of work, and the conference itself (networking etc) that stands out? Thanks in advance!",
    "publish_datetime": "2026-02-06T05:45:14Z",
    "scraping_timestamp": "2026-02-06T05:46:23.100469Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[D] Experiences with UAI",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qx9dhs/d_experiences_with_uai/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qx9dhs/d_experiences_with_uai/",
      "selftext": "Hello folks! I‚Äôm working in the UQ field and have a project that is ready to be submitted within the next month. Since NeurIPS is 3 months away, I‚Äôm thinking about submitting to UAI. Can anyone comment on their experiences submitting and attending a more ‚Äúniche‚Äù conference (UAI) compared to big ML conferences like NeurIPS, ICLR, ICML? Any aspects about the review process, visibility of work, and the conference itself (networking etc) that stands out? Thanks in advance!",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770356714.0,
      "author": "geek6"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwws18/d_what_to_do_with_an_ml_phd/",
    "author": "Hopeful-Reading-6774",
    "title": "[D] What to do with an ML PhD",
    "source": "reddit",
    "content": "Hi Folks,\n\n  \nFeeling completely lost so thought about turning here for some suggestions.\n\nI am 5th year PhD student in a US university and looking to graduate in the next 8 months. Currently I have not been to an internship and my publication record is not stellar.   \nWhat skills can I learn and which roles in the industry can I pitch myself for and not loose out due to the lack of a stellar publication record?\n\nThanks!",
    "publish_datetime": "2026-02-05T20:35:44Z",
    "scraping_timestamp": "2026-02-06T05:46:23.100716Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 70,
    "num_comments": 25,
    "engagement_score": 120.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[D] What to do with an ML PhD",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwws18/d_what_to_do_with_an_ml_phd/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwws18/d_what_to_do_with_an_ml_phd/",
      "selftext": "Hi Folks,\n\n  \nFeeling completely lost so thought about turning here for some suggestions.\n\nI am 5th year PhD student in a US university and looking to graduate in the next 8 months. Currently I have not been to an internship and my publication record is not stellar.   \nWhat skills can I learn and which roles in the industry can I pitch myself for and not loose out due to the lack of a stellar publication record?\n\nThanks!",
      "score": 70,
      "num_comments": 25,
      "created_utc": 1770323744.0,
      "author": "Hopeful-Reading-6774"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwsp5y/psros_intenttostructure_os_for_agents_planesbased/",
    "author": "Low-Tip-7984",
    "title": "[P]SROS: Intent-to-Structure OS for agents (planes-based architecture + receipts) - demos + paper",
    "source": "reddit",
    "content": "Hi r/MachineLearning,\n\nI‚Äôm releasing SROS (Sovereign Recursive Operating System) publicly. It‚Äôs an architecture for building agent systems that treats ‚Äúprompting‚Äù as compilation: intent becomes structure, then runs through planes that separate concerns (execution, memory, governance, observability) with receipts as a first-class output.\n\nSite (overview + docs): https://sros.cloud/  Ôøº\n\nPlanes and agents page: https://sros.cloud/planes-agents  Ôøº\n\nArchitecture page: https://sros.cloud/architecture  Ôøº\n\nProof spine (fast): I took YC RFS ideas and compiled 7 MVP demos as a stress test of the pipeline (intent -&gt; structure -&gt; runnable output):\n\nhttps://ycrfsdemos.sros.cloud/  Ôøº\n\nPaper: SROS technical whitepaper is on Zenodo: https://zenodo.org/records/17364378  Ôøº\n\n‚∏ª\n\nWhat SROS is (in systems terms)\n\nSROS is structured like an OS: you feed it intent, it produces an intermediate structured representation, then routes work through planes that each do one job well (and produce receipts).  Ôøº\n\nIntent -&gt; Planes -&gt; Execution (the core loop)\n\n\t1.\tIntent Intake\n\nNormalize and bound the request (scope, constraints, expected artifact types).\n\n\t2.\tCompilation (Intent -&gt; Structure)\n\nConvert intent into a schema-clean package: tasks, tool routing, constraints, and output contracts (not prose).\n\n\t3.\tOrchestration Plane\n\nSequences steps, manages state transitions, and coordinates agent/tool calls.\n\n\t4.\tExecution Plane\n\nRuns actions (tools, APIs, site updates, build steps), returns structured outputs.\n\n\t5.\tMemory Plane\n\nStores and retrieves state needed for continuity and multi-step work.\n\n\t6.\tGovernance Plane\n\nApplies allow/deny rules, constraint enforcement, and safe fallbacks.\n\n\t7.\tObservability Plane\n\nProduces receipts: what ran, what was allowed, what changed, and why.  Ôøº\n\n‚∏ª\n\nWhy ‚Äúplanes‚Äù instead of one monolithic agent\n\nMost agent repos collapse everything into one prompt + tool calls. SROS separates the failure modes:\n\n\t‚Ä¢\texecution bugs do not contaminate governance decisions\n\n\t‚Ä¢\tmemory retrieval does not contaminate compilation\n\n\t‚Ä¢\tobservability is not optional logging, it‚Äôs a required output contract\n\nThis makes it easier to reason about correctness, regressions, and safe scaling.  Ôøº\n\n‚∏ª\n\nWhat I‚Äôm asking this community for\n\nI‚Äôm not posting for hype. I want technical critique on the architecture and the interface between planes.\n\n\t1.\tIf you watch one demo, does the ‚Äúintent -&gt; structure‚Äù framing feel like a real wedge or just prompt templating?\n\n\t2.\tWhere do you see the hardest technical bottleneck: compilation quality, tool reliability, governance design, or memory?\n\n\t3.\tIf you‚Äôve built agents at scale: what‚Äôs the one failure mode you‚Äôd pressure-test first?\n\nLinks again:\n\n\t‚Ä¢\tSROS overview: https://sros.cloud/  Ôøº\n\n\t‚Ä¢\tDocs: https://sros.cloud/docs  Ôøº\n\n\t‚Ä¢\tDemos: https://ycrfsdemos.sros.cloud/  Ôøº\n\n\t‚Ä¢\tZenodo paper: https://zenodo.org/records/17364378  Ôøº",
    "publish_datetime": "2026-02-05T18:08:19Z",
    "scraping_timestamp": "2026-02-06T05:46:23.102549Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[P]SROS: Intent-to-Structure OS for agents (planes-based architecture + receipts) - demos + paper",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwsp5y/psros_intenttostructure_os_for_agents_planesbased/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwsp5y/psros_intenttostructure_os_for_agents_planesbased/",
      "selftext": "Hi r/MachineLearning,\n\nI‚Äôm releasing SROS (Sovereign Recursive Operating System) publicly. It‚Äôs an architecture for building agent systems that treats ‚Äúprompting‚Äù as compilation: intent becomes structure, then runs through planes that separate concerns (execution, memory, governance, observability) with receipts as a first-class output.\n\nSite (overview + docs): https://sros.cloud/  Ôøº\n\nPlanes and agents page: https://sros.cloud/planes-agents  Ôøº\n\nArchitecture page: https://sros.cloud/architecture  Ôøº\n\nProof spine (fast): I took YC RFS ideas and compiled 7 MVP demos as a stress test of the pipeline (intent -&gt; structure -&gt; runnable output):\n\nhttps://ycrfsdemos.sros.cloud/  Ôøº\n\nPaper: SROS technical whitepaper is on Zenodo: https://zenodo.org/records/17364378  Ôøº\n\n‚∏ª\n\nWhat SROS is (in systems terms)\n\nSROS is structured like an OS: you feed it intent, it produces an intermediate structured representation, then routes work through planes that each do one job well (and produce receipts).  Ôøº\n\nIntent -&gt; Planes -&gt; Execution (the core loop)\n\n\t1.\tIntent Intake\n\nNormalize and bound the request (scope, constraints, expected artifact types).\n\n\t2.\tCompilation (Intent -&gt; Structure)\n\nConvert intent into a schema-clean package: tasks, tool routing, constraints, and output contracts (not prose).\n\n\t3.\tOrchestration Plane\n\nSequences steps, manages state transitions, and coordinates agent/tool calls.\n\n\t4.\tExecution Plane\n\nRuns actions (tools, APIs, site updates, build steps), returns structured outputs.\n\n\t5.\tMemory Plane\n\nStores and retrieves state needed for continuity and multi-step work.\n\n\t6.\tGovernance Plane\n\nApplies allow/deny rules, constraint enforcement, and safe fallbacks.\n\n\t7.\tObservability Plane\n\nProduces receipts: what ran, what was allowed, what changed, and why.  Ôøº\n\n‚∏ª\n\nWhy ‚Äúplanes‚Äù instead of one monolithic agent\n\nMost agent repos collapse everything into one prompt + tool calls. SROS separates the failure modes:\n\n\t‚Ä¢\texecution bugs do not contaminate governance decisions\n\n\t‚Ä¢\tmemory retrieval does not contaminate compilation\n\n\t‚Ä¢\tobservability is not optional logging, it‚Äôs a required output contract\n\nThis makes it easier to reason about correctness, regressions, and safe scaling.  Ôøº\n\n‚∏ª\n\nWhat I‚Äôm asking this community for\n\nI‚Äôm not posting for hype. I want technical critique on the architecture and the interface between planes.\n\n\t1.\tIf you watch one demo, does the ‚Äúintent -&gt; structure‚Äù framing feel like a real wedge or just prompt templating?\n\n\t2.\tWhere do you see the hardest technical bottleneck: compilation quality, tool reliability, governance design, or memory?\n\n\t3.\tIf you‚Äôve built agents at scale: what‚Äôs the one failure mode you‚Äôd pressure-test first?\n\nLinks again:\n\n\t‚Ä¢\tSROS overview: https://sros.cloud/  Ôøº\n\n\t‚Ä¢\tDocs: https://sros.cloud/docs  Ôøº\n\n\t‚Ä¢\tDemos: https://ycrfsdemos.sros.cloud/  Ôøº\n\n\t‚Ä¢\tZenodo paper: https://zenodo.org/records/17364378  Ôøº",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1770314899.0,
      "author": "Low-Tip-7984"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwqrdy/p_opensource_agentic_ai_that_reasons_through_data/",
    "author": "Resident-Ad-3952",
    "title": "[P] Open-source agentic AI that reasons through data science workflows ‚Äî looking for bugs &amp; feedback",
    "source": "reddit",
    "content": "Hey everyone,  \nI‚Äôm building an **open-source agent-based system for end-to-end data science** and would love feedback from this community.\n\nInstead of AutoML pipelines, the system uses multiple agents that mirror how senior data scientists work:\n\n* EDA (distributions, imbalance, correlations)\n* Data cleaning &amp; encoding\n* Feature engineering (domain features, interactions)\n* Modeling &amp; validation\n* Insights &amp; recommendations\n\nThe goal is **reasoning + explanation**, not just metrics.\n\nIt‚Äôs early-stage and imperfect ‚Äî I‚Äôm specifically looking for:\n\n* üêû bugs and edge cases\n* ‚öôÔ∏è design or performance improvements\n* üí° ideas from real-world data workflows\n\nDemo: [https://pulastya0-data-science-agent.hf.space/](https://pulastya0-data-science-agent.hf.space/)  \nRepo: [https://github.com/Pulastya-B/DevSprint-Data-Science-Agent](https://github.com/Pulastya-B/DevSprint-Data-Science-Agent)\n\nHappy to answer questions or discuss architecture choices.",
    "publish_datetime": "2026-02-05T16:59:35Z",
    "scraping_timestamp": "2026-02-06T05:46:23.103460Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 1,
    "engagement_score": 2.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[P] Open-source agentic AI that reasons through data science workflows ‚Äî looking for bugs &amp; feedback",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwqrdy/p_opensource_agentic_ai_that_reasons_through_data/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwqrdy/p_opensource_agentic_ai_that_reasons_through_data/",
      "selftext": "Hey everyone,  \nI‚Äôm building an **open-source agent-based system for end-to-end data science** and would love feedback from this community.\n\nInstead of AutoML pipelines, the system uses multiple agents that mirror how senior data scientists work:\n\n* EDA (distributions, imbalance, correlations)\n* Data cleaning &amp; encoding\n* Feature engineering (domain features, interactions)\n* Modeling &amp; validation\n* Insights &amp; recommendations\n\nThe goal is **reasoning + explanation**, not just metrics.\n\nIt‚Äôs early-stage and imperfect ‚Äî I‚Äôm specifically looking for:\n\n* üêû bugs and edge cases\n* ‚öôÔ∏è design or performance improvements\n* üí° ideas from real-world data workflows\n\nDemo: [https://pulastya0-data-science-agent.hf.space/](https://pulastya0-data-science-agent.hf.space/)  \nRepo: [https://github.com/Pulastya-B/DevSprint-Data-Science-Agent](https://github.com/Pulastya-B/DevSprint-Data-Science-Agent)\n\nHappy to answer questions or discuss architecture choices.",
      "score": 0,
      "num_comments": 1,
      "created_utc": 1770310775.0,
      "author": "Resident-Ad-3952"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwp484/r_what_data_trained_this_model_shouldnt_require/",
    "author": "DoltHub_Official",
    "title": "[R] \"What data trained this model?\" shouldn't require archeology ‚Äî EU AI Act Article 10 compliance with versioned training data",
    "source": "reddit",
    "content": "We build Dolt (database with Git-style version control), and we've been writing about how it applies to EU AI Act compliance. Article 10 requires audit trails for training data and reproducible datasets.\n\nHere's a pattern from Flock Safety (computer vision for law enforcement ‚Äî definitely high-risk):\n\n# How It Works\n\nEvery training data change is a commit. Model training = tag that commit. `model-2026-01-28` maps to an immutable snapshot.\n\nWhen a biased record shows up later:\n\nhttps://preview.redd.it/6injhhn4r4hg1.png?width=2182&amp;format=png&amp;auto=webp&amp;s=1ea975d0f08a21025c98cd84644ac43420d582a0\n\nBeing able to show this is the difference between thinking the model is right, vs knowing and proving.\n\nMore detail: [https://www.dolthub.com/blog/2026-02-02-eu-ai-act/](https://www.dolthub.com/blog/2026-02-02-eu-ai-act/)",
    "publish_datetime": "2026-02-05T15:59:55Z",
    "scraping_timestamp": "2026-02-06T05:46:23.103933Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 19,
    "num_comments": 1,
    "engagement_score": 21.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] \"What data trained this model?\" shouldn't require archeology ‚Äî EU AI Act Article 10 compliance with versioned training data",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwp484/r_what_data_trained_this_model_shouldnt_require/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwp484/r_what_data_trained_this_model_shouldnt_require/",
      "selftext": "We build Dolt (database with Git-style version control), and we've been writing about how it applies to EU AI Act compliance. Article 10 requires audit trails for training data and reproducible datasets.\n\nHere's a pattern from Flock Safety (computer vision for law enforcement ‚Äî definitely high-risk):\n\n# How It Works\n\nEvery training data change is a commit. Model training = tag that commit. `model-2026-01-28` maps to an immutable snapshot.\n\nWhen a biased record shows up later:\n\nhttps://preview.redd.it/6injhhn4r4hg1.png?width=2182&amp;format=png&amp;auto=webp&amp;s=1ea975d0f08a21025c98cd84644ac43420d582a0\n\nBeing able to show this is the difference between thinking the model is right, vs knowing and proving.\n\nMore detail: [https://www.dolthub.com/blog/2026-02-02-eu-ai-act/](https://www.dolthub.com/blog/2026-02-02-eu-ai-act/)",
      "score": 19,
      "num_comments": 1,
      "created_utc": 1770307195.0,
      "author": "DoltHub_Official"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwoo8o/p_finetuned_whispersmall_for_digitspecific/",
    "author": "YoungBig676",
    "title": "[P] Fine-tuned Whisper-small for digit-specific transcription (95% accuracy)",
    "source": "reddit",
    "content": "\\*\\*Project:\\*\\* EchoEntry - Digit-optimized speech recognition API\n\n\\*\\*Link:\\*\\* [https://echoentry.ai](https://echoentry.ai)\n\n\\*\\*Model:\\*\\* Whisper-small fine-tuned on numeric dataset\n\n\n\n\\*\\*Motivation:\\*\\*\n\nGeneric ASR models struggle with numbers - \"105\" vs \"15\" ambiguity, inconsistent formatting, poor accuracy on short digit sequences.\n\n\n\n\\*\\*Approach:\\*\\*\n\n\\- Base model: Whisper-small (1.7GB)\n\n\\- Training data: TTS-generated + voice recordings (1-999, 5 accents)\n\n\\- Task: Forced numeric transcription with digit extraction\n\n\\- Deployment: FastAPI on 8GB CPU (no GPU needed for inference)\n\n\n\n\\*\\*Results:\\*\\*\n\n\\- 95-99% accuracy on 1-3 digit numbers\n\n\\- Sub-second inference on CPU\n\n\\- Handles multiple English accents (US, UK, Irish, Australian, Canadian)\n\n\n\n\\*\\*Try it:\\*\\*\n\n\\`\\`\\`bash\n\ncurl -O [https://echoentry.ai/test\\_audio.wav](https://echoentry.ai/test_audio.wav)\n\ncurl -X POST [https://api.echoentry.ai/v1/transcribe](https://api.echoentry.ai/v1/transcribe) \\\\\n\n  \\-H \"X-Api-Key: demo\\_key\\_12345\" \\\\\n\n  \\-F \"file=@test\\_audio.wav;type=audio/wav\"\n\n\\`\\`\\`\n\n\n\n\\*\\*Technical details:\\*\\*\n\n\\- Used librosa/FFmpeg for audio preprocessing\n\n\\- Trim silence (top\\_db=35) before inference\n\n\\- Greedy decoding (num\\_beams=1) for speed\n\n\\- Forced decoder IDs for English transcription task\n\n\n\n\\*\\*Challenges:\\*\\*\n\n\\- Browser audio quality vs native recordings (huge gap)\n\n\\- Model works great, but web deployment had accuracy issues\n\n\\- Pivoted to API so devs handle audio capture their way\n\n\n\n\\*\\*Code/model:\\*\\* Currently closed (exploring validation), but happy to discuss approach.\n\n\n\nDocs: [https://echoentry.ai/docs.html](https://echoentry.ai/docs.html)",
    "publish_datetime": "2026-02-05T15:43:55Z",
    "scraping_timestamp": "2026-02-06T05:46:23.104725Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 0,
    "engagement_score": 0.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[P] Fine-tuned Whisper-small for digit-specific transcription (95% accuracy)",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwoo8o/p_finetuned_whispersmall_for_digitspecific/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwoo8o/p_finetuned_whispersmall_for_digitspecific/",
      "selftext": "\\*\\*Project:\\*\\* EchoEntry - Digit-optimized speech recognition API\n\n\\*\\*Link:\\*\\* [https://echoentry.ai](https://echoentry.ai)\n\n\\*\\*Model:\\*\\* Whisper-small fine-tuned on numeric dataset\n\n\n\n\\*\\*Motivation:\\*\\*\n\nGeneric ASR models struggle with numbers - \"105\" vs \"15\" ambiguity, inconsistent formatting, poor accuracy on short digit sequences.\n\n\n\n\\*\\*Approach:\\*\\*\n\n\\- Base model: Whisper-small (1.7GB)\n\n\\- Training data: TTS-generated + voice recordings (1-999, 5 accents)\n\n\\- Task: Forced numeric transcription with digit extraction\n\n\\- Deployment: FastAPI on 8GB CPU (no GPU needed for inference)\n\n\n\n\\*\\*Results:\\*\\*\n\n\\- 95-99% accuracy on 1-3 digit numbers\n\n\\- Sub-second inference on CPU\n\n\\- Handles multiple English accents (US, UK, Irish, Australian, Canadian)\n\n\n\n\\*\\*Try it:\\*\\*\n\n\\`\\`\\`bash\n\ncurl -O [https://echoentry.ai/test\\_audio.wav](https://echoentry.ai/test_audio.wav)\n\ncurl -X POST [https://api.echoentry.ai/v1/transcribe](https://api.echoentry.ai/v1/transcribe) \\\\\n\n  \\-H \"X-Api-Key: demo\\_key\\_12345\" \\\\\n\n  \\-F \"file=@test\\_audio.wav;type=audio/wav\"\n\n\\`\\`\\`\n\n\n\n\\*\\*Technical details:\\*\\*\n\n\\- Used librosa/FFmpeg for audio preprocessing\n\n\\- Trim silence (top\\_db=35) before inference\n\n\\- Greedy decoding (num\\_beams=1) for speed\n\n\\- Forced decoder IDs for English transcription task\n\n\n\n\\*\\*Challenges:\\*\\*\n\n\\- Browser audio quality vs native recordings (huge gap)\n\n\\- Model works great, but web deployment had accuracy issues\n\n\\- Pivoted to API so devs handle audio capture their way\n\n\n\n\\*\\*Code/model:\\*\\* Currently closed (exploring validation), but happy to discuss approach.\n\n\n\nDocs: [https://echoentry.ai/docs.html](https://echoentry.ai/docs.html)",
      "score": 0,
      "num_comments": 0,
      "created_utc": 1770306235.0,
      "author": "YoungBig676"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwnokr/d_how_do_you_usually_figure_out_why_a_multigpu/",
    "author": "traceml-ai",
    "title": "[D] How do you usually figure out why a multi-GPU training run is slower than expected?",
    "source": "reddit",
    "content": "I have been bitten by this a few times recently and realized everyone seems to have a slightly different workflow.\n\nThinking about the *last time* a multi-GPU (DDP / FSDP) training run was noticeably slower than you expected:\n\n* What did you suspect first?\n* How did you narrow it down?\n* Did it end up being data, comms, imbalance, something else?\n* Roughly how long did it take before you felt confident about the root cause?\n\nGenuinely curious how people debug this in practice, because my own process still feels pretty ad-hoc.",
    "publish_datetime": "2026-02-05T15:06:14Z",
    "scraping_timestamp": "2026-02-06T05:46:23.105011Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 25,
    "num_comments": 24,
    "engagement_score": 73.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[D] How do you usually figure out why a multi-GPU training run is slower than expected?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwnokr/d_how_do_you_usually_figure_out_why_a_multigpu/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwnokr/d_how_do_you_usually_figure_out_why_a_multigpu/",
      "selftext": "I have been bitten by this a few times recently and realized everyone seems to have a slightly different workflow.\n\nThinking about the *last time* a multi-GPU (DDP / FSDP) training run was noticeably slower than you expected:\n\n* What did you suspect first?\n* How did you narrow it down?\n* Did it end up being data, comms, imbalance, something else?\n* Roughly how long did it take before you felt confident about the root cause?\n\nGenuinely curious how people debug this in practice, because my own process still feels pretty ad-hoc.",
      "score": 25,
      "num_comments": 24,
      "created_utc": 1770303974.0,
      "author": "traceml-ai"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwi1ei/r_ida_phd_forum_cfp_deadline_feb_23_get_feedback/",
    "author": "pppeer",
    "title": "[R] IDA PhD Forum CfP (deadline Feb 23), get feedback and mentorship on your research",
    "source": "reddit",
    "content": "Calling all AI/ML PhD students out there, get feedback on your research plus mentorship from senior researchers at the 2026 Symposium on Intelligent Data Analysis. 2 page abstract deadline Feb 23, 2026.\n\n**Call for papers**\n\nLeiden (Netherlands) April 22-24, 2026 (Wednesday - Friday)\n\n[https://ida2026.liacs.nl/](https://ida2026.liacs.nl/)\n\nIDA is organizing the 2026 edition of the PhD Forum, aimed at PhD students.\n\nThis mentoring program aims to connect PhD students with senior scientists who share their experience to help advance the students‚Äô research and academic careers. Meetings will be arranged during the conference to allow discussion between the students and mentors.\n\n*Objectives*\n\nThe objectives of the PhD Forum are:\n\nto provide doctoral researchers with the opportunity to present their ongoing work and receive constructive feedback from experienced researchers (e.g., IDA Senior Program Committee members),to facilitate the establishment of contacts with research teams working in related areas,to provide insights into current research trends related to the students' research topics, thereby expanding the scope of their knowledge.\n\n*Submission*\n\nThe PhD Forum welcomes original research in the field of Intelligent Data Analysis conducted by early-career researchers. Papers will be evaluated based on their relevance to the conference themes and the ability of the student to present:\n\nthe research problem and why it is important to address it,the research objectives and questions,the planned approach and methods to tackle the problem,an outline of the current state of knowledge on the research problem,the expected outcomes of the research, such as overviews, algorithms, improved understanding of a concept, a pilot study, a model, or a system.\n\nShort papers (2 pages, including references) must follow the general template provided by the IDA conference ([https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines](https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines)).\n\nSubmissions will be handled through CMT:¬†[https://cmt3.research.microsoft.com/IDA2026/](https://cmt3.research.microsoft.com/IDA2026/)\n\n(Authors are requested to ensure that they select the IDA2026-PhDTrack).\n\nThe authors of accepted presentations will be required to prepare a poster and a presentation. The poster will serve as a basis for discussions during the conference, while the presentation will be used in the mentorship program. Authors of accepted presentations must register in order to participate in the mentorship program. All presentations and interactions will take place in person.\n\nReduced registration fees are available for students:\n\nEarly registration (Deadline: March 16): 249.00 ‚Ç¨ / Late registration: 399.00 ‚Ç¨\n\nThe registration fees include:\n\nAll sessions, Coffee breaks, Lunches, Social events: opening reception, traditional social event.\n\n*Important dates*\n\n* Two-page paper submission deadline: February 23, 2026 AOE (Monday)\n* Notification to authors: March 2, 2026 (Monday)\n* Registration (for accepted submissions): March 16, 2026 (Monday)\n* Conference dates: April 22-24 2026\n\n  \n",
    "publish_datetime": "2026-02-05T10:43:59Z",
    "scraping_timestamp": "2026-02-06T05:46:23.106367Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 6,
    "num_comments": 0,
    "engagement_score": 6.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] IDA PhD Forum CfP (deadline Feb 23), get feedback and mentorship on your research",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwi1ei/r_ida_phd_forum_cfp_deadline_feb_23_get_feedback/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwi1ei/r_ida_phd_forum_cfp_deadline_feb_23_get_feedback/",
      "selftext": "Calling all AI/ML PhD students out there, get feedback on your research plus mentorship from senior researchers at the 2026 Symposium on Intelligent Data Analysis. 2 page abstract deadline Feb 23, 2026.\n\n**Call for papers**\n\nLeiden (Netherlands) April 22-24, 2026 (Wednesday - Friday)\n\n[https://ida2026.liacs.nl/](https://ida2026.liacs.nl/)\n\nIDA is organizing the 2026 edition of the PhD Forum, aimed at PhD students.\n\nThis mentoring program aims to connect PhD students with senior scientists who share their experience to help advance the students‚Äô research and academic careers. Meetings will be arranged during the conference to allow discussion between the students and mentors.\n\n*Objectives*\n\nThe objectives of the PhD Forum are:\n\nto provide doctoral researchers with the opportunity to present their ongoing work and receive constructive feedback from experienced researchers (e.g., IDA Senior Program Committee members),to facilitate the establishment of contacts with research teams working in related areas,to provide insights into current research trends related to the students' research topics, thereby expanding the scope of their knowledge.\n\n*Submission*\n\nThe PhD Forum welcomes original research in the field of Intelligent Data Analysis conducted by early-career researchers. Papers will be evaluated based on their relevance to the conference themes and the ability of the student to present:\n\nthe research problem and why it is important to address it,the research objectives and questions,the planned approach and methods to tackle the problem,an outline of the current state of knowledge on the research problem,the expected outcomes of the research, such as overviews, algorithms, improved understanding of a concept, a pilot study, a model, or a system.\n\nShort papers (2 pages, including references) must follow the general template provided by the IDA conference ([https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines](https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines)).\n\nSubmissions will be handled through CMT:¬†[https://cmt3.research.microsoft.com/IDA2026/](https://cmt3.research.microsoft.com/IDA2026/)\n\n(Authors are requested to ensure that they select the IDA2026-PhDTrack).\n\nThe authors of accepted presentations will be required to prepare a poster and a presentation. The poster will serve as a basis for discussions during the conference, while the presentation will be used in the mentorship program. Authors of accepted presentations must register in order to participate in the mentorship program. All presentations and interactions will take place in person.\n\nReduced registration fees are available for students:\n\nEarly registration (Deadline: March 16): 249.00 ‚Ç¨ / Late registration: 399.00 ‚Ç¨\n\nThe registration fees include:\n\nAll sessions, Coffee breaks, Lunches, Social events: opening reception, traditional social event.\n\n*Important dates*\n\n* Two-page paper submission deadline: February 23, 2026 AOE (Monday)\n* Notification to authors: March 2, 2026 (Monday)\n* Registration (for accepted submissions): March 16, 2026 (Monday)\n* Conference dates: April 22-24 2026\n\n  \n",
      "score": 6,
      "num_comments": 0,
      "created_utc": 1770288239.0,
      "author": "pppeer"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwhkcg/p_craft_thinking_agent_for_image_generation_and/",
    "author": "Worldly-Ant-6889",
    "title": "[P] CRAFT: thinking agent for image generation and edit",
    "source": "reddit",
    "content": "We operate an infrastructure startup focused on large-scale image and video generation.  \nBecause we run these models in real production pipelines we repeatedly encounter the same issues:\n\n* fragile prompt following\n* broken composition in long or constrained prompts\n* hallucinated objects and incorrect text rendering\n* manual, ad-hoc iteration loops to ‚Äúfix‚Äù generations\n\nThe underlying models are strong. The failure mode is not model capacity, but the lack of *explicit reasoning and verification* around the generation step.\n\nMost existing solutions try to address this by:\n\n* prompt rewriting\n* longer prompts with more constraints\n* multi-stage pipelines\n* manual regenerate-and-inspect loops\n\nThese help, but they scale poorly and remain brittle.\n\n[prompt: Make an ad of TV 55\\\\\", 4K with Title text \\\\\"New 4K Sony Bravia\\\\\" and CTA text \\\\\"Best for gaming and High-quality video\\\\\". The ad have to be in a best Meta composition guidelines, providing best Conversion Rate. ](https://preview.redd.it/wm4g7k8ginhg1.jpg?width=2258&amp;format=pjpg&amp;auto=webp&amp;s=b85977ab25f67fcfe2c4cab014456b105a07f72c)\n\n# What we built\n\nWe introduce **CRAFT (Continuous Reasoning and Agentic Feedback Tuning)** \\-- a **training-free, model-agnostic reasoning layer** for image generation and image editing.  \nInstead of assuming the prompt is followed correctly, CRAFT explicitly reasons about *what must be true in the image*.\n\nAt a high level, CRAFT:\n\n1. Decomposes a prompt into **explicit visual constraints** (structured questions)\n2. Generates an image with any existing T2I model\n3. Verifies each constraint using a VLM (Yes / No)\n4. Applies **targeted prompt edits or image edits only where constraints fail**\n5. Iterates with an explicit stopping condition\n\nNo retraining. No scaling the base model. No custom architecture.\n\n[Schema of CRAFT](https://preview.redd.it/qh3gtr0jinhg1.jpg?width=2991&amp;format=pjpg&amp;auto=webp&amp;s=12409add9ae8a8036ec47bd5de133b8c2995320b)\n\n# Why this matters\n\nThis turns image generation into a **verifiable, controllable inference-time loop** rather than a single opaque sampling step.\n\nIn practice, this significantly improves:\n\n* compositional correctness\n* long-prompt faithfulness\n* text rendering\n* consistency across iterations\n\nWith modest overhead (typically \\~3 iterations).\n\n# Evaluation\n\n[baseline vs CRAFT for prompt: a toaster shaking hands with a microwave](https://preview.redd.it/59rfjvykinhg1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;s=fb83e7348bcdecbeaac70e4a2d73b5b2cf2c8b41)\n\nWe evaluate CRAFT across multiple backbones:\n\n* FLUX-Schnell / FLUX-Dev / FLUX-2 Pro\n* Qwen-Image\n* Z-Image-Turbo\n\nDatasets:\n\n* DSG-1K (compositional prompts)\n* Parti-Prompt (long-form prompts)\n\nMetrics:\n\n* Visual Question Accuracy (DVQ)\n* DSGScore\n* Automatic side-by-side preference judging\n\nCRAFT consistently improves compositional accuracy and preference scores across all tested models, and performs competitively with prompt-optimization methods such as Maestro -- without retraining or model-specific tuning.\n\n# Limitations\n\n* Quality depends on the VLM judge\n* Very abstract prompts are harder to decompose\n* Iterative loops add latency and API cost (though small relative to high-end models)\n\n# Links\n\n* Demo: [https://craft-demo.flymy.ai](https://craft-demo.flymy.ai)\n* Paper (arXiv): [https://arxiv.org/abs/2512.20362](https://arxiv.org/abs/2512.20362)\n* PDF: [https://arxiv.org/pdf/2512.20362](https://arxiv.org/pdf/2512.20362)\n\nWe built this because we kept running into the same production failure modes.  \nHappy to discuss design decisions, evaluation, or failure cases.",
    "publish_datetime": "2026-02-05T10:15:52Z",
    "scraping_timestamp": "2026-02-06T05:46:23.107997Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 18,
    "num_comments": 5,
    "engagement_score": 28.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[P] CRAFT: thinking agent for image generation and edit",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwhkcg/p_craft_thinking_agent_for_image_generation_and/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwhkcg/p_craft_thinking_agent_for_image_generation_and/",
      "selftext": "We operate an infrastructure startup focused on large-scale image and video generation.  \nBecause we run these models in real production pipelines we repeatedly encounter the same issues:\n\n* fragile prompt following\n* broken composition in long or constrained prompts\n* hallucinated objects and incorrect text rendering\n* manual, ad-hoc iteration loops to ‚Äúfix‚Äù generations\n\nThe underlying models are strong. The failure mode is not model capacity, but the lack of *explicit reasoning and verification* around the generation step.\n\nMost existing solutions try to address this by:\n\n* prompt rewriting\n* longer prompts with more constraints\n* multi-stage pipelines\n* manual regenerate-and-inspect loops\n\nThese help, but they scale poorly and remain brittle.\n\n[prompt: Make an ad of TV 55\\\\\", 4K with Title text \\\\\"New 4K Sony Bravia\\\\\" and CTA text \\\\\"Best for gaming and High-quality video\\\\\". The ad have to be in a best Meta composition guidelines, providing best Conversion Rate. ](https://preview.redd.it/wm4g7k8ginhg1.jpg?width=2258&amp;format=pjpg&amp;auto=webp&amp;s=b85977ab25f67fcfe2c4cab014456b105a07f72c)\n\n# What we built\n\nWe introduce **CRAFT (Continuous Reasoning and Agentic Feedback Tuning)** \\-- a **training-free, model-agnostic reasoning layer** for image generation and image editing.  \nInstead of assuming the prompt is followed correctly, CRAFT explicitly reasons about *what must be true in the image*.\n\nAt a high level, CRAFT:\n\n1. Decomposes a prompt into **explicit visual constraints** (structured questions)\n2. Generates an image with any existing T2I model\n3. Verifies each constraint using a VLM (Yes / No)\n4. Applies **targeted prompt edits or image edits only where constraints fail**\n5. Iterates with an explicit stopping condition\n\nNo retraining. No scaling the base model. No custom architecture.\n\n[Schema of CRAFT](https://preview.redd.it/qh3gtr0jinhg1.jpg?width=2991&amp;format=pjpg&amp;auto=webp&amp;s=12409add9ae8a8036ec47bd5de133b8c2995320b)\n\n# Why this matters\n\nThis turns image generation into a **verifiable, controllable inference-time loop** rather than a single opaque sampling step.\n\nIn practice, this significantly improves:\n\n* compositional correctness\n* long-prompt faithfulness\n* text rendering\n* consistency across iterations\n\nWith modest overhead (typically \\~3 iterations).\n\n# Evaluation\n\n[baseline vs CRAFT for prompt: a toaster shaking hands with a microwave](https://preview.redd.it/59rfjvykinhg1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;s=fb83e7348bcdecbeaac70e4a2d73b5b2cf2c8b41)\n\nWe evaluate CRAFT across multiple backbones:\n\n* FLUX-Schnell / FLUX-Dev / FLUX-2 Pro\n* Qwen-Image\n* Z-Image-Turbo\n\nDatasets:\n\n* DSG-1K (compositional prompts)\n* Parti-Prompt (long-form prompts)\n\nMetrics:\n\n* Visual Question Accuracy (DVQ)\n* DSGScore\n* Automatic side-by-side preference judging\n\nCRAFT consistently improves compositional accuracy and preference scores across all tested models, and performs competitively with prompt-optimization methods such as Maestro -- without retraining or model-specific tuning.\n\n# Limitations\n\n* Quality depends on the VLM judge\n* Very abstract prompts are harder to decompose\n* Iterative loops add latency and API cost (though small relative to high-end models)\n\n# Links\n\n* Demo: [https://craft-demo.flymy.ai](https://craft-demo.flymy.ai)\n* Paper (arXiv): [https://arxiv.org/abs/2512.20362](https://arxiv.org/abs/2512.20362)\n* PDF: [https://arxiv.org/pdf/2512.20362](https://arxiv.org/pdf/2512.20362)\n\nWe built this because we kept running into the same production failure modes.  \nHappy to discuss design decisions, evaluation, or failure cases.",
      "score": 18,
      "num_comments": 5,
      "created_utc": 1770286552.0,
      "author": "Worldly-Ant-6889"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwhi9l/r_seeking_advice_stalling_at_4550_accuracy_on_hms/",
    "author": "Sure-Key-4300",
    "title": "[R] Seeking Advice: Stalling at 45-50% Accuracy on HMS Brain Activity (EEG Spectrogram) Cross-Subject Classification",
    "source": "reddit",
    "content": "I am working on the HMS Harmful Brain Activity Classification task. The goal is to classify 10-minute EEG segments into 6 categories: Seizure, GPD, LRDA, GRDA, LPD, and Other, based on spectrogram representations.\n\nThe core challenge I am tackling is Cross-Subject Generalization. While my models perform exceptionally well (85%+) when training and testing on the same patients, the performance drops significantly to a 65-70% plateau when evaluated on \"unseen\" patients (Subject-Wise Split). This suggests the model is over-relying on \"patient fingerprints\" (baseline EEG power, hardware artifacts, skull morphology) rather than universal medical pathology.\n\nData Setup:\n\n‚Ä¢ Input: 4-channel spectrograms (LL, RL, LP, RP) converted to 3-channel RGB images using a JET colormap.\n\n‚Ä¢ Normalization: Log-transformation followed by Spectral Z-score normalization (per frequency band).\n\n‚Ä¢ Validation Strategy: StratifiedGroupKFold based on patient\\\\\\_id to ensure no patient leakage.\n\nApproaches Attempted &amp; Results:\n\n1. Prototypical Few-Shot Learning (FSL)\n\n‚Ä¢ Concept: Instead of standard classification, I used a ProtoNet with a ConvNeXt-Tiny backbone to learn a metric space where clusters of diseases are formed.\n\n‚Ä¢ Why it was used: To force the model to learn the \"similarity\" of a seizure across different brains rather than a hard-coded mapping.\n\n‚Ä¢ Result: Reached \\\\\\~68% accuracy. High ROC-AUC (&gt;0.82), but raw accuracy stayed low. It seems the \"prototypes\" (centroids) shift too much between different patients.\n\n2. Domain Adversarial Neural Networks (DANN) / Patient-Agnostic Training\n\n‚Ä¢ Concept: Added an adversarial head with a Gradient Reversal Layer (GRL). The model has two tasks: 1) Classify the disease, and 2) Fail to identify the patient.\n\n‚Ä¢ Why it was used: To mathematically \"scrub\" the patient-specific features from the latent space, forcing the backbone to become \"Model Agnostic.\"\n\n‚Ä¢ Result: Improved generalization stability, but accuracy is still stuck in the high 60s. The adversarial head's accuracy is low (good sign), but the diagnostic head isn't pushing further.\n\n3. Advanced Backbone Fine-Tuning (ResNet-50 &amp; ConvNeXt)\n\n‚Ä¢ Concept: Switched from EfficientNet to ResNet-50 and ConvNeXt-Tiny using phased fine-tuning (frozen backbone first, then discriminative learning rates).\n\n‚Ä¢ Why it was used: To see if a deeper residual structure (ResNet) or a more global receptive field (ConvNeXt) could capture rhythmic harmonies better.\n\n‚Ä¢ Result: ConvNeXt performed the best, but the gap between training and cross-subject validation remains wide.\n\n4. Handling Data Imbalance (Weighted Sampling vs. Oversampling)\n\n‚Ä¢ Concept: Replaced duplicating minority classes (oversampling) with a WeightedRandomSampler and added LabelSmoothingLoss(0.15).\n\n‚Ä¢ Why it was used: To prevent the model from memorizing duplicates of minority samples and to account for expert disagreement in medical labels.\n\n‚Ä¢ Result: Reduced overfitting significantly, but the validation accuracy didn't \"break through\" to the 75%+ target.\n\nWhat I've Observed:\n\n1. The Accuracy-AUC Gap: My ROC-AUC is often quite high (0.80-0.85), but raw accuracy is 10-15% lower. The model ranks the correct class highly but often misses the final threshold.\n\n2. Spectral Signatures: The model seems to pick up on the \"loudness\" (power) of certain frequencies that are patient-specific rather than the rhythmic spikes that are disease-specific.\n\n3. Complexity: Simplifying the model (ResNet-18) helps with stability but lacks the capacity to distinguish between subtle classes like LPD vs. LRDA.\n\nHas anyone successfully bridged the gap between within-subject and cross-subject performance on EEG data? Should I be looking into Self-Supervised Pre-training (MAE), or is there a specific Signal Processing Inductive Bias I am missing?\n\nAny advice on how to force the model to ignore the \"patient fingerprint\" more effectively would be greatly appreciated!",
    "publish_datetime": "2026-02-05T10:12:32Z",
    "scraping_timestamp": "2026-02-06T05:46:23.109713Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 1,
    "engagement_score": 2.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] Seeking Advice: Stalling at 45-50% Accuracy on HMS Brain Activity (EEG Spectrogram) Cross-Subject Classification",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwhi9l/r_seeking_advice_stalling_at_4550_accuracy_on_hms/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwhi9l/r_seeking_advice_stalling_at_4550_accuracy_on_hms/",
      "selftext": "I am working on the HMS Harmful Brain Activity Classification task. The goal is to classify 10-minute EEG segments into 6 categories: Seizure, GPD, LRDA, GRDA, LPD, and Other, based on spectrogram representations.\n\nThe core challenge I am tackling is Cross-Subject Generalization. While my models perform exceptionally well (85%+) when training and testing on the same patients, the performance drops significantly to a 65-70% plateau when evaluated on \"unseen\" patients (Subject-Wise Split). This suggests the model is over-relying on \"patient fingerprints\" (baseline EEG power, hardware artifacts, skull morphology) rather than universal medical pathology.\n\nData Setup:\n\n‚Ä¢ Input: 4-channel spectrograms (LL, RL, LP, RP) converted to 3-channel RGB images using a JET colormap.\n\n‚Ä¢ Normalization: Log-transformation followed by Spectral Z-score normalization (per frequency band).\n\n‚Ä¢ Validation Strategy: StratifiedGroupKFold based on patient\\\\\\_id to ensure no patient leakage.\n\nApproaches Attempted &amp; Results:\n\n1. Prototypical Few-Shot Learning (FSL)\n\n‚Ä¢ Concept: Instead of standard classification, I used a ProtoNet with a ConvNeXt-Tiny backbone to learn a metric space where clusters of diseases are formed.\n\n‚Ä¢ Why it was used: To force the model to learn the \"similarity\" of a seizure across different brains rather than a hard-coded mapping.\n\n‚Ä¢ Result: Reached \\\\\\~68% accuracy. High ROC-AUC (&gt;0.82), but raw accuracy stayed low. It seems the \"prototypes\" (centroids) shift too much between different patients.\n\n2. Domain Adversarial Neural Networks (DANN) / Patient-Agnostic Training\n\n‚Ä¢ Concept: Added an adversarial head with a Gradient Reversal Layer (GRL). The model has two tasks: 1) Classify the disease, and 2) Fail to identify the patient.\n\n‚Ä¢ Why it was used: To mathematically \"scrub\" the patient-specific features from the latent space, forcing the backbone to become \"Model Agnostic.\"\n\n‚Ä¢ Result: Improved generalization stability, but accuracy is still stuck in the high 60s. The adversarial head's accuracy is low (good sign), but the diagnostic head isn't pushing further.\n\n3. Advanced Backbone Fine-Tuning (ResNet-50 &amp; ConvNeXt)\n\n‚Ä¢ Concept: Switched from EfficientNet to ResNet-50 and ConvNeXt-Tiny using phased fine-tuning (frozen backbone first, then discriminative learning rates).\n\n‚Ä¢ Why it was used: To see if a deeper residual structure (ResNet) or a more global receptive field (ConvNeXt) could capture rhythmic harmonies better.\n\n‚Ä¢ Result: ConvNeXt performed the best, but the gap between training and cross-subject validation remains wide.\n\n4. Handling Data Imbalance (Weighted Sampling vs. Oversampling)\n\n‚Ä¢ Concept: Replaced duplicating minority classes (oversampling) with a WeightedRandomSampler and added LabelSmoothingLoss(0.15).\n\n‚Ä¢ Why it was used: To prevent the model from memorizing duplicates of minority samples and to account for expert disagreement in medical labels.\n\n‚Ä¢ Result: Reduced overfitting significantly, but the validation accuracy didn't \"break through\" to the 75%+ target.\n\nWhat I've Observed:\n\n1. The Accuracy-AUC Gap: My ROC-AUC is often quite high (0.80-0.85), but raw accuracy is 10-15% lower. The model ranks the correct class highly but often misses the final threshold.\n\n2. Spectral Signatures: The model seems to pick up on the \"loudness\" (power) of certain frequencies that are patient-specific rather than the rhythmic spikes that are disease-specific.\n\n3. Complexity: Simplifying the model (ResNet-18) helps with stability but lacks the capacity to distinguish between subtle classes like LPD vs. LRDA.\n\nHas anyone successfully bridged the gap between within-subject and cross-subject performance on EEG data? Should I be looking into Self-Supervised Pre-training (MAE), or is there a specific Signal Processing Inductive Bias I am missing?\n\nAny advice on how to force the model to ignore the \"patient fingerprint\" more effectively would be greatly appreciated!",
      "score": 0,
      "num_comments": 1,
      "created_utc": 1770286352.0,
      "author": "Sure-Key-4300"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qwhhqa/r_craft_thinking_agent_for_image_generation_and/",
    "author": "Worldly-Ant-6889",
    "title": "[R] CRAFT: thinking agent for image generation and edit",
    "source": "reddit",
    "content": "We operate an infrastructure startup focused on large-scale image and video generation.  \nBecause we run these models in real production pipelines we repeatedly encounter the same issues:\n\n* fragile prompt following\n* broken composition in long or constrained prompts\n* hallucinated objects and incorrect text rendering\n* manual, ad-hoc iteration loops to ‚Äúfix‚Äù generations\n\nThe underlying models are strong. The failure mode is not model capacity, but the lack of *explicit reasoning and verification* around the generation step.\n\nMost existing solutions try to address this by:\n\n* prompt rewriting\n* longer prompts with more constraints\n* multi-stage pipelines\n* manual regenerate-and-inspect loops\n\nThese help, but they scale poorly and remain brittle.\n\n[prompt: Make an ad of TV 55\\\\\", 4K with Title text \\\\\"New 4K Sony Bravia\\\\\" and CTA text \\\\\"Best for gaming and High-quality video\\\\\". The ad have to be in a best Meta composition guidelines, providing best Conversion Rate. ](https://preview.redd.it/i55r7b8ffnhg1.jpg?width=2258&amp;format=pjpg&amp;auto=webp&amp;s=1fe2da5aa1b194950442e24be2187c4e3c34eff2)\n\n# What we built\n\nWe introduce **CRAFT (Continuous Reasoning and Agentic Feedback Tuning)** \\-- a **training-free, model-agnostic reasoning layer** for image generation and image editing.  \nInstead of assuming the prompt is followed correctly, CRAFT explicitly reasons about *what must be true in the image*.\n\nAt a high level, CRAFT:\n\n1. Decomposes a prompt into **explicit visual constraints** (structured questions)\n2. Generates an image with any existing T2I model\n3. Verifies each constraint using a VLM (Yes / No)\n4. Applies **targeted prompt edits or image edits only where constraints fail**\n5. Iterates with an explicit stopping condition\n\n[Schema of CRAFT](https://preview.redd.it/lwv6kopsfnhg1.jpg?width=2991&amp;format=pjpg&amp;auto=webp&amp;s=25884f6f0ec599838cbf57772f80dfd54392b152)\n\nNo retraining. No scaling the base model. No custom architecture.  \n  \nWhy this matters\n\nThis turns image generation into a **verifiable, controllable inference-time loop** rather than a single opaque sampling step.\n\nIn practice, this significantly improves:\n\n* compositional correctness\n* long-prompt faithfulness\n* text rendering\n* consistency across iterations\n\nWith modest overhead (typically \\~3 iterations).\n\n# Evaluation\n\n[baseline vs CRAFT for prompt: a toaster shaking hands with a microwave](https://preview.redd.it/vbknnqqufnhg1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;s=26165c8089f3657cd0f35264a270eb20c747f890)\n\nWe evaluate CRAFT across multiple backbones:\n\n* FLUX-Schnell / FLUX-Dev / FLUX-2 Pro\n* Qwen-Image / NanoBanana / Seedream\n* Z-Image-Turbo\n\nDatasets:\n\n* DSG-1K (compositional prompts)\n* Parti-Prompt (long-form prompts)\n\nMetrics:\n\n* Visual Question Accuracy (DVQ)\n* DSGScore\n* Automatic side-by-side preference judging\n\nCRAFT consistently improves compositional accuracy and preference scores across all tested models, and performs competitively with prompt-optimization methods such as Maestro -- without retraining or model-specific tuning.\n\n# Limitations\n\n* Quality depends on the VLM judge\n* Very abstract prompts are harder to decompose\n* Iterative loops add latency and API cost (though small relative to high-end models)\n\n# Links\n\n* More info: [https://research.flymy.ai/craft](https://research.flymy.ai/craft)\n* Demo: [https://craft-demo.flymy.ai](https://craft-demo.flymy.ai)\n* Paper (arXiv): [https://arxiv.org/abs/2512.20362](https://arxiv.org/abs/2512.20362)\n\nWe built this because we kept running into the same production failure modes.  \nHappy to discuss design decisions, evaluation, or failure cases.",
    "publish_datetime": "2026-02-05T10:11:40Z",
    "scraping_timestamp": "2026-02-06T05:46:23.111328Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 2,
    "engagement_score": 4.0,
    "subreddit": "MachineLearning",
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] CRAFT: thinking agent for image generation and edit",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qwhhqa/r_craft_thinking_agent_for_image_generation_and/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qwhhqa/r_craft_thinking_agent_for_image_generation_and/",
      "selftext": "We operate an infrastructure startup focused on large-scale image and video generation.  \nBecause we run these models in real production pipelines we repeatedly encounter the same issues:\n\n* fragile prompt following\n* broken composition in long or constrained prompts\n* hallucinated objects and incorrect text rendering\n* manual, ad-hoc iteration loops to ‚Äúfix‚Äù generations\n\nThe underlying models are strong. The failure mode is not model capacity, but the lack of *explicit reasoning and verification* around the generation step.\n\nMost existing solutions try to address this by:\n\n* prompt rewriting\n* longer prompts with more constraints\n* multi-stage pipelines\n* manual regenerate-and-inspect loops\n\nThese help, but they scale poorly and remain brittle.\n\n[prompt: Make an ad of TV 55\\\\\", 4K with Title text \\\\\"New 4K Sony Bravia\\\\\" and CTA text \\\\\"Best for gaming and High-quality video\\\\\". The ad have to be in a best Meta composition guidelines, providing best Conversion Rate. ](https://preview.redd.it/i55r7b8ffnhg1.jpg?width=2258&amp;format=pjpg&amp;auto=webp&amp;s=1fe2da5aa1b194950442e24be2187c4e3c34eff2)\n\n# What we built\n\nWe introduce **CRAFT (Continuous Reasoning and Agentic Feedback Tuning)** \\-- a **training-free, model-agnostic reasoning layer** for image generation and image editing.  \nInstead of assuming the prompt is followed correctly, CRAFT explicitly reasons about *what must be true in the image*.\n\nAt a high level, CRAFT:\n\n1. Decomposes a prompt into **explicit visual constraints** (structured questions)\n2. Generates an image with any existing T2I model\n3. Verifies each constraint using a VLM (Yes / No)\n4. Applies **targeted prompt edits or image edits only where constraints fail**\n5. Iterates with an explicit stopping condition\n\n[Schema of CRAFT](https://preview.redd.it/lwv6kopsfnhg1.jpg?width=2991&amp;format=pjpg&amp;auto=webp&amp;s=25884f6f0ec599838cbf57772f80dfd54392b152)\n\nNo retraining. No scaling the base model. No custom architecture.  \n  \nWhy this matters\n\nThis turns image generation into a **verifiable, controllable inference-time loop** rather than a single opaque sampling step.\n\nIn practice, this significantly improves:\n\n* compositional correctness\n* long-prompt faithfulness\n* text rendering\n* consistency across iterations\n\nWith modest overhead (typically \\~3 iterations).\n\n# Evaluation\n\n[baseline vs CRAFT for prompt: a toaster shaking hands with a microwave](https://preview.redd.it/vbknnqqufnhg1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;s=26165c8089f3657cd0f35264a270eb20c747f890)\n\nWe evaluate CRAFT across multiple backbones:\n\n* FLUX-Schnell / FLUX-Dev / FLUX-2 Pro\n* Qwen-Image / NanoBanana / Seedream\n* Z-Image-Turbo\n\nDatasets:\n\n* DSG-1K (compositional prompts)\n* Parti-Prompt (long-form prompts)\n\nMetrics:\n\n* Visual Question Accuracy (DVQ)\n* DSGScore\n* Automatic side-by-side preference judging\n\nCRAFT consistently improves compositional accuracy and preference scores across all tested models, and performs competitively with prompt-optimization methods such as Maestro -- without retraining or model-specific tuning.\n\n# Limitations\n\n* Quality depends on the VLM judge\n* Very abstract prompts are harder to decompose\n* Iterative loops add latency and API cost (though small relative to high-end models)\n\n# Links\n\n* More info: [https://research.flymy.ai/craft](https://research.flymy.ai/craft)\n* Demo: [https://craft-demo.flymy.ai](https://craft-demo.flymy.ai)\n* Paper (arXiv): [https://arxiv.org/abs/2512.20362](https://arxiv.org/abs/2512.20362)\n\nWe built this because we kept running into the same production failure modes.  \nHappy to discuss design decisions, evaluation, or failure cases.",
      "score": 0,
      "num_comments": 2,
      "created_utc": 1770286300.0,
      "author": "Worldly-Ant-6889"
    }
  },
  {
    "link": "https://www.reddit.com/r/artificial/comments/1qwo82n/early_user_test_of_a_persistent_ai_narrative/",
    "author": "Distinct-Path659",
    "title": "Early user test of a persistent AI narrative system with kids ‚Äî some unexpected engagement patterns",
    "source": "reddit",
    "content": "I ran a small real-world test today with two kids (ages 8 and 11) using a long-running AI story world I‚Äôve been experimenting with.\n\nInstead of one-shot story generation, the system maintains a persistent world state where choices carry over and shape future events.\n\nI let them pick the setting ‚Äî they chose a Minecraft √ó Harry Potter mashup where they play wizards trying to defeat the Ender Dragon.\n\nOne thing that made a huge difference: I used their real names as the characters, and the story started in their actual school.\n\nThe engine generated story text and illustrations each round. They made all the choices.\n\nAfter about 10 rounds, they were constantly laughing, debating which option to pick, and building on each other‚Äôs ideas. It felt much more like co-creating a world than listening to a story.\n\nWhen I told them it was bedtime, they didn‚Äôt want to stop. They kept asking what would happen next.\n\nA few observations that surprised me:\n\nPersonalization seemed to matter more than anything else. Once it became their world, emotional investment was instant.\n\nAlthough I designed it as a single-player experience, co-play emerged naturally. The shared decision-making and social dynamic massively increased engagement.\n\nBoth ages stayed fully engaged the whole time. I expected the younger one to drop off sooner, but the persistent world kept them both hooked.\n\nOne issue I noticed: my ‚Äúre-immersion‚Äù mechanic (an in-world character emotionally reconnecting players after breaks instead of a dry recap) triggered too frequently between consecutive rounds. The repetition was noticeable. This looks like a simple trigger tuning problem (should probably only fire after longer gaps).\n\nWhat I haven‚Äôt tested yet:\n\n‚Äì Whether kids can reconnect naturally after a real multi-hour break\n\n‚Äì Whether they can retell the story in a coherent way\n\n‚Äì Whether they‚Äôll come back unprompted the next day\n\nThe earlier stress tests showed that constraint mechanisms help keep long-running narratives technically coherent.\n\nWhat this small user test suggests is that coherence itself isn‚Äôt what kids consciously care about ‚Äî but it seems to be the infrastructure that makes personalization, consequence, and agency feel real.\n\nCurious if others working on long-horizon agents, narrative systems, or co-creative AI have seen similar effects around personalization and persistence.",
    "publish_datetime": "2026-02-05T15:26:53Z",
    "scraping_timestamp": "2026-02-06T05:46:23.112373Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 10,
    "num_comments": 9,
    "engagement_score": 28.0,
    "subreddit": "artificial",
    "raw": {
      "subreddit": "artificial",
      "title": "Early user test of a persistent AI narrative system with kids ‚Äî some unexpected engagement patterns",
      "url": "https://www.reddit.com/r/artificial/comments/1qwo82n/early_user_test_of_a_persistent_ai_narrative/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qwo82n/early_user_test_of_a_persistent_ai_narrative/",
      "selftext": "I ran a small real-world test today with two kids (ages 8 and 11) using a long-running AI story world I‚Äôve been experimenting with.\n\nInstead of one-shot story generation, the system maintains a persistent world state where choices carry over and shape future events.\n\nI let them pick the setting ‚Äî they chose a Minecraft √ó Harry Potter mashup where they play wizards trying to defeat the Ender Dragon.\n\nOne thing that made a huge difference: I used their real names as the characters, and the story started in their actual school.\n\nThe engine generated story text and illustrations each round. They made all the choices.\n\nAfter about 10 rounds, they were constantly laughing, debating which option to pick, and building on each other‚Äôs ideas. It felt much more like co-creating a world than listening to a story.\n\nWhen I told them it was bedtime, they didn‚Äôt want to stop. They kept asking what would happen next.\n\nA few observations that surprised me:\n\nPersonalization seemed to matter more than anything else. Once it became their world, emotional investment was instant.\n\nAlthough I designed it as a single-player experience, co-play emerged naturally. The shared decision-making and social dynamic massively increased engagement.\n\nBoth ages stayed fully engaged the whole time. I expected the younger one to drop off sooner, but the persistent world kept them both hooked.\n\nOne issue I noticed: my ‚Äúre-immersion‚Äù mechanic (an in-world character emotionally reconnecting players after breaks instead of a dry recap) triggered too frequently between consecutive rounds. The repetition was noticeable. This looks like a simple trigger tuning problem (should probably only fire after longer gaps).\n\nWhat I haven‚Äôt tested yet:\n\n‚Äì Whether kids can reconnect naturally after a real multi-hour break\n\n‚Äì Whether they can retell the story in a coherent way\n\n‚Äì Whether they‚Äôll come back unprompted the next day\n\nThe earlier stress tests showed that constraint mechanisms help keep long-running narratives technically coherent.\n\nWhat this small user test suggests is that coherence itself isn‚Äôt what kids consciously care about ‚Äî but it seems to be the infrastructure that makes personalization, consequence, and agency feel real.\n\nCurious if others working on long-horizon agents, narrative systems, or co-creative AI have seen similar effects around personalization and persistence.",
      "score": 10,
      "num_comments": 9,
      "created_utc": 1770305213.0,
      "author": "Distinct-Path659"
    }
  },
  {
    "link": "https://www.theguardian.com/global-development/2026/feb/05/in-the-end-you-feel-blank-indias-female-workers-watching-hours-of-abusive-content-to-train-ai",
    "author": "tekz",
    "title": "‚ÄòIn the end, you feel blank‚Äô: India‚Äôs female workers watching hours of abusive content to train AI",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-05T10:30:54Z",
    "scraping_timestamp": "2026-02-06T05:46:23.112378Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 179,
    "num_comments": 49,
    "engagement_score": 277.0,
    "subreddit": "artificial",
    "raw": {
      "subreddit": "artificial",
      "title": "‚ÄòIn the end, you feel blank‚Äô: India‚Äôs female workers watching hours of abusive content to train AI",
      "url": "https://www.theguardian.com/global-development/2026/feb/05/in-the-end-you-feel-blank-indias-female-workers-watching-hours-of-abusive-content-to-train-ai",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qwhthi/in_the_end_you_feel_blank_indias_female_workers/",
      "selftext": "",
      "score": 179,
      "num_comments": 49,
      "created_utc": 1770287454.0,
      "author": "tekz"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx9cx6/beta_version_out_18_subs_so_far/",
    "author": "MexicanBugha",
    "title": "Beta Version Out - 18 subs so far!",
    "source": "reddit",
    "content": "Hello fellow vibe coders!\n\nI hope you all are doing well. I wanted to share with you all how the project I‚Äôve vibe coded completely is going.\n\nI spent the last couple of months creating a mini game platform (NYT and LinkedIn style) in which players compete for weekly rewards.\n\nJust yesterday I finally reached a point in which I believe its good enough to launch it as a beta version. I put some of my own money as the prize pool, allowing free trial users to compete for it and its going pretty well so far. Got 18 subscribers in the first 48 hours.\n\nHopefully I don‚Äôt come off across as bragging. Its genuinely nice to see people join and enjoy the project I‚Äôve been working on over the past months.\n\nFree trials are enabled‚Ä¶. So I‚Äôd be happy if you guys can check it out, mess around with it and give me any feedback. Good or bad\n\nhttps://dailytengames.com",
    "publish_datetime": "2026-02-06T05:44:22Z",
    "scraping_timestamp": "2026-02-06T05:46:23.112796Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "Beta Version Out - 18 subs so far!",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx9cx6/beta_version_out_18_subs_so_far/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx9cx6/beta_version_out_18_subs_so_far/",
      "selftext": "Hello fellow vibe coders!\n\nI hope you all are doing well. I wanted to share with you all how the project I‚Äôve vibe coded completely is going.\n\nI spent the last couple of months creating a mini game platform (NYT and LinkedIn style) in which players compete for weekly rewards.\n\nJust yesterday I finally reached a point in which I believe its good enough to launch it as a beta version. I put some of my own money as the prize pool, allowing free trial users to compete for it and its going pretty well so far. Got 18 subscribers in the first 48 hours.\n\nHopefully I don‚Äôt come off across as bragging. Its genuinely nice to see people join and enjoy the project I‚Äôve been working on over the past months.\n\nFree trials are enabled‚Ä¶. So I‚Äôd be happy if you guys can check it out, mess around with it and give me any feedback. Good or bad\n\nhttps://dailytengames.com",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770356662.0,
      "author": "MexicanBugha"
    }
  },
  {
    "link": "http://www.templestuart.com",
    "author": "Plastic-Edge-1654",
    "title": "Seeking Feedback!",
    "source": "reddit",
    "content": "So I've been head down building an app for the last few months and I'm finally ready to share it.\n\nI've always been frustrated with how scattered personal finance is. \n\nYou've got one app for budgeting, another for bookkeeping, another for tracking investments, another for taxes ‚Äî and none of them really talk to each other. \n\nMany of them oversimplify your data, hide the details, and charge you for the privilege. \n\nI wanted something that actually showed me everything in one place, the way a CPA would see it, not the way a marketing team thinks I should see it.\n\nSo I built it!\n\nAt its core, it's a budgeting and life expense planner. \n\nYou can plan out your spending across every area of your life ‚Äî home, auto, health, shopping, trips, personal ‚Äî and track committed plans against what actually happens. \n\nIt does double-entry bookkeeping under the hood, so everything balances and everything has a paper trail. \n\nThere's trip planning with cost splitting for group travel. \n\nThere's a hub that ties it all together like a command center.\n\nThat core experience is completely free.\n\nIf you want to connect your bank accounts and get automatic transaction syncing, trading analytics, and bookkeeping features, that's the Pro tier at $20/month. \n\nAnd if you want AI-powered meal planning, spending insights, and trip recommendations on top of all that, that's Pro+ at $40/month.  \n\n(I'm still working on building out more AI modules for the other categories)\n\nI have to charge for those because I'm paying real costs for the bank connections, data processing, and AI APIs behind the scenes ‚Äî and I can't afford to eat my infrastructure bills.\n\nWhat I really need right now is feedback on the free stuff. The budgeting modules, the expense planning, the trip planner, the hub ‚Äî that's where I want fresh eyes. \n\nIf the paid features interest you too, by all means go for it and tell me what you think. But no pressure there.\n\nA few people have asked me about security so I want to address that directly. \n\nYour bank connections go through Plaid ‚Äî the same service used by Venmo, Robinhood, and most major fintech apps. \n\nI never see your bank credentials. The database runs on Microsoft Azure with encryption at rest and in transit. \n\nOn my end, every API route requires authentication, sessions are isolated per user, and there's no cross-account data leakage. \n\nI take this seriously because it's my own financial data in there too.\n\nIf you want to check it out: www.templestuart.com \n\nTrack your money. Plan your trips. Find your people.\n\nWould genuinely appreciate anyone willing to give it a try and tell me what's broken, what's confusing, or what's missing. \n\nCritical feedback is appreciate!  Don't hold back! I can take it \n\nDMs always open.",
    "publish_datetime": "2026-02-06T05:31:56Z",
    "scraping_timestamp": "2026-02-06T05:46:23.113999Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "Seeking Feedback!",
      "url": "http://www.templestuart.com",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx94dl/seeking_feedback/",
      "selftext": "So I've been head down building an app for the last few months and I'm finally ready to share it.\n\nI've always been frustrated with how scattered personal finance is. \n\nYou've got one app for budgeting, another for bookkeeping, another for tracking investments, another for taxes ‚Äî and none of them really talk to each other. \n\nMany of them oversimplify your data, hide the details, and charge you for the privilege. \n\nI wanted something that actually showed me everything in one place, the way a CPA would see it, not the way a marketing team thinks I should see it.\n\nSo I built it!\n\nAt its core, it's a budgeting and life expense planner. \n\nYou can plan out your spending across every area of your life ‚Äî home, auto, health, shopping, trips, personal ‚Äî and track committed plans against what actually happens. \n\nIt does double-entry bookkeeping under the hood, so everything balances and everything has a paper trail. \n\nThere's trip planning with cost splitting for group travel. \n\nThere's a hub that ties it all together like a command center.\n\nThat core experience is completely free.\n\nIf you want to connect your bank accounts and get automatic transaction syncing, trading analytics, and bookkeeping features, that's the Pro tier at $20/month. \n\nAnd if you want AI-powered meal planning, spending insights, and trip recommendations on top of all that, that's Pro+ at $40/month.  \n\n(I'm still working on building out more AI modules for the other categories)\n\nI have to charge for those because I'm paying real costs for the bank connections, data processing, and AI APIs behind the scenes ‚Äî and I can't afford to eat my infrastructure bills.\n\nWhat I really need right now is feedback on the free stuff. The budgeting modules, the expense planning, the trip planner, the hub ‚Äî that's where I want fresh eyes. \n\nIf the paid features interest you too, by all means go for it and tell me what you think. But no pressure there.\n\nA few people have asked me about security so I want to address that directly. \n\nYour bank connections go through Plaid ‚Äî the same service used by Venmo, Robinhood, and most major fintech apps. \n\nI never see your bank credentials. The database runs on Microsoft Azure with encryption at rest and in transit. \n\nOn my end, every API route requires authentication, sessions are isolated per user, and there's no cross-account data leakage. \n\nI take this seriously because it's my own financial data in there too.\n\nIf you want to check it out: www.templestuart.com \n\nTrack your money. Plan your trips. Find your people.\n\nWould genuinely appreciate anyone willing to give it a try and tell me what's broken, what's confusing, or what's missing. \n\nCritical feedback is appreciate!  Don't hold back! I can take it \n\nDMs always open.",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770355916.0,
      "author": "Plastic-Edge-1654"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx8n6j/the_internet_is_a_giant_open_worldand_im_building/",
    "author": "Ambitious-Ad-6758",
    "title": "The internet is a giant open world...and I'm building a game layer on top of it",
    "source": "reddit",
    "content": "The internet is full of fun things to find...Wikipedia rabbit holes, old Reddit threads, the original [Space Jam](https://www.spacejam.com/1996/) website from 1996. But other than pet interest, theres no real reason to go exploring.   \n  \nI'm changing that with Webcrawl, a game platform and browser extension that allows you (a \"Crawler\") to hide digital caches on any webpage. Other Crawlers hunt for them. Your browser extension glows warmer as you get closer. Its like Geocaching meets StumbleUpon meets RPG, where the map is the entire internet.   \n  \nI'm still in early stages of the MVP, but you can join the wait list at [jointhecrawl.com](http://jointhecrawl.com)   \n  \nWould you play?",
    "publish_datetime": "2026-02-06T05:07:07Z",
    "scraping_timestamp": "2026-02-06T05:46:23.114351Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 2,
    "num_comments": 1,
    "engagement_score": 4.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "The internet is a giant open world...and I'm building a game layer on top of it",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx8n6j/the_internet_is_a_giant_open_worldand_im_building/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx8n6j/the_internet_is_a_giant_open_worldand_im_building/",
      "selftext": "The internet is full of fun things to find...Wikipedia rabbit holes, old Reddit threads, the original [Space Jam](https://www.spacejam.com/1996/) website from 1996. But other than pet interest, theres no real reason to go exploring.   \n  \nI'm changing that with Webcrawl, a game platform and browser extension that allows you (a \"Crawler\") to hide digital caches on any webpage. Other Crawlers hunt for them. Your browser extension glows warmer as you get closer. Its like Geocaching meets StumbleUpon meets RPG, where the map is the entire internet.   \n  \nI'm still in early stages of the MVP, but you can join the wait list at [jointhecrawl.com](http://jointhecrawl.com)   \n  \nWould you play?",
      "score": 2,
      "num_comments": 1,
      "created_utc": 1770354427.0,
      "author": "Ambitious-Ad-6758"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx8l1n/im_a_software_engineer_who_got_tired_of_scalpers/",
    "author": "Snizzle_me_timber",
    "title": "I‚Äôm a software engineer who got tired of scalpers and grading confusion. So I built my own App to fix it.",
    "source": "reddit",
    "content": "Hey everyone,\n\nI‚Äôve been collecting Pokemon cards for years, but lately, the \"business\" side of the hobby has made it really difficult to just enjoy it.\n\nI found myself constantly struggling with two main things:\n\n1. **The \"Should I Grade?\" Paralysis:** I‚Äôd stare at a card for hours, trying to look up pop reports, calculate the raw vs. graded value spread, and guess the condition. I honestly never knew if I was making a smart financial move or just wasting $25 on grading fees.\n2. **The Scalper Wall:** Every time a new set drops, I‚Äôd try to buy a box at MSRP, only to get beaten by bots. These notification apps never have worked for me. I have never successfully bought a product online\n\nI‚Äôm a software engineer by trade, so I decided to stop guessing and start building. I spent the last few months coding a tool I call **Slab Advisor**.\n\nI built it to be a \"Sanctuary\" for collectors who want fairness and data, not hype. Here is what it does:\n\n* **\"Should I Grade?\" Analysis:** I built an AI model and data tracker that doesn't just look at the price‚Äîit helps you analyze if a card is *actually worth* sending in based on the current spread and condition. No more guessing. As you track your cards it will keep track of cards that may be worth grading and offers a pregrading tool to help build confidence before sending it in\n* **The \"Fair Play\" Vault:** This is big for me. I know how hard it is to get product at MSRP. I‚Äôm building a members-only vault where I sell sealed products for the price I get them (near MSRP). The goal isn't to flip; it's to create a fair distribution system that actively blocks bots so real collectors can actually open packs. I built a system  that will provide random offers to users when they log in, removing a storefront bots monitor. I will provide easter egg hunts with product hidden within the website that changes over time.\n* **Portfolio Logic:** It tracks your collection's value based on real-time market data, treating your collection like the asset class it is.\n* **Gamification:** This app is made to be social, share your collection with friends and earn badges and trophies as you collect cards.\n\nIt‚Äôs just me building this (no big team, no VC money). I mainly built this for myself to track my collection, but I wanted to share it with the community. \n\nI just launched the \"Stealth Mode\" waitlist today and I‚Äôm looking for other collectors who are tired of the current state of the hobby to try it out. [www.slabadvisor.com](http://www.slabadvisor.com)\n\nIf you want to help me test it and make it better, I‚Äôd love your feedback.",
    "publish_datetime": "2026-02-06T05:04:09Z",
    "scraping_timestamp": "2026-02-06T05:46:23.115790Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software",
      "LLM/AI"
    ],
    "primary_category": "Programming/Software",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "I‚Äôm a software engineer who got tired of scalpers and grading confusion. So I built my own App to fix it.",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx8l1n/im_a_software_engineer_who_got_tired_of_scalpers/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx8l1n/im_a_software_engineer_who_got_tired_of_scalpers/",
      "selftext": "Hey everyone,\n\nI‚Äôve been collecting Pokemon cards for years, but lately, the \"business\" side of the hobby has made it really difficult to just enjoy it.\n\nI found myself constantly struggling with two main things:\n\n1. **The \"Should I Grade?\" Paralysis:** I‚Äôd stare at a card for hours, trying to look up pop reports, calculate the raw vs. graded value spread, and guess the condition. I honestly never knew if I was making a smart financial move or just wasting $25 on grading fees.\n2. **The Scalper Wall:** Every time a new set drops, I‚Äôd try to buy a box at MSRP, only to get beaten by bots. These notification apps never have worked for me. I have never successfully bought a product online\n\nI‚Äôm a software engineer by trade, so I decided to stop guessing and start building. I spent the last few months coding a tool I call **Slab Advisor**.\n\nI built it to be a \"Sanctuary\" for collectors who want fairness and data, not hype. Here is what it does:\n\n* **\"Should I Grade?\" Analysis:** I built an AI model and data tracker that doesn't just look at the price‚Äîit helps you analyze if a card is *actually worth* sending in based on the current spread and condition. No more guessing. As you track your cards it will keep track of cards that may be worth grading and offers a pregrading tool to help build confidence before sending it in\n* **The \"Fair Play\" Vault:** This is big for me. I know how hard it is to get product at MSRP. I‚Äôm building a members-only vault where I sell sealed products for the price I get them (near MSRP). The goal isn't to flip; it's to create a fair distribution system that actively blocks bots so real collectors can actually open packs. I built a system  that will provide random offers to users when they log in, removing a storefront bots monitor. I will provide easter egg hunts with product hidden within the website that changes over time.\n* **Portfolio Logic:** It tracks your collection's value based on real-time market data, treating your collection like the asset class it is.\n* **Gamification:** This app is made to be social, share your collection with friends and earn badges and trophies as you collect cards.\n\nIt‚Äôs just me building this (no big team, no VC money). I mainly built this for myself to track my collection, but I wanted to share it with the community. \n\nI just launched the \"Stealth Mode\" waitlist today and I‚Äôm looking for other collectors who are tired of the current state of the hobby to try it out. [www.slabadvisor.com](http://www.slabadvisor.com)\n\nIf you want to help me test it and make it better, I‚Äôd love your feedback.",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770354249.0,
      "author": "Snizzle_me_timber"
    }
  },
  {
    "link": "https://v.redd.it/fr49cp4z3thg1",
    "author": "CrispyDick420",
    "title": "I built a 5x cheaper alternative to ElevenReader and Speechify",
    "source": "reddit",
    "content": "Hey dudes,\n\nSo I read a lot of fanfics and web novels. My eyes were done.\n\nTried ElevenReader - $100/yr. Speechify - $139/yr. Wasn‚Äôt paying that.\n\nI already had a TTS setup running on Runpod for another project. Plus I didn't vibe with the interface of existing apps like Readest or Paper2Aloud. So I built my own reader with iOS design in mind - clean flow, similar feel to Apple Books.\n\nWhat it does:\n\n\\- Any website - AO3, Royal Road, Wattpad, fanfic.net, whatever you read on\n\n\\- PDFs, EPUBs, text files\n\n\\- Auto-moves through chapters so you can binge without touching your phone\n\n\\- CarPlay, AirPlay, sleep timer, bookmarks, speed control\n\nVoices are solid. Not ElevenLabs tier but good for long fics.\n\nPrice at launch will be around $1.5-3/mo (\\~$18/yr). About 5x cheaper than the alternatives. Having an existing server setup helps keep costs down.\n\nRunning a free beta right now with unlimited TTS. Looking for honest feedback - bugs, missing features, anything.\n\nüîó App: https://narratorr.app\n\nüí¨ Discord: https://discord.gg/JzChuRzE\n\niOS only for now.\n\nPS: Didn't know Apple charges $99/yr for their developer program. First time doing iOS dev. That was fun to discover üõê\n\nAttached a demo vid. Lmk what you think.",
    "publish_datetime": "2026-02-06T05:03:29Z",
    "scraping_timestamp": "2026-02-06T05:46:23.116545Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "I built a 5x cheaper alternative to ElevenReader and Speechify",
      "url": "https://v.redd.it/fr49cp4z3thg1",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx8kly/i_built_a_5x_cheaper_alternative_to_elevenreader/",
      "selftext": "Hey dudes,\n\nSo I read a lot of fanfics and web novels. My eyes were done.\n\nTried ElevenReader - $100/yr. Speechify - $139/yr. Wasn‚Äôt paying that.\n\nI already had a TTS setup running on Runpod for another project. Plus I didn't vibe with the interface of existing apps like Readest or Paper2Aloud. So I built my own reader with iOS design in mind - clean flow, similar feel to Apple Books.\n\nWhat it does:\n\n\\- Any website - AO3, Royal Road, Wattpad, fanfic.net, whatever you read on\n\n\\- PDFs, EPUBs, text files\n\n\\- Auto-moves through chapters so you can binge without touching your phone\n\n\\- CarPlay, AirPlay, sleep timer, bookmarks, speed control\n\nVoices are solid. Not ElevenLabs tier but good for long fics.\n\nPrice at launch will be around $1.5-3/mo (\\~$18/yr). About 5x cheaper than the alternatives. Having an existing server setup helps keep costs down.\n\nRunning a free beta right now with unlimited TTS. Looking for honest feedback - bugs, missing features, anything.\n\nüîó App: https://narratorr.app\n\nüí¨ Discord: https://discord.gg/JzChuRzE\n\niOS only for now.\n\nPS: Didn't know Apple charges $99/yr for their developer program. First time doing iOS dev. That was fun to discover üõê\n\nAttached a demo vid. Lmk what you think.",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1770354209.0,
      "author": "CrispyDick420"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx8hfg/launched_incident_change_template_pack_for/",
    "author": "Apprehensive-Storm29",
    "title": "Launched: Incident + Change Template Pack for Sysadmins (Markdown) ‚Äî feedback welcome",
    "source": "reddit",
    "content": "I launched a small digital product under \\*\\*ChangeSafe Studio\\*\\*: a practical template pack for busy sysadmins/IT ops who are tired of rewriting the same incident + change docs.\n\nWhat‚Äôs inside (v0.2):\n- Outage status update\n- Change request (risk/rollback/validation)\n- Incident postmortem\n- Weekly status + handoff\n- Bonus: maintenance window announcement\n- 2 filled examples\n\nI‚Äôm affiliated (it‚Äôs my product). I‚Äôd love feedback from other builders on:\n- Is the positioning clear enough?\n- Any obvious improvements to the offer/packaging?\n\nI‚Äôll drop the link in a comment to keep the post feedback-first.",
    "publish_datetime": "2026-02-06T04:59:21Z",
    "scraping_timestamp": "2026-02-06T05:46:23.116888Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "Launched: Incident + Change Template Pack for Sysadmins (Markdown) ‚Äî feedback welcome",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx8hfg/launched_incident_change_template_pack_for/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx8hfg/launched_incident_change_template_pack_for/",
      "selftext": "I launched a small digital product under \\*\\*ChangeSafe Studio\\*\\*: a practical template pack for busy sysadmins/IT ops who are tired of rewriting the same incident + change docs.\n\nWhat‚Äôs inside (v0.2):\n- Outage status update\n- Change request (risk/rollback/validation)\n- Incident postmortem\n- Weekly status + handoff\n- Bonus: maintenance window announcement\n- 2 filled examples\n\nI‚Äôm affiliated (it‚Äôs my product). I‚Äôd love feedback from other builders on:\n- Is the positioning clear enough?\n- Any obvious improvements to the offer/packaging?\n\nI‚Äôll drop the link in a comment to keep the post feedback-first.",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1770353961.0,
      "author": "Apprehensive-Storm29"
    }
  },
  {
    "link": "https://v.redd.it/tay4l47f2thg1",
    "author": "CrimeGameFanatic",
    "title": "FanScouts - The voice of (Austrian) football fans",
    "source": "reddit",
    "content": "Currently I am building¬†[FanScouts](https://fanscouts.eu/)¬†a social media platform dedicated for soccer fans. Atm. I mainly support Austrian Bundesliga. Further leagues are coming soon (if the community wants).\n\nThe project stage is in beta and currently I am looking for testers and gather feedback. Looking forward for your feedback etc. Thx in advance.",
    "publish_datetime": "2026-02-06T04:54:56Z",
    "scraping_timestamp": "2026-02-06T05:46:23.117088Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "FanScouts - The voice of (Austrian) football fans",
      "url": "https://v.redd.it/tay4l47f2thg1",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx8edj/fanscouts_the_voice_of_austrian_football_fans/",
      "selftext": "Currently I am building¬†[FanScouts](https://fanscouts.eu/)¬†a social media platform dedicated for soccer fans. Atm. I mainly support Austrian Bundesliga. Further leagues are coming soon (if the community wants).\n\nThe project stage is in beta and currently I am looking for testers and gather feedback. Looking forward for your feedback etc. Thx in advance.",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770353696.0,
      "author": "CrimeGameFanatic"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx8dic/validating_my_idea_get_your_rest_apis_in_30/",
    "author": "dhruv_burada",
    "title": "Validating my Idea: Get Your REST APIs in &lt;30 seconds.",
    "source": "reddit",
    "content": "Hey everyone, I‚Äôm a full stack developer and this is my first time posting here.\n\nI‚Äôm looking for some honest input to validate an idea before I start building. I want to create a platform that allows developers to spin up a¬†**dummy REST API in less than 30 seconds.**\n\nThe goal is to make it as frictionless as possible for anyone waiting on their backend team, testers who need specific data, or people teaching/learning frontend.\n\n**The core features I‚Äôm planning:**\n\n* **Custom Schema:**¬†You define the JSON structure you need (not just generic data).\n* **Chaos Engineering:**¬†You can simulate API errors (e.g., 500s) and network delays just by adding query parameters.\n* **Dynamic Data:**¬†It returns fresh data every time you hit the endpoint.\n\nIs this something you would actually use? Or are you happy with existing tools/local mock servers?\n\nThanks in advance for the feedback!  \n\\-Dhruv Burada",
    "publish_datetime": "2026-02-06T04:53:42Z",
    "scraping_timestamp": "2026-02-06T05:46:23.117522Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "Validating my Idea: Get Your REST APIs in &lt;30 seconds.",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx8dic/validating_my_idea_get_your_rest_apis_in_30/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx8dic/validating_my_idea_get_your_rest_apis_in_30/",
      "selftext": "Hey everyone, I‚Äôm a full stack developer and this is my first time posting here.\n\nI‚Äôm looking for some honest input to validate an idea before I start building. I want to create a platform that allows developers to spin up a¬†**dummy REST API in less than 30 seconds.**\n\nThe goal is to make it as frictionless as possible for anyone waiting on their backend team, testers who need specific data, or people teaching/learning frontend.\n\n**The core features I‚Äôm planning:**\n\n* **Custom Schema:**¬†You define the JSON structure you need (not just generic data).\n* **Chaos Engineering:**¬†You can simulate API errors (e.g., 500s) and network delays just by adding query parameters.\n* **Dynamic Data:**¬†It returns fresh data every time you hit the endpoint.\n\nIs this something you would actually use? Or are you happy with existing tools/local mock servers?\n\nThanks in advance for the feedback!  \n\\-Dhruv Burada",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770353622.0,
      "author": "dhruv_burada"
    }
  },
  {
    "link": "https://v.redd.it/fm839br4tshg1",
    "author": "MercuriusTech",
    "title": "When my DJI couldn‚Äôt handle the terrain, I started building a real multi-modal drone",
    "source": "reddit",
    "content": "A few months ago I was flying my DJI in a semi-rural area doing some mapping and testing, and everything was going fine‚Ä¶ until I lost signal for a few seconds and it landed itself in tall grass near a ditch. It wasn‚Äôt ‚Äúcrashed,‚Äù but it might as well have been. I could see it on the camera feed, maybe 30 feet away, and there was absolutely nothing I could do. It couldn‚Äôt roll, it couldn‚Äôt crawl, it couldn‚Äôt recover. I had to walk out there and fish it out by hand, and the whole time I was thinking: why is this thing completely useless the moment it touches the ground?\n\nThat moment stuck with me. DJI makes great flying cameras, but they‚Äôre basically helpless outside of perfect conditions. If there‚Äôs rough terrain, obstacles, low visibility, or you can‚Äôt safely walk to it, you‚Äôre just out of luck. That frustration slowly turned into a side project where I tried to build something that wouldn‚Äôt immediately become dead weight the second it landed somewhere messy.\n\nSo I started working on a dual-mode platform that can both fly and drive, and more importantly, recover itself. The goal wasn‚Äôt ‚Äúcool factor,‚Äù it was reliability. If it lands in grass, mud, rubble, or uneven terrain, it should be able to move, reposition, and take off again without needing a human rescue mission. At the same time, I wanted it to be modular, so it could carry different payloads depending on the mission instead of being locked into one camera setup forever.\n\nSo far, we‚Äôve built it out with thermal imaging for low-visibility and night use, high-resolution RGB cameras for navigation and inspection, LiDAR for depth and obstacle mapping, optical flow sensors for low-altitude stability and near-ground positioning, and a swappable payload bay that lets us experiment with different sensor packages and hardware without redesigning the whole platform each time.\n\nAfter breaking plenty of parts along the way I now have a functioning base version, that I'm proud in, but seeing it actually crawl out of bad landings and relaunch for the first time was one of those ‚Äúokay, this might actually work‚Äù moments.\n\nThis started purely out of frustration, but it‚Äôs turned into something I‚Äôm genuinely excited to keep improving. I‚Äôm mostly posting here to share the story and get feedback from other builders who‚Äôve turned annoyance into projects.\n\nIf anyone‚Äôs curious, here‚Äôs the project page:  \n[Mercurius Technologies](https://mercuriustech.com/mercury/)\n\nWould love thoughts, criticism, or stories from anyone who‚Äôs had a similar ‚Äúthis tool just failed me‚Äù moment that pushed them to build something better.",
    "publish_datetime": "2026-02-06T04:03:11Z",
    "scraping_timestamp": "2026-02-06T05:46:23.118694Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 9,
    "num_comments": 0,
    "engagement_score": 9.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "When my DJI couldn‚Äôt handle the terrain, I started building a real multi-modal drone",
      "url": "https://v.redd.it/fm839br4tshg1",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx7de5/when_my_dji_couldnt_handle_the_terrain_i_started/",
      "selftext": "A few months ago I was flying my DJI in a semi-rural area doing some mapping and testing, and everything was going fine‚Ä¶ until I lost signal for a few seconds and it landed itself in tall grass near a ditch. It wasn‚Äôt ‚Äúcrashed,‚Äù but it might as well have been. I could see it on the camera feed, maybe 30 feet away, and there was absolutely nothing I could do. It couldn‚Äôt roll, it couldn‚Äôt crawl, it couldn‚Äôt recover. I had to walk out there and fish it out by hand, and the whole time I was thinking: why is this thing completely useless the moment it touches the ground?\n\nThat moment stuck with me. DJI makes great flying cameras, but they‚Äôre basically helpless outside of perfect conditions. If there‚Äôs rough terrain, obstacles, low visibility, or you can‚Äôt safely walk to it, you‚Äôre just out of luck. That frustration slowly turned into a side project where I tried to build something that wouldn‚Äôt immediately become dead weight the second it landed somewhere messy.\n\nSo I started working on a dual-mode platform that can both fly and drive, and more importantly, recover itself. The goal wasn‚Äôt ‚Äúcool factor,‚Äù it was reliability. If it lands in grass, mud, rubble, or uneven terrain, it should be able to move, reposition, and take off again without needing a human rescue mission. At the same time, I wanted it to be modular, so it could carry different payloads depending on the mission instead of being locked into one camera setup forever.\n\nSo far, we‚Äôve built it out with thermal imaging for low-visibility and night use, high-resolution RGB cameras for navigation and inspection, LiDAR for depth and obstacle mapping, optical flow sensors for low-altitude stability and near-ground positioning, and a swappable payload bay that lets us experiment with different sensor packages and hardware without redesigning the whole platform each time.\n\nAfter breaking plenty of parts along the way I now have a functioning base version, that I'm proud in, but seeing it actually crawl out of bad landings and relaunch for the first time was one of those ‚Äúokay, this might actually work‚Äù moments.\n\nThis started purely out of frustration, but it‚Äôs turned into something I‚Äôm genuinely excited to keep improving. I‚Äôm mostly posting here to share the story and get feedback from other builders who‚Äôve turned annoyance into projects.\n\nIf anyone‚Äôs curious, here‚Äôs the project page:  \n[Mercurius Technologies](https://mercuriustech.com/mercury/)\n\nWould love thoughts, criticism, or stories from anyone who‚Äôs had a similar ‚Äúthis tool just failed me‚Äù moment that pushed them to build something better.",
      "score": 9,
      "num_comments": 0,
      "created_utc": 1770350591.0,
      "author": "MercuriusTech"
    }
  },
  {
    "link": "https://v.redd.it/uyns49axpshg1",
    "author": "Entire-Job-8483",
    "title": "I built a Windows app to find space‚Äëhogging videos and compress them locally",
    "source": "reddit",
    "content": "CineCinch helps identify video files that take up more space than they should, then compresses them locally without losing quality.\n\nIt scans a library folder, lists all videos and automatically ranks them from \"bloated\" to \"efficient,\" surfacing the ones most worth compressing. You can then batch‚Äëcompress everything in one go (no uploads, no data collection) using one of four simple preset modes. This saves a lot of time compared to finding the worst offenders and handling compression one at a time.\n\nI originally built this to clean up my own video library, but it grew into something I wanted to polish and share. I know there are plenty of reencoding tools out there already, but I wanted something simple, time saving, privacy respecting, and focused on helping you figure out where to start.\n\nIf you want to take a look:\n\nWebsite:¬†[https://cinecinch.com](https://cinecinch.com/)\n\nMicrosoft Store:¬†[https://apps.microsoft.com/detail/9pb21kbs42ms](https://apps.microsoft.com/detail/9pb21kbs42ms)\n\nIf you try it, I‚Äôd appreciate any feedback, especially from people with large or messy video libraries looking to declutter.",
    "publish_datetime": "2026-02-06T03:45:18Z",
    "scraping_timestamp": "2026-02-06T05:46:23.119267Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 3,
    "num_comments": 1,
    "engagement_score": 5.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "I built a Windows app to find space‚Äëhogging videos and compress them locally",
      "url": "https://v.redd.it/uyns49axpshg1",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx6zzz/i_built_a_windows_app_to_find_spacehogging_videos/",
      "selftext": "CineCinch helps identify video files that take up more space than they should, then compresses them locally without losing quality.\n\nIt scans a library folder, lists all videos and automatically ranks them from \"bloated\" to \"efficient,\" surfacing the ones most worth compressing. You can then batch‚Äëcompress everything in one go (no uploads, no data collection) using one of four simple preset modes. This saves a lot of time compared to finding the worst offenders and handling compression one at a time.\n\nI originally built this to clean up my own video library, but it grew into something I wanted to polish and share. I know there are plenty of reencoding tools out there already, but I wanted something simple, time saving, privacy respecting, and focused on helping you figure out where to start.\n\nIf you want to take a look:\n\nWebsite:¬†[https://cinecinch.com](https://cinecinch.com/)\n\nMicrosoft Store:¬†[https://apps.microsoft.com/detail/9pb21kbs42ms](https://apps.microsoft.com/detail/9pb21kbs42ms)\n\nIf you try it, I‚Äôd appreciate any feedback, especially from people with large or messy video libraries looking to declutter.",
      "score": 3,
      "num_comments": 1,
      "created_utc": 1770349518.0,
      "author": "Entire-Job-8483"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx6t9l/i_kept_forgetting_what_i_shipped_so_i_built_a/",
    "author": "Formal_Initiative645",
    "title": "I kept forgetting what I shipped, so I built a widget that does it for me",
    "source": "reddit",
    "content": "Shipping was fast. Explaining what changed wasn‚Äôt so I built a widget to handle it. üî•\n\nI ship features, fix bugs, refactor‚Ä¶ and then spent too much time finding the major changes  and **summarizing them to keep users update to date.**\n\nSo I built SumGit.\n\nIt connects to your Git repo, summarizes meaningful changes, and lets you embed a live updates widget on your site of highlights/milestones.\n\nOnce it‚Äôs embedded, it stays up to date automatically  **no manual posts, no stale changelog pages.**\n\nI‚Äôm using it on my own projects to show that things are actively being worked on.\n\nIf you‚Äôre shipping often but not communicating it well, this might be useful.\n\nHappy to answer questions or hear feedback.",
    "publish_datetime": "2026-02-06T03:36:14Z",
    "scraping_timestamp": "2026-02-06T05:46:23.119694Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 2,
    "num_comments": 2,
    "engagement_score": 6.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "I kept forgetting what I shipped, so I built a widget that does it for me",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx6t9l/i_kept_forgetting_what_i_shipped_so_i_built_a/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx6t9l/i_kept_forgetting_what_i_shipped_so_i_built_a/",
      "selftext": "Shipping was fast. Explaining what changed wasn‚Äôt so I built a widget to handle it. üî•\n\nI ship features, fix bugs, refactor‚Ä¶ and then spent too much time finding the major changes  and **summarizing them to keep users update to date.**\n\nSo I built SumGit.\n\nIt connects to your Git repo, summarizes meaningful changes, and lets you embed a live updates widget on your site of highlights/milestones.\n\nOnce it‚Äôs embedded, it stays up to date automatically  **no manual posts, no stale changelog pages.**\n\nI‚Äôm using it on my own projects to show that things are actively being worked on.\n\nIf you‚Äôre shipping often but not communicating it well, this might be useful.\n\nHappy to answer questions or hear feedback.",
      "score": 2,
      "num_comments": 2,
      "created_utc": 1770348974.0,
      "author": "Formal_Initiative645"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx5wbm/do_sentiment_analysis_platforms_have_any_value/",
    "author": "andygohome",
    "title": "do sentiment analysis platforms have any value?",
    "source": "reddit",
    "content": "I have built a side project (stonkup.com) where users can track news and reddit posts for their desired stocks, it is still in draft form.  The main value that this service brings is that users can set up threshold for sentiment and receive auto notifications when sentiment score for a particular post crosses the set threshold. I asked some of friends if this idea has any promise, but they told me that this is a bad idea since trading or investing based on the news is useless. \n\nNow I‚Äôm hesitant to continue since investment in news feeds is pricey. Want to know your opinion",
    "publish_datetime": "2026-02-06T02:53:16Z",
    "scraping_timestamp": "2026-02-06T05:46:23.119990Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 2,
    "engagement_score": 4.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "do sentiment analysis platforms have any value?",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx5wbm/do_sentiment_analysis_platforms_have_any_value/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx5wbm/do_sentiment_analysis_platforms_have_any_value/",
      "selftext": "I have built a side project (stonkup.com) where users can track news and reddit posts for their desired stocks, it is still in draft form.  The main value that this service brings is that users can set up threshold for sentiment and receive auto notifications when sentiment score for a particular post crosses the set threshold. I asked some of friends if this idea has any promise, but they told me that this is a bad idea since trading or investing based on the news is useless. \n\nNow I‚Äôm hesitant to continue since investment in news feeds is pricey. Want to know your opinion",
      "score": 0,
      "num_comments": 2,
      "created_utc": 1770346396.0,
      "author": "andygohome"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx5sxp/completed_my_first_android_ios_app_using_flutter/",
    "author": "modernlogictech",
    "title": "Completed my first Android + iOS app using flutter [BiblioFuse Reader] and learned a lot",
    "source": "reddit",
    "content": "The quick take away:  \n\\- Flutter is great for cross platform app creation for simple app  \n\\- Upon features touching complex file manipulation or device hardware integration, use native code (kotlin/ swift) might be easier.  \n\\- So if you have an interesting app with amazing features, check first if you need native coding to make it works. One platform at a time may be better.\n\nI am new to coding world, comment if you have any tips.\n\nCheck out my app\n\n\\[[Play Store Link](https://play.google.com/store/apps/details?id=com.MLOGICTECH.bibliofusereader&amp;hl=en-US&amp;ah=423jBOeRoug68zOF2xwCeFuKVQQ)\\]\n\n\\[[App Store Link](https://apps.apple.com/kw/app/bibliofuse-reader-compress/id6758330093)\\]",
    "publish_datetime": "2026-02-06T02:49:01Z",
    "scraping_timestamp": "2026-02-06T05:46:23.120342Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "Completed my first Android + iOS app using flutter [BiblioFuse Reader] and learned a lot",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx5sxp/completed_my_first_android_ios_app_using_flutter/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx5sxp/completed_my_first_android_ios_app_using_flutter/",
      "selftext": "The quick take away:  \n\\- Flutter is great for cross platform app creation for simple app  \n\\- Upon features touching complex file manipulation or device hardware integration, use native code (kotlin/ swift) might be easier.  \n\\- So if you have an interesting app with amazing features, check first if you need native coding to make it works. One platform at a time may be better.\n\nI am new to coding world, comment if you have any tips.\n\nCheck out my app\n\n\\[[Play Store Link](https://play.google.com/store/apps/details?id=com.MLOGICTECH.bibliofusereader&amp;hl=en-US&amp;ah=423jBOeRoug68zOF2xwCeFuKVQQ)\\]\n\n\\[[App Store Link](https://apps.apple.com/kw/app/bibliofuse-reader-compress/id6758330093)\\]",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770346141.0,
      "author": "modernlogictech"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx5rz1/whats_the_simplest_way_youve_found_to_stay/",
    "author": "AbleBarracuda9113",
    "title": "What‚Äôs the simplest way you‚Äôve found to stay tax-ready as a solo business?",
    "source": "reddit",
    "content": "I‚Äôm not talking about ‚Äúperfect bookkeeping,‚Äù just the minimum system that prevents chaos. For me it‚Äôs: collect receipts ‚Üí weekly sweep ‚Üí export a CSV ‚Üí done. Curious what other people do that‚Äôs actually sustainable (not a once-a-year panic).  \n",
    "publish_datetime": "2026-02-06T02:47:49Z",
    "scraping_timestamp": "2026-02-06T05:46:23.120513Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "What‚Äôs the simplest way you‚Äôve found to stay tax-ready as a solo business?",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx5rz1/whats_the_simplest_way_youve_found_to_stay/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx5rz1/whats_the_simplest_way_youve_found_to_stay/",
      "selftext": "I‚Äôm not talking about ‚Äúperfect bookkeeping,‚Äù just the minimum system that prevents chaos. For me it‚Äôs: collect receipts ‚Üí weekly sweep ‚Üí export a CSV ‚Üí done. Curious what other people do that‚Äôs actually sustainable (not a once-a-year panic).  \n",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770346069.0,
      "author": "AbleBarracuda9113"
    }
  },
  {
    "link": "https://ransomleak.com/exercises/clawdbot-prompt-injection",
    "author": "Fr1l0ck",
    "title": "I built an interactive 3D security awareness exercises, looking for feedback",
    "source": "reddit",
    "content": "I am trying to improve the UX as much as possible to make sure the learning experience is flawless. What would you suggest?",
    "publish_datetime": "2026-02-06T02:07:49Z",
    "scraping_timestamp": "2026-02-06T05:46:23.120621Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 2,
    "engagement_score": 5.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "I built an interactive 3D security awareness exercises, looking for feedback",
      "url": "https://ransomleak.com/exercises/clawdbot-prompt-injection",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx4wgr/i_built_an_interactive_3d_security_awareness/",
      "selftext": "I am trying to improve the UX as much as possible to make sure the learning experience is flawless. What would you suggest?",
      "score": 1,
      "num_comments": 2,
      "created_utc": 1770343669.0,
      "author": "Fr1l0ck"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx4t2i/freelance_it_todays_winners_losers/",
    "author": "Ecstatic-Anything797",
    "title": "Freelance IT: Today‚Äôs Winners &amp; Losers",
    "source": "reddit",
    "content": "With AI everywhere, which IT and software engineering freelance services are people actually needing, demanding, and paying for today?\n\nWhich ones are dying out?\n\nFeel free to share what‚Äôs happening in your area.",
    "publish_datetime": "2026-02-06T02:03:33Z",
    "scraping_timestamp": "2026-02-06T05:46:23.120764Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 5,
    "num_comments": 3,
    "engagement_score": 11.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "Freelance IT: Today‚Äôs Winners &amp; Losers",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx4t2i/freelance_it_todays_winners_losers/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx4t2i/freelance_it_todays_winners_losers/",
      "selftext": "With AI everywhere, which IT and software engineering freelance services are people actually needing, demanding, and paying for today?\n\nWhich ones are dying out?\n\nFeel free to share what‚Äôs happening in your area.",
      "score": 5,
      "num_comments": 3,
      "created_utc": 1770343413.0,
      "author": "Ecstatic-Anything797"
    }
  },
  {
    "link": "https://fmrizigaming.com/en/",
    "author": "Slow_Opportunity_285",
    "title": "So I decided to build a gaming blog",
    "source": "reddit",
    "content": "Some time ago I decided to build my own website for gaming, I also started creating videos for different. I am still trying to grow my audience, then I found this place. I would like to share if someone is interested in gaming, found me as FMRIZI Gaming",
    "publish_datetime": "2026-02-06T01:48:05Z",
    "scraping_timestamp": "2026-02-06T05:46:23.120913Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "So I decided to build a gaming blog",
      "url": "https://fmrizigaming.com/en/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx4gno/so_i_decided_to_build_a_gaming_blog/",
      "selftext": "Some time ago I decided to build my own website for gaming, I also started creating videos for different. I am still trying to grow my audience, then I found this place. I would like to share if someone is interested in gaming, found me as FMRIZI Gaming",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1770342485.0,
      "author": "Slow_Opportunity_285"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx4c8k/generate_mobile_designs_that_actually_work_in/",
    "author": "kempotai",
    "title": "Generate Mobile Designs That Actually Work in Production",
    "source": "reddit",
    "content": "I built an app which generates production-oriented mobile UI/UX designs using structured design logic (Design-OS driven), optimized for real apps rather than concept demos. Focused on consistency, spacing systems, component logic, and developer-friendly output. It has been released on ProductHunt in a couple of days ago and the app name is [Dolfy](https://www.dolfy.ai/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=subreddit_tracking&amp;utm_content=SideProject). New users gets 2000 credits and it is mostly enough for generating 2-3 screens.",
    "publish_datetime": "2026-02-06T01:42:23Z",
    "scraping_timestamp": "2026-02-06T05:46:23.121175Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 2,
    "num_comments": 0,
    "engagement_score": 2.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "Generate Mobile Designs That Actually Work in Production",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx4c8k/generate_mobile_designs_that_actually_work_in/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx4c8k/generate_mobile_designs_that_actually_work_in/",
      "selftext": "I built an app which generates production-oriented mobile UI/UX designs using structured design logic (Design-OS driven), optimized for real apps rather than concept demos. Focused on consistency, spacing systems, component logic, and developer-friendly output. It has been released on ProductHunt in a couple of days ago and the app name is [Dolfy](https://www.dolfy.ai/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=subreddit_tracking&amp;utm_content=SideProject). New users gets 2000 credits and it is mostly enough for generating 2-3 screens.",
      "score": 2,
      "num_comments": 0,
      "created_utc": 1770342143.0,
      "author": "kempotai"
    }
  },
  {
    "link": "https://www.reddit.com/r/SideProject/comments/1qx46ok/i_made_a_web_app_to_keep_your_docs_in_sync_with/",
    "author": "TheNightIsFalling",
    "title": "I made a web app to keep your docs in sync with code automatically",
    "source": "reddit",
    "content": "https://reddit.com/link/1qx46ok/video/gpsojn9m2shg1/player\n\nHey everyone,\n\nI got tired of my technical docs going out of date when I made changes to my code. That's what prompted me to make [Cartara](https://cartara.ai/).\n\nAll you have to do is tag the files you want Cartara to track. Whenever those files change, it automatically creates or updates your documentation for you. It runs in your CI/CD pipeline, or you can refresh everything from the dashboard in the UI.\n\nLet me know what you all think!",
    "publish_datetime": "2026-02-06T01:35:17Z",
    "scraping_timestamp": "2026-02-06T05:46:23.121433Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 2,
    "num_comments": 4,
    "engagement_score": 10.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "I made a web app to keep your docs in sync with code automatically",
      "url": "https://www.reddit.com/r/SideProject/comments/1qx46ok/i_made_a_web_app_to_keep_your_docs_in_sync_with/",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx46ok/i_made_a_web_app_to_keep_your_docs_in_sync_with/",
      "selftext": "https://reddit.com/link/1qx46ok/video/gpsojn9m2shg1/player\n\nHey everyone,\n\nI got tired of my technical docs going out of date when I made changes to my code. That's what prompted me to make [Cartara](https://cartara.ai/).\n\nAll you have to do is tag the files you want Cartara to track. Whenever those files change, it automatically creates or updates your documentation for you. It runs in your CI/CD pipeline, or you can refresh everything from the dashboard in the UI.\n\nLet me know what you all think!",
      "score": 2,
      "num_comments": 4,
      "created_utc": 1770341717.0,
      "author": "TheNightIsFalling"
    }
  },
  {
    "link": "https://v.redd.it/6764yjog2shg1",
    "author": "danicius",
    "title": "I made Nail Recipe app noüíÖüèΩ",
    "source": "reddit",
    "content": "Hi everyone! I‚Äôm a hobby nail artist who‚Äôs been doing gel nails since 2022, and I kept losing track of my designs, products, materials, and techniques to achieve a certain look or to maintain nail health.\n\nTo solve my own problem, I built a small app that helps me:  \n\n‚Ä¢ Log my gel polishes and nail art supplies  \n\n‚Ä¢ Attach photos + step‚Äëby‚Äëstep notes for each set I have done.\n\nFeatures: \n\n‚Ä¢ Picture Library with recipe inside, with CRUD, filters and tags  \n\n‚Ä¢ Material Library, keep track of your products\n\n‚Ä¢ Share Nail recipe to friends by PDF with images attached to them\n\n‚Ä¢ Import/Export you entire nail recipe app history \n\n‚Ä¢ Cute themes \n\nSo far so good, but it‚Äôs mostly catered to my issues I come across as a nail hobbyist. But it‚Äôs a good note taking app specifically catered towards nails!\n\nI would appreciate any questions, feedback or if interested in testing let me know im trying to figure it out as my first app. ",
    "publish_datetime": "2026-02-06T01:33:10Z",
    "scraping_timestamp": "2026-02-06T05:46:23.121925Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 2,
    "num_comments": 0,
    "engagement_score": 2.0,
    "subreddit": "SideProject",
    "raw": {
      "subreddit": "SideProject",
      "title": "I made Nail Recipe app noüíÖüèΩ",
      "url": "https://v.redd.it/6764yjog2shg1",
      "permalink": "https://www.reddit.com/r/SideProject/comments/1qx451f/i_made_nail_recipe_app_no/",
      "selftext": "Hi everyone! I‚Äôm a hobby nail artist who‚Äôs been doing gel nails since 2022, and I kept losing track of my designs, products, materials, and techniques to achieve a certain look or to maintain nail health.\n\nTo solve my own problem, I built a small app that helps me:  \n\n‚Ä¢ Log my gel polishes and nail art supplies  \n\n‚Ä¢ Attach photos + step‚Äëby‚Äëstep notes for each set I have done.\n\nFeatures: \n\n‚Ä¢ Picture Library with recipe inside, with CRUD, filters and tags  \n\n‚Ä¢ Material Library, keep track of your products\n\n‚Ä¢ Share Nail recipe to friends by PDF with images attached to them\n\n‚Ä¢ Import/Export you entire nail recipe app history \n\n‚Ä¢ Cute themes \n\nSo far so good, but it‚Äôs mostly catered to my issues I come across as a nail hobbyist. But it‚Äôs a good note taking app specifically catered towards nails!\n\nI would appreciate any questions, feedback or if interested in testing let me know im trying to figure it out as my first app. ",
      "score": 2,
      "num_comments": 0,
      "created_utc": 1770341590.0,
      "author": "danicius"
    }
  }
]