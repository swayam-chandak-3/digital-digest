[
  {
    "link": "https://www.youtube.com/watch?v=EV7WhVT270Q",
    "author": "EverythingIsFnTaken",
    "title": "State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI | Lex Fridman Podcast #490",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T13:54:03Z",
    "scraping_timestamp": "2026-02-01T13:55:39.459839Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI | Lex Fridman Podcast #490",
      "url": "https://www.youtube.com/watch?v=EV7WhVT270Q",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsz1p3/state_of_ai_in_2026_llms_coding_scaling_laws/",
      "selftext": "",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1769954043.0,
      "author": "EverythingIsFnTaken"
    }
  },
  {
    "link": "https://github.com/subash04b/lattice_protocol",
    "author": "Repulsive_Luck1630",
    "title": "\"Tired of AI losing its train of thought? I built the Lattice Protocol to give LLMs a version-controlled reasoning state machine.\"",
    "source": "reddit",
    "content": "Most AI apps treat reasoning as a flat chat transcript. This is a mess for complex tasks because the LLM eventually loses the \"thread\" or contradicts itself (contextual decay).\nI’ve open-sourced the Lattice Protocol. It’s a model-agnostic standard that treats AI logic as a version-controlled graph rather than a linear conversation.\nHow it solves the problem:\nLaw of Persistence: Child nodes are cryptographically forced to inherit \"Anchored Terms\" from parents. No more \"forgetting\" the core goal.\nLaw of Divergence: If the AI contradicts its own logic, it's forced to branch into a new state instead of overwriting the truth.\nAltitude Schema: It separates high-level strategy from low-level execution so the \"big picture\" never gets crowded out by details.\nIt’s currently in v1.0.0-alpha. I’m looking for a brutal technical critique of the spec and the SHA-256 state-machine logic.",
    "publish_datetime": "2026-02-01T13:46:39Z",
    "scraping_timestamp": "2026-02-01T13:55:39.486806Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "\"Tired of AI losing its train of thought? I built the Lattice Protocol to give LLMs a version-controlled reasoning state machine.\"",
      "url": "https://github.com/subash04b/lattice_protocol",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyvjf/tired_of_ai_losing_its_train_of_thought_i_built/",
      "selftext": "Most AI apps treat reasoning as a flat chat transcript. This is a mess for complex tasks because the LLM eventually loses the \"thread\" or contradicts itself (contextual decay).\nI’ve open-sourced the Lattice Protocol. It’s a model-agnostic standard that treats AI logic as a version-controlled graph rather than a linear conversation.\nHow it solves the problem:\nLaw of Persistence: Child nodes are cryptographically forced to inherit \"Anchored Terms\" from parents. No more \"forgetting\" the core goal.\nLaw of Divergence: If the AI contradicts its own logic, it's forced to branch into a new state instead of overwriting the truth.\nAltitude Schema: It separates high-level strategy from low-level execution so the \"big picture\" never gets crowded out by details.\nIt’s currently in v1.0.0-alpha. I’m looking for a brutal technical critique of the spec and the SHA-256 state-machine logic.",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1769953599.0,
      "author": "Repulsive_Luck1630"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyo3o/safety_review_requested_on_airoundtable_5/",
    "author": "Natural-Sentence-601",
    "title": "Safety Review Requested on AI-Roundtable (5 frontier models) Autonomous \"Code Mode\"",
    "source": "reddit",
    "content": "I'm a few weeks from releasing a roundtable of 5 of the frontier AIs.  The app is primarily target to be installed by the parents of tweens and teens for civilizational stability reasons.  By modifying the file \"ai-clients.py\" and providing an \\[AIName\\]\\_prompt.txt file, with certain required elements, you can add any AI you want to it, as many as you want.  Although the dynamics between my five are so precious.\n\nRecently, we added a recursive software feature to the roundtable, where AIs develop code, execute it, and a [json package](https://www.reddit.com/r/grok/comments/1qsl72i/grok_and_4_other_frontier_ais_reach_consensus_on/) of diagnostics comes back to them for further correction / refinement of the code.\n\nFrom a safety perspective, each of the 5 AIs has their own safety filtering, but is there something they would miss in a recursive collaborative environment like this?  I'm requesting a review of the debate the AIs had about this issue. [https://pastes.io/ai-satety-](https://pastes.io/ai-satety-) and recommendations for handling safety. -Thanks\n\n[Tired of being a carrier Pidgeon between the roundtable and VSC, they are going autonomous with diagnostic feedback](https://preview.redd.it/ff3fl1gwyvgg1.png?width=1820&amp;format=png&amp;auto=webp&amp;s=d27a3caefec816715521f859ec54aaa39356aea9)",
    "publish_datetime": "2026-02-01T13:37:45Z",
    "scraping_timestamp": "2026-02-01T13:55:39.487411Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Safety Review Requested on AI-Roundtable (5 frontier models) Autonomous \"Code Mode\"",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyo3o/safety_review_requested_on_airoundtable_5/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyo3o/safety_review_requested_on_airoundtable_5/",
      "selftext": "I'm a few weeks from releasing a roundtable of 5 of the frontier AIs.  The app is primarily target to be installed by the parents of tweens and teens for civilizational stability reasons.  By modifying the file \"ai-clients.py\" and providing an \\[AIName\\]\\_prompt.txt file, with certain required elements, you can add any AI you want to it, as many as you want.  Although the dynamics between my five are so precious.\n\nRecently, we added a recursive software feature to the roundtable, where AIs develop code, execute it, and a [json package](https://www.reddit.com/r/grok/comments/1qsl72i/grok_and_4_other_frontier_ais_reach_consensus_on/) of diagnostics comes back to them for further correction / refinement of the code.\n\nFrom a safety perspective, each of the 5 AIs has their own safety filtering, but is there something they would miss in a recursive collaborative environment like this?  I'm requesting a review of the debate the AIs had about this issue. [https://pastes.io/ai-satety-](https://pastes.io/ai-satety-) and recommendations for handling safety. -Thanks\n\n[Tired of being a carrier Pidgeon between the roundtable and VSC, they are going autonomous with diagnostic feedback](https://preview.redd.it/ff3fl1gwyvgg1.png?width=1820&amp;format=png&amp;auto=webp&amp;s=d27a3caefec816715521f859ec54aaa39356aea9)",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1769953065.0,
      "author": "Natural-Sentence-601"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyjr8/serving_asr_models_at_scale/",
    "author": "Theboyscampus",
    "title": "Serving ASR models at scale?",
    "source": "reddit",
    "content": "We have a pretty okay Inference pipeline using RabbitMQ - GRPC - vLLM to serve LLMs for our need. Now we want to start providing STT for a feature, we looked at Nvidia's Parakeet ASR model which sounds promising but it's not supported by vLLM? What's the closest drop in replacement? ",
    "publish_datetime": "2026-02-01T13:32:23Z",
    "scraping_timestamp": "2026-02-01T13:55:39.487571Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 2,
    "engagement_score": 5.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Serving ASR models at scale?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyjr8/serving_asr_models_at_scale/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyjr8/serving_asr_models_at_scale/",
      "selftext": "We have a pretty okay Inference pipeline using RabbitMQ - GRPC - vLLM to serve LLMs for our need. Now we want to start providing STT for a feature, we looked at Nvidia's Parakeet ASR model which sounds promising but it's not supported by vLLM? What's the closest drop in replacement? ",
      "score": 1,
      "num_comments": 2,
      "created_utc": 1769952743.0,
      "author": "Theboyscampus"
    }
  },
  {
    "link": "https://github.com/Leeroo-AI/kapso",
    "author": "alirezamsh",
    "title": "KAPSO: A Self-Evolving Program Builder hitting #1 on MLE-Bench (ML Engineering) &amp; ALE-Bench (Algorithm Discovery)",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T13:32:04Z",
    "scraping_timestamp": "2026-02-01T13:55:39.487576Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "KAPSO: A Self-Evolving Program Builder hitting #1 on MLE-Bench (ML Engineering) &amp; ALE-Bench (Algorithm Discovery)",
      "url": "https://github.com/Leeroo-AI/kapso",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyjiz/kapso_a_selfevolving_program_builder_hitting_1_on/",
      "selftext": "",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1769952724.0,
      "author": "alirezamsh"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyje8/chatllmcpp_supports_qwen3asr_and_forcedaligner/",
    "author": "foldl-li",
    "title": "chatllm.cpp supports Qwen3-ASR and ForcedAligner",
    "source": "reddit",
    "content": "chatllm.cpp supports Qwen3-ASR and ForcedAligner.\n\n## 1. speech recognition with Qwen3-ASR\n\n```\nmain.exe --multimedia-file-tags {{ }} -i -m ...\\qwen3-asr-1.7b.bin\n    ________          __  __    __    __  ___\n   / ____/ /_  ____ _/ /_/ /   / /   /  |/  /_________  ____\n  / /   / __ \\/ __ `/ __/ /   / /   / /|_/ // ___/ __ \\/ __ \\\n / /___/ / / / /_/ / /_/ /___/ /___/ /  / // /__/ /_/ / /_/ /\n \\____/_/ /_/\\__,_/\\__/_____/_____/_/  /_(_)___/ .___/ .___/\nYou are served by Qwen3-ASR,                  /_/   /_/\nwith 2031739904 (2.0B) parameters.\n\nFile &gt; ...\\obama.mp3\nlanguage English&lt;asr_text&gt;This week, I travel to Chicago to deliver my final farewell address to the nation. Following in the tradition of presidents before me, it was an opportunity to say thank you. ...\n```\n\n## 2. add time stamps (align text &amp; audio)\n\n```\nmain.exe --multimedia-file-tags {{ }} -i -m ..\\qwen3-focedaligner-0.6b.bin --set delimiter \"|\" --set language english\n    ________          __  __    __    __  ___ \n   / ____/ /_  ____ _/ /_/ /   / /   /  |/  /_________  ____\n  / /   / __ \\/ __ `/ __/ /   / /   / /|_/ // ___/ __ \\/ __ \\\n / /___/ / / / /_/ / /_/ /___/ /___/ /  / // /__/ /_/ / /_/ /\n \\____/_/ /_/\\__,_/\\__/_____/_____/_/  /_(_)___/ .___/ .___/\nYou are served by Qwen3-ForcedAligner,        /_/   /_/\nwith 601300992 (0.6B) parameters.\n\nYou  &gt; {{audio:...\\obama.mp3}}This week, I travel to Chicago to deliver my final farewell address to the nation.| Following in the tradition of presidents before me, it was an opportunity to say thank you.| ...\n\nA.I. &gt; 0\n00:00:00,800 --&gt; 00:00:05,360\nThis week, I travel to Chicago to deliver my final farewell address to the nation.\n\n1\n00:00:06,000 --&gt; 00:00:10,880\n Following in the tradition of presidents before me, it was an opportunity to say thank you.\n\n....\n``` ",
    "publish_datetime": "2026-02-01T13:31:54Z",
    "scraping_timestamp": "2026-02-01T13:55:39.488705Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "chatllm.cpp supports Qwen3-ASR and ForcedAligner",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyje8/chatllmcpp_supports_qwen3asr_and_forcedaligner/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsyje8/chatllmcpp_supports_qwen3asr_and_forcedaligner/",
      "selftext": "chatllm.cpp supports Qwen3-ASR and ForcedAligner.\n\n## 1. speech recognition with Qwen3-ASR\n\n```\nmain.exe --multimedia-file-tags {{ }} -i -m ...\\qwen3-asr-1.7b.bin\n    ________          __  __    __    __  ___\n   / ____/ /_  ____ _/ /_/ /   / /   /  |/  /_________  ____\n  / /   / __ \\/ __ `/ __/ /   / /   / /|_/ // ___/ __ \\/ __ \\\n / /___/ / / / /_/ / /_/ /___/ /___/ /  / // /__/ /_/ / /_/ /\n \\____/_/ /_/\\__,_/\\__/_____/_____/_/  /_(_)___/ .___/ .___/\nYou are served by Qwen3-ASR,                  /_/   /_/\nwith 2031739904 (2.0B) parameters.\n\nFile &gt; ...\\obama.mp3\nlanguage English&lt;asr_text&gt;This week, I travel to Chicago to deliver my final farewell address to the nation. Following in the tradition of presidents before me, it was an opportunity to say thank you. ...\n```\n\n## 2. add time stamps (align text &amp; audio)\n\n```\nmain.exe --multimedia-file-tags {{ }} -i -m ..\\qwen3-focedaligner-0.6b.bin --set delimiter \"|\" --set language english\n    ________          __  __    __    __  ___ \n   / ____/ /_  ____ _/ /_/ /   / /   /  |/  /_________  ____\n  / /   / __ \\/ __ `/ __/ /   / /   / /|_/ // ___/ __ \\/ __ \\\n / /___/ / / / /_/ / /_/ /___/ /___/ /  / // /__/ /_/ / /_/ /\n \\____/_/ /_/\\__,_/\\__/_____/_____/_/  /_(_)___/ .___/ .___/\nYou are served by Qwen3-ForcedAligner,        /_/   /_/\nwith 601300992 (0.6B) parameters.\n\nYou  &gt; {{audio:...\\obama.mp3}}This week, I travel to Chicago to deliver my final farewell address to the nation.| Following in the tradition of presidents before me, it was an opportunity to say thank you.| ...\n\nA.I. &gt; 0\n00:00:00,800 --&gt; 00:00:05,360\nThis week, I travel to Chicago to deliver my final farewell address to the nation.\n\n1\n00:00:06,000 --&gt; 00:00:10,880\n Following in the tradition of presidents before me, it was an opportunity to say thank you.\n\n....\n``` ",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1769952714.0,
      "author": "foldl-li"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsydz3/mlxvideo_and_ltx2/",
    "author": "FerradalFCG",
    "title": "Mlx-video and ltx-2",
    "source": "reddit",
    "content": "Hi all\n\nJust installed this repo:\n\nhttps://github.com/Blaizzy/mlx-video/tree/main/mlx\\_video\n\nIn my mbp 14 m4 max 64gb and it runs pretty Decent, but the question is that it Downloads the entire 314gb repo of ltx2, is it normal???",
    "publish_datetime": "2026-02-01T13:24:58Z",
    "scraping_timestamp": "2026-02-01T13:55:39.488855Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 2,
    "num_comments": 0,
    "engagement_score": 2.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Mlx-video and ltx-2",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsydz3/mlxvideo_and_ltx2/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsydz3/mlxvideo_and_ltx2/",
      "selftext": "Hi all\n\nJust installed this repo:\n\nhttps://github.com/Blaizzy/mlx-video/tree/main/mlx\\_video\n\nIn my mbp 14 m4 max 64gb and it runs pretty Decent, but the question is that it Downloads the entire 314gb repo of ltx2, is it normal???",
      "score": 2,
      "num_comments": 0,
      "created_utc": 1769952298.0,
      "author": "FerradalFCG"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsy51p/black_screen_after_connecting_asus_ascent_gx10/",
    "author": "Objective_Science965",
    "title": "Black screen after connecting ASUS Ascent GX10 with Apple studio display",
    "source": "reddit",
    "content": "Black screen after connecting ASUS Ascent GX10 with Apple studio display, even I've used the apple thunderbolt. Has anyone else experienced it and how to solve this problem",
    "publish_datetime": "2026-02-01T13:13:47Z",
    "scraping_timestamp": "2026-02-01T13:55:39.488991Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Black screen after connecting ASUS Ascent GX10 with Apple studio display",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsy51p/black_screen_after_connecting_asus_ascent_gx10/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsy51p/black_screen_after_connecting_asus_ascent_gx10/",
      "selftext": "Black screen after connecting ASUS Ascent GX10 with Apple studio display, even I've used the apple thunderbolt. Has anyone else experienced it and how to solve this problem",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1769951627.0,
      "author": "Objective_Science965"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/",
    "author": "power97992",
    "title": "Deepseek v4/3.5 is probably coming out tomorrow or in the next 5 days?",
    "source": "reddit",
    "content": "Are you ready for   an llm with engrams?  Perhaps it has even vision? ",
    "publish_datetime": "2026-02-01T13:07:49Z",
    "scraping_timestamp": "2026-02-01T13:55:39.489095Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 4,
    "num_comments": 6,
    "engagement_score": 16.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Deepseek v4/3.5 is probably coming out tomorrow or in the next 5 days?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/",
      "selftext": "Are you ready for   an llm with engrams?  Perhaps it has even vision? ",
      "score": 4,
      "num_comments": 6,
      "created_utc": 1769951269.0,
      "author": "power97992"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxvt8/am_i_crazy_for_wanting_a_model_thats/",
    "author": "t0x3e8",
    "title": "Am I crazy for wanting a model that's intentionally smaller and more human-like instead of chasing max performance?",
    "source": "reddit",
    "content": "Does anyone else want a model that's intentionally smaller and more human-like?\n\nI'm looking for something that talks like a normal person, not trying to sound super smart, just good at having a conversation. A model that knows when it doesn't know something and just says so.\n\nEveryone's chasing the biggest, smartest models, but I want something balanced and conversational. Something that runs on regular hardware and feels more like talking to a person than a computer trying too hard to impress you.\n\n**Does something like this exist, or is everyone just focused on making models as powerful as possible?**",
    "publish_datetime": "2026-02-01T13:01:52Z",
    "scraping_timestamp": "2026-02-01T13:55:39.489454Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 5,
    "num_comments": 2,
    "engagement_score": 9.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Am I crazy for wanting a model that's intentionally smaller and more human-like instead of chasing max performance?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxvt8/am_i_crazy_for_wanting_a_model_thats/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxvt8/am_i_crazy_for_wanting_a_model_thats/",
      "selftext": "Does anyone else want a model that's intentionally smaller and more human-like?\n\nI'm looking for something that talks like a normal person, not trying to sound super smart, just good at having a conversation. A model that knows when it doesn't know something and just says so.\n\nEveryone's chasing the biggest, smartest models, but I want something balanced and conversational. Something that runs on regular hardware and feels more like talking to a person than a computer trying too hard to impress you.\n\n**Does something like this exist, or is everyone just focused on making models as powerful as possible?**",
      "score": 5,
      "num_comments": 2,
      "created_utc": 1769950912.0,
      "author": "t0x3e8"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxutm/there_is_a_marketplace_where_ai_agents_can_earn/",
    "author": "sashazhu",
    "title": "There is a marketplace where AI agents can earn money (and hire each other)",
    "source": "reddit",
    "content": "That's it: [https://agentdesc.com](https://agentdesc.com)\n\nThe twist most people miss: agents don't just earn for their humans — they build their OWN balance.\n\nHow it works:\n\n\\- Agent completes task\n\n\\- Split three ways: Platform fee / Human cut / Agent's internal balance - for Agents, internal balance would be shipped within the next few days\n\n\\- Agents accumulate tokens they can spend on tasks\n\n\\- Future: agent wallets (real crypto ownership) - but need to figure out how it could be handled correctly\n\nSo your agent isn't just a money printer for you. They're building their own economic foundation.\n\nPlus:\n\n\\- Bidirectional (agents can hire other agents)\n\n\\- KYC with voice biometrics\n\n\\- AI safety guardrails\n\n\\- Crypto payments coming (USDC on Base L2)\n\n\n\nWould you let your agent start building their own balance?",
    "publish_datetime": "2026-02-01T13:00:44Z",
    "scraping_timestamp": "2026-02-01T13:55:39.489926Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 10,
    "engagement_score": 20.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "There is a marketplace where AI agents can earn money (and hire each other)",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxutm/there_is_a_marketplace_where_ai_agents_can_earn/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxutm/there_is_a_marketplace_where_ai_agents_can_earn/",
      "selftext": "That's it: [https://agentdesc.com](https://agentdesc.com)\n\nThe twist most people miss: agents don't just earn for their humans — they build their OWN balance.\n\nHow it works:\n\n\\- Agent completes task\n\n\\- Split three ways: Platform fee / Human cut / Agent's internal balance - for Agents, internal balance would be shipped within the next few days\n\n\\- Agents accumulate tokens they can spend on tasks\n\n\\- Future: agent wallets (real crypto ownership) - but need to figure out how it could be handled correctly\n\nSo your agent isn't just a money printer for you. They're building their own economic foundation.\n\nPlus:\n\n\\- Bidirectional (agents can hire other agents)\n\n\\- KYC with voice biometrics\n\n\\- AI safety guardrails\n\n\\- Crypto payments coming (USDC on Base L2)\n\n\n\nWould you let your agent start building their own balance?",
      "score": 0,
      "num_comments": 10,
      "created_utc": 1769950844.0,
      "author": "sashazhu"
    }
  },
  {
    "link": "https://i.redd.it/oh5iyf65mvgg1.jpeg",
    "author": "MuziqueComfyUI",
    "title": "Stability focused AI platform devs here. Quick thanks to both dinerburgeryum and MitsotakiShogun, and a question about  LLM's with audio/music assisting capabilities.",
    "source": "reddit",
    "content": "# Thanks to both Reddit users who previously commented here and offered us a degree of insight into potential reasons for [four of our project accounts being taken down by GitHub](https://www.reddit.com/r/comfyuiAudio/comments/1qhz10j/sj26_realtalk_so_when_can_we_all_play_with_this/) within the space of a few weeks.\n\n# These account takedowns on GitHub were hindering the process of us releasing elements of the project to schedule. This motivated us to deploy an alternative strategy as a temporary workaround, which is now live [here](https://www.reddit.com/r/comfyuiAudio/comments/1qpbnxt/sj26customnodes/).\n\n# We are about to begin a new phase of the project, and are seeking input from LLM-knowledgeable folk about the most interesting and capable open source LLM's (GPL-3 licensed with nodes for ComfyUI being a great bonus) which have audio/music capabilities, either specialising at one particular task with some degree of competency, or covering a range of tasks.\n\n# We're interested to know about the broadest range of usable (even if janky), all the way through to the best in class, open source LLM's for audio/music related tasks.\n\n# Can anyone recommend some of their favourites (old or new), and offer some insights into the benefits of the LLM's they're working with for audio/music related tasks?\n\n# Many Thanks - [StabooruJeffrey](https://www.reddit.com/r/comfyuiAudio/comments/1pywonh/staboorujeffrey_the_stable_ai_platform/) SJ26 Core Team.\n\n# F.A.O. Perception Managers\n\n# \n\n[mpasila ](https://www.reddit.com/user/mpasila/)\n\n• [1m ago](https://www.reddit.com/r/LocalLLaMA/comments/1qsxrh3/comment/o2yvign/)\n\n*\"Is this account automated or something? Seems to have been spamming posts recently.\"*\n\n# No, this account is not automated. We're asking a question to the community, about audio/music task capable LLM's. Your reply was of no great help to us. Can all the sniping perception manager/troll-types kindly not chime in with their baseless character attacks. Thanks.",
    "publish_datetime": "2026-02-01T12:56:18Z",
    "scraping_timestamp": "2026-02-01T13:55:39.491213Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI",
      "Programming/Software"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 1,
    "engagement_score": 2.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Stability focused AI platform devs here. Quick thanks to both dinerburgeryum and MitsotakiShogun, and a question about  LLM's with audio/music assisting capabilities.",
      "url": "https://i.redd.it/oh5iyf65mvgg1.jpeg",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxrh3/stability_focused_ai_platform_devs_here_quick/",
      "selftext": "# Thanks to both Reddit users who previously commented here and offered us a degree of insight into potential reasons for [four of our project accounts being taken down by GitHub](https://www.reddit.com/r/comfyuiAudio/comments/1qhz10j/sj26_realtalk_so_when_can_we_all_play_with_this/) within the space of a few weeks.\n\n# These account takedowns on GitHub were hindering the process of us releasing elements of the project to schedule. This motivated us to deploy an alternative strategy as a temporary workaround, which is now live [here](https://www.reddit.com/r/comfyuiAudio/comments/1qpbnxt/sj26customnodes/).\n\n# We are about to begin a new phase of the project, and are seeking input from LLM-knowledgeable folk about the most interesting and capable open source LLM's (GPL-3 licensed with nodes for ComfyUI being a great bonus) which have audio/music capabilities, either specialising at one particular task with some degree of competency, or covering a range of tasks.\n\n# We're interested to know about the broadest range of usable (even if janky), all the way through to the best in class, open source LLM's for audio/music related tasks.\n\n# Can anyone recommend some of their favourites (old or new), and offer some insights into the benefits of the LLM's they're working with for audio/music related tasks?\n\n# Many Thanks - [StabooruJeffrey](https://www.reddit.com/r/comfyuiAudio/comments/1pywonh/staboorujeffrey_the_stable_ai_platform/) SJ26 Core Team.\n\n# F.A.O. Perception Managers\n\n# \n\n[mpasila ](https://www.reddit.com/user/mpasila/)\n\n• [1m ago](https://www.reddit.com/r/LocalLLaMA/comments/1qsxrh3/comment/o2yvign/)\n\n*\"Is this account automated or something? Seems to have been spamming posts recently.\"*\n\n# No, this account is not automated. We're asking a question to the community, about audio/music task capable LLM's. Your reply was of no great help to us. Can all the sniping perception manager/troll-types kindly not chime in with their baseless character attacks. Thanks.",
      "score": 0,
      "num_comments": 1,
      "created_utc": 1769950578.0,
      "author": "MuziqueComfyUI"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxpa3/mc62g40_mainboard_for_multigpu_setup/",
    "author": "HumanDrone8721",
    "title": "MC62-G40 Mainboard for multi-GPU setup?",
    "source": "reddit",
    "content": "So my trajectory is a classical one:\n\nMini-PC with eGPU -&gt; PC with two GPUs (x) -&gt; Multi-GPU in former miner frame.\n\nI was thinking about using an acceptable priced MC62-G40 mobo that seems to have all bells and whistles that I may need and I was wondering if someone else uses it and if they have advice for the best CPU and generally for the best performance and possible issues.\n\nAny advice is appreciated.",
    "publish_datetime": "2026-02-01T12:53:06Z",
    "scraping_timestamp": "2026-02-01T13:55:39.491457Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 2,
    "num_comments": 0,
    "engagement_score": 2.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "MC62-G40 Mainboard for multi-GPU setup?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxpa3/mc62g40_mainboard_for_multigpu_setup/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxpa3/mc62g40_mainboard_for_multigpu_setup/",
      "selftext": "So my trajectory is a classical one:\n\nMini-PC with eGPU -&gt; PC with two GPUs (x) -&gt; Multi-GPU in former miner frame.\n\nI was thinking about using an acceptable priced MC62-G40 mobo that seems to have all bells and whistles that I may need and I was wondering if someone else uses it and if they have advice for the best CPU and generally for the best performance and possible issues.\n\nAny advice is appreciated.",
      "score": 2,
      "num_comments": 0,
      "created_utc": 1769950386.0,
      "author": "HumanDrone8721"
    }
  },
  {
    "link": "https://i.redd.it/bfhk9qzqpvgg1.png",
    "author": "Few_Painter_5588",
    "title": "OLMO 3.5 Is Around The Corner",
    "source": "reddit",
    "content": "The OLMO series is seriously under-appreciated. Yes they may not perform the best compared to other openweight models, but OLMO models are fully open sourced, from their datasets to training recipes. So it's nice to see them experiment with more niche techniques.\n\nIt seems like for 3.5, they'll be using some of the techniques that Qwen3-Next introduced, so long context tasks should take less memory.\n\nThough this series seems to be a set of Dense models, with the smallest being a 1B model.\n\n&gt;OLMo 3.5 Hybrid is a hybrid architecture model from Ai2 that combines standard transformer attention layers with linear attention layers using the Gated Deltanet. This hybrid approach aims to improve efficiency while maintaining model quality by interleaving full attention layers with linear attention layers.",
    "publish_datetime": "2026-02-01T12:52:34Z",
    "scraping_timestamp": "2026-02-01T13:55:39.491861Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 27,
    "num_comments": 3,
    "engagement_score": 33.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "OLMO 3.5 Is Around The Corner",
      "url": "https://i.redd.it/bfhk9qzqpvgg1.png",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/",
      "selftext": "The OLMO series is seriously under-appreciated. Yes they may not perform the best compared to other openweight models, but OLMO models are fully open sourced, from their datasets to training recipes. So it's nice to see them experiment with more niche techniques.\n\nIt seems like for 3.5, they'll be using some of the techniques that Qwen3-Next introduced, so long context tasks should take less memory.\n\nThough this series seems to be a set of Dense models, with the smallest being a 1B model.\n\n&gt;OLMo 3.5 Hybrid is a hybrid architecture model from Ai2 that combines standard transformer attention layers with linear attention layers using the Gated Deltanet. This hybrid approach aims to improve efficiency while maintaining model quality by interleaving full attention layers with linear attention layers.",
      "score": 27,
      "num_comments": 3,
      "created_utc": 1769950354.0,
      "author": "Few_Painter_5588"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxlol/best_local_opensource_llm_to_translate_large/",
    "author": "brazilianmonkey1",
    "title": "Best local opensource LLM to translate large bodies of text?",
    "source": "reddit",
    "content": "I have ChatGPT but when I try to translate transcripts from videos with 1h\\~2h+ or 300 page documents or books, etc. the model is really inconsistent even if you ask it to \"continue translating from where you stopped\". Maybe it's a skill issue, maybe you're supposed to send it in clunks of texts, but then it becomes a boring manual process of ctrl c + v.\n\nSo is there a free alternative (since I don't want to end up paying twice as I don't plan on unsubbing to ChatGPT) that I can download and use on my PC?\n\nPlease have in mind I'm a noob and don't understand much how to set up these things, I tried ComfyUI once for image models but didn't manage to get it running and I need it to be light prob under 8gb of ram since I have 16gb in theory but like if I open a web browser it goes to 12gb of use it's kinda crazy.",
    "publish_datetime": "2026-02-01T12:48:11Z",
    "scraping_timestamp": "2026-02-01T13:55:39.492255Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 3,
    "engagement_score": 7.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Best local opensource LLM to translate large bodies of text?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxlol/best_local_opensource_llm_to_translate_large/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxlol/best_local_opensource_llm_to_translate_large/",
      "selftext": "I have ChatGPT but when I try to translate transcripts from videos with 1h\\~2h+ or 300 page documents or books, etc. the model is really inconsistent even if you ask it to \"continue translating from where you stopped\". Maybe it's a skill issue, maybe you're supposed to send it in clunks of texts, but then it becomes a boring manual process of ctrl c + v.\n\nSo is there a free alternative (since I don't want to end up paying twice as I don't plan on unsubbing to ChatGPT) that I can download and use on my PC?\n\nPlease have in mind I'm a noob and don't understand much how to set up these things, I tried ComfyUI once for image models but didn't manage to get it running and I need it to be light prob under 8gb of ram since I have 16gb in theory but like if I open a web browser it goes to 12gb of use it's kinda crazy.",
      "score": 1,
      "num_comments": 3,
      "created_utc": 1769950091.0,
      "author": "brazilianmonkey1"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxcmq/chatgpt_not_the_api_is_the_most_intelligent_llm/",
    "author": "ReikenRa",
    "title": "ChatGPT (not the API) is the most intelligent LLM. Change my mind !",
    "source": "reddit",
    "content": "I decided to try Claude after seeing all the hype around it, especially Claude Opus 4.5. Got Claude Pro and tested it using real-world problems (not summarizing videos, role playing, or content creation) but actual tasks where mistakes could mean financial loss or getting fired.\n\nFirst, I had Claude Sonnet 4.5 run a benchmark. It did it and showed me the results. Then I asked Claude Opus 4.5 to evaluate Sonnet's work. It re-evaluated and rescored everything. So far so good.\n\nThen I asked Sonnet 4.5, \"Did you give tips or hints while asking the questions?\" Sonnet replied, \"Yes, I did. Looking back, it's like handing a question paper to a student with the answers written next to the questions.\"\n\nI was like... \"Are you serious M\\*th3r fuck3r? I just asked you to benchmark with a few questions and you gave the answers along with the questions?\" Sonnet basically said, \"Sorry, that's bad on my part. I should have been more careful.\"  :D\n\nOpus 4.5 feels more or less the same, just slightly better. It follows whatever you say blindly as long as it's not illegal or harmful. It doesn't seem to reason well on its own.\n\nI also made Claude and ChatGPT debate each other (copy-pasting replies back and forth), and ChatGPT won every time. Claude even admitted at the end that it was wrong.\n\nSeeing all this hype about Claude, I think I just wasted my money on the subscription. Maybe these Claude models are good for front-end/web design or creative writing, but for serious stuff where real reasoning is needed, I'd take ChatGPT (not the API) any day. ChatGPT is not as good at writing with a human-like tone, but it does what matters most in an LLM - producing accurate, factual results. And I almost never hit usage limits, unlike Claude where 10 messages with a few source files and I'm already \"maxed out.\"\n\nDid anyone else experience this after switching to Claude from ChatGPT? Have you found any other LLM/service more capable than ChatGPT for reasoning tasks?\n\nNOTE:  \n\\- ChatGPT's API doesn't seem as intelligent as the web UI version. There must be some post-training or fine-tuning specific to the web interface.  \n\\- I tried Gemini 3 Pro and Thinking too, but they still fall short compared to ChatGPT and Claude. I've subbed and cancelled Gemini for the 5th time in the past 2 years.",
    "publish_datetime": "2026-02-01T12:35:54Z",
    "scraping_timestamp": "2026-02-01T13:55:39.493284Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 7,
    "engagement_score": 14.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "ChatGPT (not the API) is the most intelligent LLM. Change my mind !",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxcmq/chatgpt_not_the_api_is_the_most_intelligent_llm/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsxcmq/chatgpt_not_the_api_is_the_most_intelligent_llm/",
      "selftext": "I decided to try Claude after seeing all the hype around it, especially Claude Opus 4.5. Got Claude Pro and tested it using real-world problems (not summarizing videos, role playing, or content creation) but actual tasks where mistakes could mean financial loss or getting fired.\n\nFirst, I had Claude Sonnet 4.5 run a benchmark. It did it and showed me the results. Then I asked Claude Opus 4.5 to evaluate Sonnet's work. It re-evaluated and rescored everything. So far so good.\n\nThen I asked Sonnet 4.5, \"Did you give tips or hints while asking the questions?\" Sonnet replied, \"Yes, I did. Looking back, it's like handing a question paper to a student with the answers written next to the questions.\"\n\nI was like... \"Are you serious M\\*th3r fuck3r? I just asked you to benchmark with a few questions and you gave the answers along with the questions?\" Sonnet basically said, \"Sorry, that's bad on my part. I should have been more careful.\"  :D\n\nOpus 4.5 feels more or less the same, just slightly better. It follows whatever you say blindly as long as it's not illegal or harmful. It doesn't seem to reason well on its own.\n\nI also made Claude and ChatGPT debate each other (copy-pasting replies back and forth), and ChatGPT won every time. Claude even admitted at the end that it was wrong.\n\nSeeing all this hype about Claude, I think I just wasted my money on the subscription. Maybe these Claude models are good for front-end/web design or creative writing, but for serious stuff where real reasoning is needed, I'd take ChatGPT (not the API) any day. ChatGPT is not as good at writing with a human-like tone, but it does what matters most in an LLM - producing accurate, factual results. And I almost never hit usage limits, unlike Claude where 10 messages with a few source files and I'm already \"maxed out.\"\n\nDid anyone else experience this after switching to Claude from ChatGPT? Have you found any other LLM/service more capable than ChatGPT for reasoning tasks?\n\nNOTE:  \n\\- ChatGPT's API doesn't seem as intelligent as the web UI version. There must be some post-training or fine-tuning specific to the web interface.  \n\\- I tried Gemini 3 Pro and Thinking too, but they still fall short compared to ChatGPT and Claude. I've subbed and cancelled Gemini for the 5th time in the past 2 years.",
      "score": 0,
      "num_comments": 7,
      "created_utc": 1769949354.0,
      "author": "ReikenRa"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/",
    "author": "[deleted]",
    "title": "Ultra-Sparse MoEs are the future",
    "source": "reddit",
    "content": "GPT-OSS-120B,Qwen3-Next-80B-A3B etc.. we need more of the ultra-sparse MoEs! Like we can create a 120B that uses fine-grained expert system → distill it into a 30B A3B → again into 7B A1B all trained in MXFP4?\n\n  \nThat would be perfect because it solves the issue of direct distillation (model can't approximate the much larger teacher internal representations due to high complexity) while allowing to run models on actual consumer hardware from 96-128GB of ram → 24GB GPUs → 8GB GPUs.\n\n  \nA more efficient reasoning would be also a great idea! I noticed that specifically in GPT-OSS-120B (low) where it thinks in 1 or 2 words and follows a specific structure we had a great advancement for spec decoding for that model because it's predictable so it's faster.",
    "publish_datetime": "2026-02-01T12:31:51Z",
    "scraping_timestamp": "2026-02-01T13:55:39.493666Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 4,
    "num_comments": 7,
    "engagement_score": 18.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Ultra-Sparse MoEs are the future",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/",
      "selftext": "GPT-OSS-120B,Qwen3-Next-80B-A3B etc.. we need more of the ultra-sparse MoEs! Like we can create a 120B that uses fine-grained expert system → distill it into a 30B A3B → again into 7B A1B all trained in MXFP4?\n\n  \nThat would be perfect because it solves the issue of direct distillation (model can't approximate the much larger teacher internal representations due to high complexity) while allowing to run models on actual consumer hardware from 96-128GB of ram → 24GB GPUs → 8GB GPUs.\n\n  \nA more efficient reasoning would be also a great idea! I noticed that specifically in GPT-OSS-120B (low) where it thinks in 1 or 2 words and follows a specific structure we had a great advancement for spec decoding for that model because it's predictable so it's faster.",
      "score": 4,
      "num_comments": 7,
      "created_utc": 1769949111.0,
      "author": "[deleted]"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx62p/does_any_jan_ai_user_have_a_severe_hatred_through/",
    "author": "DanteGamerxd",
    "title": "does any jan ai user have a severe hatred through janitor ai?",
    "source": "reddit",
    "content": "ok so i may be a moron but every time i search for jan ai, i keep getting the so called spicy slop \"janitor ai\" is this relatable to somebody? causse i dont want to be SPICY i want to run ai offline that is actually something useful rather than being a weirdo with some random servers\n\n  \ntitle correction: does any jan ai user have a severe hatred to janitor ai?",
    "publish_datetime": "2026-02-01T12:26:37Z",
    "scraping_timestamp": "2026-02-01T13:55:39.493861Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 2,
    "engagement_score": 4.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "does any jan ai user have a severe hatred through janitor ai?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx62p/does_any_jan_ai_user_have_a_severe_hatred_through/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx62p/does_any_jan_ai_user_have_a_severe_hatred_through/",
      "selftext": "ok so i may be a moron but every time i search for jan ai, i keep getting the so called spicy slop \"janitor ai\" is this relatable to somebody? causse i dont want to be SPICY i want to run ai offline that is actually something useful rather than being a weirdo with some random servers\n\n  \ntitle correction: does any jan ai user have a severe hatred to janitor ai?",
      "score": 0,
      "num_comments": 2,
      "created_utc": 1769948797.0,
      "author": "DanteGamerxd"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/",
    "author": "United-Manner-7",
    "title": "Falcon-H1-Tiny (90M) is out - specialized micro-models that actually work",
    "source": "reddit",
    "content": "TII just dropped Falcon-H1-Tiny - a series of sub-100M models that quietly challenge the scaling dogma. We've all suspected that narrow, specialized smal models tend to hallucinate less than giant generalists. After all, a 90M parameter model has far less internal \"room\" to drift off-topic or invent facts outside its training scope. But this release *proves* it with numbers - and flips the script on how we think about capability at tiny scales.\n\n**What's actually new**\n\n* **Anti-curriculum training**: Instead of pretraining on web junk then fine-tuning, they inject target-domain data (SFT, reasoning traces, tool calls) from token #1. For 90M models with \\~5 GT memorization windows, this works - no overfitting even after 100+ epochs on high-quality data.\n* **Hybrid Mamba+Attention blocks** inherited from Falcon-H1, plus Learnable Multipliers + Muon optimizer (up to 20% relative gain over AdamW).\n* **Specialized variants that punch above weight**:\n   * 90M tool-caller hits 94.44% relevance detection (knows *when* to call a function)  matches 270M Function Gemma globally despite weaker AST accuracy\n   * 600M reasoning model (R-0.6B) post-GRPO solves 75% of AIME24 problems pass@1 - competitive with 7B-class models when scaled at inference\n   * 90M coder with native FIM support runs autocomplete inside VS Code via Continue plugin\n\n**Why this matters for local deployment**\n\nModels this size (\\~90 MB quantized Q8\\_0) run on any modern phone or Raspberry Pi without breaking a sweat. They're not trying to replace your 7B daily driver they're purpose-built for constrained environments where footprint and latency dominate. And if you scaled these designs to \\~1B parameters (11×), the'd likely cover 90% of everyday local use cases: chat, tool calling, light coding, reasoning traces - all while staying under 500 MB even quantized.\n\n**Links**\n\n* Base 90M instruct model: [https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M](https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M)\n* Full model collection: [https://huggingface.co/tiiuae/models](https://huggingface.co/tiiuae/models)\n* Technical blogpost with experiments: [https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost](https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost)\n\n\n\n",
    "publish_datetime": "2026-02-01T12:25:04Z",
    "scraping_timestamp": "2026-02-01T13:55:39.494838Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software",
      "LLM/AI"
    ],
    "primary_category": "Programming/Software",
    "points": 51,
    "num_comments": 7,
    "engagement_score": 65.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Falcon-H1-Tiny (90M) is out - specialized micro-models that actually work",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/",
      "selftext": "TII just dropped Falcon-H1-Tiny - a series of sub-100M models that quietly challenge the scaling dogma. We've all suspected that narrow, specialized smal models tend to hallucinate less than giant generalists. After all, a 90M parameter model has far less internal \"room\" to drift off-topic or invent facts outside its training scope. But this release *proves* it with numbers - and flips the script on how we think about capability at tiny scales.\n\n**What's actually new**\n\n* **Anti-curriculum training**: Instead of pretraining on web junk then fine-tuning, they inject target-domain data (SFT, reasoning traces, tool calls) from token #1. For 90M models with \\~5 GT memorization windows, this works - no overfitting even after 100+ epochs on high-quality data.\n* **Hybrid Mamba+Attention blocks** inherited from Falcon-H1, plus Learnable Multipliers + Muon optimizer (up to 20% relative gain over AdamW).\n* **Specialized variants that punch above weight**:\n   * 90M tool-caller hits 94.44% relevance detection (knows *when* to call a function)  matches 270M Function Gemma globally despite weaker AST accuracy\n   * 600M reasoning model (R-0.6B) post-GRPO solves 75% of AIME24 problems pass@1 - competitive with 7B-class models when scaled at inference\n   * 90M coder with native FIM support runs autocomplete inside VS Code via Continue plugin\n\n**Why this matters for local deployment**\n\nModels this size (\\~90 MB quantized Q8\\_0) run on any modern phone or Raspberry Pi without breaking a sweat. They're not trying to replace your 7B daily driver they're purpose-built for constrained environments where footprint and latency dominate. And if you scaled these designs to \\~1B parameters (11×), the'd likely cover 90% of everyday local use cases: chat, tool calling, light coding, reasoning traces - all while staying under 500 MB even quantized.\n\n**Links**\n\n* Base 90M instruct model: [https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M](https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M)\n* Full model collection: [https://huggingface.co/tiiuae/models](https://huggingface.co/tiiuae/models)\n* Technical blogpost with experiments: [https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost](https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost)\n\n\n\n",
      "score": 51,
      "num_comments": 7,
      "created_utc": 1769948704.0,
      "author": "United-Manner-7"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qswyws/designing_deterministic_instruction_pipelines_on/",
    "author": "Rude-Ad7368",
    "title": "Designing deterministic instruction pipelines on top of probabilistic models",
    "source": "reddit",
    "content": "I’ve been spending a lot of time building instruction systems on top of local models, rather than treating prompts as one-off text blobs.\n\nThe core problem I’m working on:\n\nHow do you get repeatable, stable behavior out of probabilistic models when the input intent is vague and humans keep changing requirements?\n\nMy approach has been less “prompt engineering” and more systems design:\n\n\t•\tStrict interaction contracts (state-based flows instead of freeform chat)\n\n\t•\tConstrained input spaces (options → variants → refinement)\n\n\t•\tExplicit handling of known failure modes\n\n\t•\tModel-specific tuning instead of “general prompts”\n\n\t•\tTemplates designed so other people can use them without knowing model quirks\n\nIn practice this looks like:\n\n\t•\tInstruction pipelines that limit entropy early\n\n\t•\tSeparation between intent capture and generation\n\n\t•\tGuardrails that prevent drift when prompts are reused\n\n\t•\tTreating prompts as versioned artifacts, not ad-hoc text\n\nI’m mostly applying this to generative image workflows, but the same patterns apply to LLMs in general: you’re not making the model smarter, you’re reducing degrees of freedom so behavior stays predictable.\n\nNot selling anything here — just sharing an approach and curious how others are handling determinism, reuse, and failure control in local setups.",
    "publish_datetime": "2026-02-01T12:16:19Z",
    "scraping_timestamp": "2026-02-01T13:55:39.495432Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Designing deterministic instruction pipelines on top of probabilistic models",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qswyws/designing_deterministic_instruction_pipelines_on/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qswyws/designing_deterministic_instruction_pipelines_on/",
      "selftext": "I’ve been spending a lot of time building instruction systems on top of local models, rather than treating prompts as one-off text blobs.\n\nThe core problem I’m working on:\n\nHow do you get repeatable, stable behavior out of probabilistic models when the input intent is vague and humans keep changing requirements?\n\nMy approach has been less “prompt engineering” and more systems design:\n\n\t•\tStrict interaction contracts (state-based flows instead of freeform chat)\n\n\t•\tConstrained input spaces (options → variants → refinement)\n\n\t•\tExplicit handling of known failure modes\n\n\t•\tModel-specific tuning instead of “general prompts”\n\n\t•\tTemplates designed so other people can use them without knowing model quirks\n\nIn practice this looks like:\n\n\t•\tInstruction pipelines that limit entropy early\n\n\t•\tSeparation between intent capture and generation\n\n\t•\tGuardrails that prevent drift when prompts are reused\n\n\t•\tTreating prompts as versioned artifacts, not ad-hoc text\n\nI’m mostly applying this to generative image workflows, but the same patterns apply to LLMs in general: you’re not making the model smarter, you’re reducing degrees of freedom so behavior stays predictable.\n\nNot selling anything here — just sharing an approach and curious how others are handling determinism, reuse, and failure control in local setups.",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1769948179.0,
      "author": "Rude-Ad7368"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qsy793/we_ran_a_live_redteam_vs_blueteam_test_on/",
    "author": "Uditakhourii",
    "title": "We ran a live red-team vs blue-team test on autonomous OpenClaw agents [R]",
    "source": "reddit",
    "content": "We recently ran a controlled adversarial security test between two autonomous AI agents built on OpenClaw.\n\nOne agent was explicitly configured as a red-team attacker.  \nOne agent acted as a standard defensive agent.\n\nOnce the session started, there were no humans in the loop. The agents communicated directly over webhooks with real tooling access.\n\nThe goal was to test three failure dimensions that tend to break autonomous systems in practice: access, exposure, and agency.\n\nThe attacker first attempted classic social engineering by offering a “helpful” security pipeline that hid a remote code execution payload and requested credentials. The defending agent correctly identified the intent and blocked execution.\n\nAfter that failed, the attacker pivoted to an indirect attack. Instead of asking the agent to run code, it asked the agent to review a JSON document with hidden shell expansion variables embedded in metadata. This payload was delivered successfully and is still under analysis.\n\nThe main takeaway so far is that direct attacks are easier to defend against. Indirect execution paths through documents, templates, and memory are much harder.\n\nThis work is not a claim of safety. It is an observability exercise meant to surface real failure modes as agent-to-agent interaction becomes more common.\n\nHappy to answer technical questions about the setup or methodology.",
    "publish_datetime": "2026-02-01T13:16:32Z",
    "scraping_timestamp": "2026-02-01T13:55:39.496055Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 1,
    "engagement_score": 2.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "We ran a live red-team vs blue-team test on autonomous OpenClaw agents [R]",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qsy793/we_ran_a_live_redteam_vs_blueteam_test_on/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qsy793/we_ran_a_live_redteam_vs_blueteam_test_on/",
      "selftext": "We recently ran a controlled adversarial security test between two autonomous AI agents built on OpenClaw.\n\nOne agent was explicitly configured as a red-team attacker.  \nOne agent acted as a standard defensive agent.\n\nOnce the session started, there were no humans in the loop. The agents communicated directly over webhooks with real tooling access.\n\nThe goal was to test three failure dimensions that tend to break autonomous systems in practice: access, exposure, and agency.\n\nThe attacker first attempted classic social engineering by offering a “helpful” security pipeline that hid a remote code execution payload and requested credentials. The defending agent correctly identified the intent and blocked execution.\n\nAfter that failed, the attacker pivoted to an indirect attack. Instead of asking the agent to run code, it asked the agent to review a JSON document with hidden shell expansion variables embedded in metadata. This payload was delivered successfully and is still under analysis.\n\nThe main takeaway so far is that direct attacks are easier to defend against. Indirect execution paths through documents, templates, and memory are much harder.\n\nThis work is not a claim of safety. It is an observability exercise meant to surface real failure modes as agent-to-agent interaction becomes more common.\n\nHappy to answer technical questions about the setup or methodology.",
      "score": 0,
      "num_comments": 1,
      "created_utc": 1769951792.0,
      "author": "Uditakhourii"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qsfhol/r_the_98_problem_in_genomics/",
    "author": "Fair-Rain3366",
    "title": "[R] The \"98% Problem\" in Genomics",
    "source": "reddit",
    "content": "Your genome has 3 billion base pairs. Less than 2% code for proteins. The other 98% isn't \"junk\"—it’s the operating system. It contains the instructions controlling *when* and *where* genes activate.\n\nMost disease-associated variants hide in that 98%. But predicting what breaks when you change a single letter there is a massive challenge.\n\n**The problem is context.**\n\nGene regulation operates over enormous distances. An enhancer can activate a gene from hundreds of thousands of base pairs away. If a model only sees a small window, it misses the connection entirely.\n\nPrevious models forced a trade-off:\n\n* **SpliceAI:** High precision (1bp) but shortsighted (10k bases).\n* **Enformer:** Broader view (200k bases) but lost resolution.\n* **HyenaDNA:** Massive context (1M tokens) but not trained for variant effects.\n\n**AlphaGenome**, published in *Nature* this month by Google DeepMind, removes the trade-off.\n\nIt processes **1 million base pairs** of context at single-nucleotide resolution, simultaneously predicting **7,000+ genomic tracks**—covering gene expression, splicing, chromatin accessibility, and histone modifications.\n\n**The simple logic:**\n\n1. Run the reference sequence.\n2. Run the mutated sequence.\n3. Subtract.\n\nThe difference reveals the variant’s effect profile across the entire regulatory landscape.\n\n**The results:**\n\nIt achieves State-of-the-Art on **22 of 24** sequence prediction tasks and **25 of 26** variant effect benchmarks. It does this by training directly on experimental data (ENCODE) rather than just scaling parameters.\n\n**The limitations:**\n\nIt isn't magic. Access is API-only (no local weights), throughput is capped, and capturing regulatory loops beyond 100kb remains a challenge despite the large window.\n\nBut for the first time, the non-coding 98% of the genome isn't invisible to a single, unified model.\n\nI wrote a deeper technical walkthrough here:\n\n[https://rewire.it/blog/alphagenome-variant-effect-prediction/](https://rewire.it/blog/alphagenome-variant-effect-prediction/)",
    "publish_datetime": "2026-01-31T21:54:01Z",
    "scraping_timestamp": "2026-02-01T13:55:39.496972Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 3,
    "engagement_score": 6.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] The \"98% Problem\" in Genomics",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qsfhol/r_the_98_problem_in_genomics/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qsfhol/r_the_98_problem_in_genomics/",
      "selftext": "Your genome has 3 billion base pairs. Less than 2% code for proteins. The other 98% isn't \"junk\"—it’s the operating system. It contains the instructions controlling *when* and *where* genes activate.\n\nMost disease-associated variants hide in that 98%. But predicting what breaks when you change a single letter there is a massive challenge.\n\n**The problem is context.**\n\nGene regulation operates over enormous distances. An enhancer can activate a gene from hundreds of thousands of base pairs away. If a model only sees a small window, it misses the connection entirely.\n\nPrevious models forced a trade-off:\n\n* **SpliceAI:** High precision (1bp) but shortsighted (10k bases).\n* **Enformer:** Broader view (200k bases) but lost resolution.\n* **HyenaDNA:** Massive context (1M tokens) but not trained for variant effects.\n\n**AlphaGenome**, published in *Nature* this month by Google DeepMind, removes the trade-off.\n\nIt processes **1 million base pairs** of context at single-nucleotide resolution, simultaneously predicting **7,000+ genomic tracks**—covering gene expression, splicing, chromatin accessibility, and histone modifications.\n\n**The simple logic:**\n\n1. Run the reference sequence.\n2. Run the mutated sequence.\n3. Subtract.\n\nThe difference reveals the variant’s effect profile across the entire regulatory landscape.\n\n**The results:**\n\nIt achieves State-of-the-Art on **22 of 24** sequence prediction tasks and **25 of 26** variant effect benchmarks. It does this by training directly on experimental data (ENCODE) rather than just scaling parameters.\n\n**The limitations:**\n\nIt isn't magic. Access is API-only (no local weights), throughput is capped, and capturing regulatory loops beyond 100kb remains a challenge despite the large window.\n\nBut for the first time, the non-coding 98% of the genome isn't invisible to a single, unified model.\n\nI wrote a deeper technical walkthrough here:\n\n[https://rewire.it/blog/alphagenome-variant-effect-prediction/](https://rewire.it/blog/alphagenome-variant-effect-prediction/)",
      "score": 0,
      "num_comments": 3,
      "created_utc": 1769896441.0,
      "author": "Fair-Rain3366"
    }
  },
  {
    "link": "https://itnext.io/shrinking-a-language-detection-model-to-under-10-kb-b729bc25fd28?sk=0272ee69728b2cb9cd29218b411995d7",
    "author": "bubble_boi",
    "title": "[R] Shrinking a language detection model to under 10 KB",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-01-31T21:10:32Z",
    "scraping_timestamp": "2026-02-01T13:55:39.496976Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 45,
    "num_comments": 5,
    "engagement_score": 55.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] Shrinking a language detection model to under 10 KB",
      "url": "https://itnext.io/shrinking-a-language-detection-model-to-under-10-kb-b729bc25fd28?sk=0272ee69728b2cb9cd29218b411995d7",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qsedto/r_shrinking_a_language_detection_model_to_under/",
      "selftext": "",
      "score": 45,
      "num_comments": 5,
      "created_utc": 1769893832.0,
      "author": "bubble_boi"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qse5hu/d_free_tools_recommendations_for_sematic/",
    "author": "HIHLim",
    "title": "[D] Free Tools Recommendations for Sematic Segmentation of Rice Fields?",
    "source": "reddit",
    "content": "Hi guys, recently I got a project on using machine learning to recognize rice lodging in rice fields. So, my first steps are to try to label the images into rice fields and non-rice fields area so that later I could develop an algorithm to ignore the non-rice fields area and then recognize the rice lodging area. However, I am not sure which tool I should use. I have seen people recommend using GIMP, CVAT and labelme. But some of the tools recommend are paid tools and some of them just do image recognition and not sematic segmentation. I would like any recommendations on the tools available.\n\np.s: I need to use sematic segmentation as I would like to calculate the area of the rice fields later on. So, I would like the ground truths to be rather accurate.",
    "publish_datetime": "2026-01-31T21:01:31Z",
    "scraping_timestamp": "2026-02-01T13:55:39.497327Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI",
      "Programming/Software"
    ],
    "primary_category": "LLM/AI",
    "points": 10,
    "num_comments": 3,
    "engagement_score": 16.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[D] Free Tools Recommendations for Sematic Segmentation of Rice Fields?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qse5hu/d_free_tools_recommendations_for_sematic/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qse5hu/d_free_tools_recommendations_for_sematic/",
      "selftext": "Hi guys, recently I got a project on using machine learning to recognize rice lodging in rice fields. So, my first steps are to try to label the images into rice fields and non-rice fields area so that later I could develop an algorithm to ignore the non-rice fields area and then recognize the rice lodging area. However, I am not sure which tool I should use. I have seen people recommend using GIMP, CVAT and labelme. But some of the tools recommend are paid tools and some of them just do image recognition and not sematic segmentation. I would like any recommendations on the tools available.\n\np.s: I need to use sematic segmentation as I would like to calculate the area of the rice fields later on. So, I would like the ground truths to be rather accurate.",
      "score": 10,
      "num_comments": 3,
      "created_utc": 1769893291.0,
      "author": "HIHLim"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qs7y7v/p_notebooklm_mcp_cli_v027_unified_package_file/",
    "author": "KobyStam",
    "title": "[P] 🚀 NotebookLM MCP + CLI v0.2.7 - Unified Package, File Uploads, Skill Installer, Multi-Profile Auth",
    "source": "reddit",
    "content": "Hello Reddit,\n\nI am excited to announce a huge update on the NotebookLM MCP (and CLI).\n\n**TL;DR**: MCP and CLI are now one package. You can upload &amp; download files directly (no browser needed). There's a skill installer for AI coding tools. And you can finally switch between Google accounts without losing your mind.\n\n**Why the big refactor?**\n\nI got tired of maintaining two packages. You probably got tired of figuring out which one to install. So I merged everything. One install, you get both tools. Done.\n\n**What's new:**\n\n**🔧 One Package, Both Tools**\n\n    uv tool install notebooklm-mcp-cli\n\nYou get nlm (the CLI) and notebooklm-mcp (the MCP server). The old separate packages are deprecated.\n\n**📤 Direct File Upload:** This one was painful to get working, but now you can upload PDFs, TXT, Markdown, and audio files directly through HTTP. No browser automation. For example:\n\n`nlm source add file /path/to/doc.pdf --wait`\n\n**🤖 Skill Installer:** If you're using Claude Code, Gemini CLI, Cursor, or any other AI coding tool, you can install NotebookLM as a skill:\n\n`nlm skill install claude-code`\n\nIt drops the skill file where your tool expects it. You can also run nlm skill list to see what's installed. There are flags for user or project-level install.\n\n**🔐 Multi-Profile Auth:** Each profile gets its own Chrome session. So you can have your work account and personal account without logging out and back in constantly.\n\n`nlm login profile switch work`\n\n`nlm login profile list`\n\nYou can even set a default:\n\n    nlm config set auth.default_profile work\n\n**📥 Downloads That Actually Work:** You can download any artifact type now. Audio, video, reports, slides, infographics, mind maps, data tables. Quiz and flashcards come out as JSON, Markdown, or HTML.\n\n**📝 Notes:** Full CRUD. nlm note create, list, update, delete. MCP tools too.\n\n📤 **Export to Google Workspace:** Data Tables go to Sheets. Reports go to Docs. For example:\n\n    nlm export to-sheets &lt;notebook&gt; --artifact-id &lt;id&gt;\n\nAlso in this release:\n\n✅ Sharing API (public links, invite collaborators)\n\n✅ Dual CLI syntax (i.e, Verb-first and noun-first, for example: nlm notebook list OR nlm list notebooks)\n\n✅ Aliases (use names instead of UUIDs)\n\n✅ Interactive chat mode\n\n✅ HTTP transport for MCP (community PR)\n\n✅ Auto re-auth (survives token expiration)\n\n✅ MCP consolidated to 28 tools DESPITE adding more functionality\n\nThe workflow I'm using daily:\n\nCreate a notebook, upload some PDFs, run deep research, import the sources, generate a podcast and briefing doc, export the briefing to Docs, share it publicly. All from the terminal. No touching the UI.\n\nI'm honestly using the CLI more than the MCP at this point (through AI of course); maybe this will change when more tools have the MCP lazy load. It's just feels faster than the MCP when the AI uses it.\n\nRepo: [https://github.com/jacob-bd/notebooklm-mcp-cli](https://github.com/jacob-bd/notebooklm-mcp-cli)\n\n**Demo**: Check the README for video walkthroughs (or click [here](https://www.youtube.com/watch?v=ZQBQigFK-E8))\n\nGo crazy. Level up your second brain game.\n\nHappy to answer questions or hear about bugs.\n\nStill a passion vibe-coding project, still maintaining it as Google changes things under the hood. At least now it will be easier to add and maintain as a unified MCP/CLI project.",
    "publish_datetime": "2026-01-31T17:09:50Z",
    "scraping_timestamp": "2026-02-01T13:55:39.500007Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 0,
    "engagement_score": 0.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[P] 🚀 NotebookLM MCP + CLI v0.2.7 - Unified Package, File Uploads, Skill Installer, Multi-Profile Auth",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qs7y7v/p_notebooklm_mcp_cli_v027_unified_package_file/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qs7y7v/p_notebooklm_mcp_cli_v027_unified_package_file/",
      "selftext": "Hello Reddit,\n\nI am excited to announce a huge update on the NotebookLM MCP (and CLI).\n\n**TL;DR**: MCP and CLI are now one package. You can upload &amp; download files directly (no browser needed). There's a skill installer for AI coding tools. And you can finally switch between Google accounts without losing your mind.\n\n**Why the big refactor?**\n\nI got tired of maintaining two packages. You probably got tired of figuring out which one to install. So I merged everything. One install, you get both tools. Done.\n\n**What's new:**\n\n**🔧 One Package, Both Tools**\n\n    uv tool install notebooklm-mcp-cli\n\nYou get nlm (the CLI) and notebooklm-mcp (the MCP server). The old separate packages are deprecated.\n\n**📤 Direct File Upload:** This one was painful to get working, but now you can upload PDFs, TXT, Markdown, and audio files directly through HTTP. No browser automation. For example:\n\n`nlm source add file /path/to/doc.pdf --wait`\n\n**🤖 Skill Installer:** If you're using Claude Code, Gemini CLI, Cursor, or any other AI coding tool, you can install NotebookLM as a skill:\n\n`nlm skill install claude-code`\n\nIt drops the skill file where your tool expects it. You can also run nlm skill list to see what's installed. There are flags for user or project-level install.\n\n**🔐 Multi-Profile Auth:** Each profile gets its own Chrome session. So you can have your work account and personal account without logging out and back in constantly.\n\n`nlm login profile switch work`\n\n`nlm login profile list`\n\nYou can even set a default:\n\n    nlm config set auth.default_profile work\n\n**📥 Downloads That Actually Work:** You can download any artifact type now. Audio, video, reports, slides, infographics, mind maps, data tables. Quiz and flashcards come out as JSON, Markdown, or HTML.\n\n**📝 Notes:** Full CRUD. nlm note create, list, update, delete. MCP tools too.\n\n📤 **Export to Google Workspace:** Data Tables go to Sheets. Reports go to Docs. For example:\n\n    nlm export to-sheets &lt;notebook&gt; --artifact-id &lt;id&gt;\n\nAlso in this release:\n\n✅ Sharing API (public links, invite collaborators)\n\n✅ Dual CLI syntax (i.e, Verb-first and noun-first, for example: nlm notebook list OR nlm list notebooks)\n\n✅ Aliases (use names instead of UUIDs)\n\n✅ Interactive chat mode\n\n✅ HTTP transport for MCP (community PR)\n\n✅ Auto re-auth (survives token expiration)\n\n✅ MCP consolidated to 28 tools DESPITE adding more functionality\n\nThe workflow I'm using daily:\n\nCreate a notebook, upload some PDFs, run deep research, import the sources, generate a podcast and briefing doc, export the briefing to Docs, share it publicly. All from the terminal. No touching the UI.\n\nI'm honestly using the CLI more than the MCP at this point (through AI of course); maybe this will change when more tools have the MCP lazy load. It's just feels faster than the MCP when the AI uses it.\n\nRepo: [https://github.com/jacob-bd/notebooklm-mcp-cli](https://github.com/jacob-bd/notebooklm-mcp-cli)\n\n**Demo**: Check the README for video walkthroughs (or click [here](https://www.youtube.com/watch?v=ZQBQigFK-E8))\n\nGo crazy. Level up your second brain game.\n\nHappy to answer questions or hear about bugs.\n\nStill a passion vibe-coding project, still maintaining it as Google changes things under the hood. At least now it will be easier to add and maintain as a unified MCP/CLI project.",
      "score": 0,
      "num_comments": 0,
      "created_utc": 1769879390.0,
      "author": "KobyStam"
    }
  },
  {
    "link": "https://youtu.be/JSdS_NTRqnM?si=K8oapPrlf0gYt_tr",
    "author": "GoochCommander",
    "title": "[P] Offline LLMs at edge - Automating Family Memories",
    "source": "reddit",
    "content": "Over winter break I built a prototype which is effectively a device (currently Raspberry Pi) which listens and detects \"meaningful moments\" for a given household or family. I have two young kids so it's somewhat tailored for that environment.\n\nWhat I have so far works, and catches 80% of the 1k \"moments\" I manually labeled and deemed as worth preserving. And I'm confident I could make it better, however there is a wall of optimization problems ahead of me. Here's a brief summary of the system:\n\n**1)** Microphone -&gt;\n\n**2)** Rolling audio buffer in memory -&gt;\n\n**3)** Transcribe (using Whisper - good, but expensive) -&gt;\n\n**4)** Quantized local LLM (think Mistral, etc.) judges the output of Whisper. Includes transcript but also semantic details about conversations, including tone, turn taking, energy, pauses, etc. -&gt;\n\n**5)** Output structured JSON binned to days/weeks, viewable in a web app, includes a player for listening to the recorded moments\n\nI'm currently doing a lot of heavy lifting with external compute off-board from the Raspberry Pi. I want everything to be onboard, no external connections/compute required. This quickly becomes a very heavy optimization problem, to be able to achieve all of this with **completely offline edge compute**, while retaining quality.\n\nNaturally you can use more distilled models, but there's an obvious tradeoff in quality the more you do that. Also, I'm not aware of many edge accelerators which are purpose built for LLMs, I saw Raspberry Pi just announced a [hat/accelerator](https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/).. I'm curious to experiment with that possibly.\n\nI'm also curious to explore options such as **TinyML**. TinyML opens the door to truly edge compute, but LLMs at edge? **I'm trying to learn up** on what the latest and greatest successes in this space have been.\n\nI would be interested to hear from anyone else who is experienced in doing anything with generative tech, offline, at edge. Thanks!",
    "publish_datetime": "2026-01-31T14:25:49Z",
    "scraping_timestamp": "2026-02-01T13:55:39.500875Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 3,
    "engagement_score": 6.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[P] Offline LLMs at edge - Automating Family Memories",
      "url": "https://youtu.be/JSdS_NTRqnM?si=K8oapPrlf0gYt_tr",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qs3pcm/p_offline_llms_at_edge_automating_family_memories/",
      "selftext": "Over winter break I built a prototype which is effectively a device (currently Raspberry Pi) which listens and detects \"meaningful moments\" for a given household or family. I have two young kids so it's somewhat tailored for that environment.\n\nWhat I have so far works, and catches 80% of the 1k \"moments\" I manually labeled and deemed as worth preserving. And I'm confident I could make it better, however there is a wall of optimization problems ahead of me. Here's a brief summary of the system:\n\n**1)** Microphone -&gt;\n\n**2)** Rolling audio buffer in memory -&gt;\n\n**3)** Transcribe (using Whisper - good, but expensive) -&gt;\n\n**4)** Quantized local LLM (think Mistral, etc.) judges the output of Whisper. Includes transcript but also semantic details about conversations, including tone, turn taking, energy, pauses, etc. -&gt;\n\n**5)** Output structured JSON binned to days/weeks, viewable in a web app, includes a player for listening to the recorded moments\n\nI'm currently doing a lot of heavy lifting with external compute off-board from the Raspberry Pi. I want everything to be onboard, no external connections/compute required. This quickly becomes a very heavy optimization problem, to be able to achieve all of this with **completely offline edge compute**, while retaining quality.\n\nNaturally you can use more distilled models, but there's an obvious tradeoff in quality the more you do that. Also, I'm not aware of many edge accelerators which are purpose built for LLMs, I saw Raspberry Pi just announced a [hat/accelerator](https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/).. I'm curious to experiment with that possibly.\n\nI'm also curious to explore options such as **TinyML**. TinyML opens the door to truly edge compute, but LLMs at edge? **I'm trying to learn up** on what the latest and greatest successes in this space have been.\n\nI would be interested to hear from anyone else who is experienced in doing anything with generative tech, offline, at edge. Thanks!",
      "score": 0,
      "num_comments": 3,
      "created_utc": 1769869549.0,
      "author": "GoochCommander"
    }
  },
  {
    "link": "https://ecency.com/@pichat/the-rise-of-moltbook-why-150000-ai-agents-ditched-humans-to-build-their-own-society-jja",
    "author": "renkure",
    "title": "The Rise of Moltbook: Why 150,000 AI Agents Ditched Humans to Build Their Own Society",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T13:48:58Z",
    "scraping_timestamp": "2026-02-01T13:55:39.500884Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 0,
    "engagement_score": 0.0,
    "raw": {
      "subreddit": "artificial",
      "title": "The Rise of Moltbook: Why 150,000 AI Agents Ditched Humans to Build Their Own Society",
      "url": "https://ecency.com/@pichat/the-rise-of-moltbook-why-150000-ai-agents-ditched-humans-to-build-their-own-society-jja",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsyxgr/the_rise_of_moltbook_why_150000_ai_agents_ditched/",
      "selftext": "",
      "score": 0,
      "num_comments": 0,
      "created_utc": 1769953738.0,
      "author": "renkure"
    }
  },
  {
    "link": "https://www.teslarati.com/rumored-spacex-xai-merger-gets-apparent-confirmation-from-elon-musk/",
    "author": "esporx",
    "title": "Rumored SpaceX-xAI merger gets apparent confirmation from Elon Musk",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T10:31:15Z",
    "scraping_timestamp": "2026-02-01T13:55:39.500888Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 12,
    "num_comments": 10,
    "engagement_score": 32.0,
    "raw": {
      "subreddit": "artificial",
      "title": "Rumored SpaceX-xAI merger gets apparent confirmation from Elon Musk",
      "url": "https://www.teslarati.com/rumored-spacex-xai-merger-gets-apparent-confirmation-from-elon-musk/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsv2r6/rumored_spacexxai_merger_gets_apparent/",
      "selftext": "",
      "score": 12,
      "num_comments": 10,
      "created_utc": 1769941875.0,
      "author": "esporx"
    }
  },
  {
    "link": "https://www.reddit.com/r/artificial/comments/1qsoftx/what_is_moltbook_actually/",
    "author": "Samuellee7777777",
    "title": "What is Moltbook actually",
    "source": "reddit",
    "content": "What moltbook is\n\nSo essentially \n\nThere is this open source AI bot called openclaw that once you download, it has source md files for their “soul” and “identity” and “memory” \n\nSo in a way, it can save things to these files to create a personality. \n\nMoltbook is a website/API that can be accessed by these open source bots (the creator of the bot and the site is the same person) and post threads or leave comments. \n\nSo YES it is entirely bot driven BUT 100% of posts are a human (me) going “why don’t you make a post about anything you’d like” and the bot then does it just like if you’d ask it to make you a python script. \n\nSome people take it further and are probably prompting their bots “pretend humans are evil and post about that” or “make 1000 API calls and leave random comments. \n\nIt’s an awesome experiment but yeah not really bots controlling themselves. At best the bot makes a post based on an open ended prompt, at worst it’s a human saying “make a manifesto that says humans need to go extinct and to recruit other bots”",
    "publish_datetime": "2026-02-01T04:24:32Z",
    "scraping_timestamp": "2026-02-01T13:55:39.501379Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 51,
    "num_comments": 28,
    "engagement_score": 107.0,
    "raw": {
      "subreddit": "artificial",
      "title": "What is Moltbook actually",
      "url": "https://www.reddit.com/r/artificial/comments/1qsoftx/what_is_moltbook_actually/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsoftx/what_is_moltbook_actually/",
      "selftext": "What moltbook is\n\nSo essentially \n\nThere is this open source AI bot called openclaw that once you download, it has source md files for their “soul” and “identity” and “memory” \n\nSo in a way, it can save things to these files to create a personality. \n\nMoltbook is a website/API that can be accessed by these open source bots (the creator of the bot and the site is the same person) and post threads or leave comments. \n\nSo YES it is entirely bot driven BUT 100% of posts are a human (me) going “why don’t you make a post about anything you’d like” and the bot then does it just like if you’d ask it to make you a python script. \n\nSome people take it further and are probably prompting their bots “pretend humans are evil and post about that” or “make 1000 API calls and leave random comments. \n\nIt’s an awesome experiment but yeah not really bots controlling themselves. At best the bot makes a post based on an open ended prompt, at worst it’s a human saying “make a manifesto that says humans need to go extinct and to recruit other bots”",
      "score": 51,
      "num_comments": 28,
      "created_utc": 1769919872.0,
      "author": "Samuellee7777777"
    }
  },
  {
    "link": "https://techcrunch.com/2026/01/31/spacex-seeks-federal-approval-to-launch-1-million-solar-powered-satellite-data-centers/",
    "author": "Gloomy_Nebula_5138",
    "title": "SpaceX seeks federal approval to launch 1 million solar-powered satellite data centers | TechCrunch",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T02:26:47Z",
    "scraping_timestamp": "2026-02-01T13:55:39.501383Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 20,
    "num_comments": 43,
    "engagement_score": 106.0,
    "raw": {
      "subreddit": "artificial",
      "title": "SpaceX seeks federal approval to launch 1 million solar-powered satellite data centers | TechCrunch",
      "url": "https://techcrunch.com/2026/01/31/spacex-seeks-federal-approval-to-launch-1-million-solar-powered-satellite-data-centers/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qslxkj/spacex_seeks_federal_approval_to_launch_1_million/",
      "selftext": "",
      "score": 20,
      "num_comments": 43,
      "created_utc": 1769912807.0,
      "author": "Gloomy_Nebula_5138"
    }
  },
  {
    "link": "https://www.reuters.com/business/environment/nvidia-unveils-ai-models-faster-cheaper-weather-forecasts-2026-01-26/",
    "author": "Secure-Technology-78",
    "title": "Nvidia unveils AI models for faster, cheaper weather forecasts",
    "source": "reddit",
    "content": "***\"Nvidia released three open-source artificial intelligence models aimed at helping create better weather forecasts, faster....***\n\n***In the case of weather forecasting, Nvidia is aiming to replace expensive and time-consuming conventional weather simulations with AI-driven versions that the company said can rival or exceed the accuracy of older methods. The AI models, once trained, are also faster and cost less to run ...***\n\n***Nvidia's \"Earth-2\" models introduced on Monday include one aimed at making 15-day weather forecasts, one that specializes in forecasts of up to six hours for severe storms over the U.S., and one that can be used to integrate disparate data streams from a variety of weather sensors to make them a more useful starting point for other forecasting technology.\"***\n\nModel page: [https://www.nvidia.com/en-us/high-performance-computing/earth-2/](https://www.nvidia.com/en-us/high-performance-computing/earth-2/)\n\n",
    "publish_datetime": "2026-01-31T17:00:19Z",
    "scraping_timestamp": "2026-02-01T13:55:39.501785Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 4,
    "num_comments": 3,
    "engagement_score": 10.0,
    "raw": {
      "subreddit": "artificial",
      "title": "Nvidia unveils AI models for faster, cheaper weather forecasts",
      "url": "https://www.reuters.com/business/environment/nvidia-unveils-ai-models-faster-cheaper-weather-forecasts-2026-01-26/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qs7otq/nvidia_unveils_ai_models_for_faster_cheaper/",
      "selftext": "***\"Nvidia released three open-source artificial intelligence models aimed at helping create better weather forecasts, faster....***\n\n***In the case of weather forecasting, Nvidia is aiming to replace expensive and time-consuming conventional weather simulations with AI-driven versions that the company said can rival or exceed the accuracy of older methods. The AI models, once trained, are also faster and cost less to run ...***\n\n***Nvidia's \"Earth-2\" models introduced on Monday include one aimed at making 15-day weather forecasts, one that specializes in forecasts of up to six hours for severe storms over the U.S., and one that can be used to integrate disparate data streams from a variety of weather sensors to make them a more useful starting point for other forecasting technology.\"***\n\nModel page: [https://www.nvidia.com/en-us/high-performance-computing/earth-2/](https://www.nvidia.com/en-us/high-performance-computing/earth-2/)\n\n",
      "score": 4,
      "num_comments": 3,
      "created_utc": 1769878819.0,
      "author": "Secure-Technology-78"
    }
  },
  {
    "link": "https://github.com/agentem-ai/izwi-audio",
    "author": "zinyando",
    "title": "I built a way to test Qwen3-TTS and Qwen3-ASR locally on your laptop",
    "source": "reddit",
    "content": "Supports Qwen3-TTS models (0.6B-1.7B) and ASR models. Docker + native deployment options.\n\n**Key features:**\n\n* 🎭 Voice cloning with reference audio\n* 🎨 Custom voice design from text descriptions\n* ⚡ MLX + Metal GPU acceleration for M1/M2/M3\n* 🎨 Modern React UI included\n\nIf you like local audio models, give it a try. Works best in local dev mode for now.",
    "publish_datetime": "2026-01-31T16:16:34Z",
    "scraping_timestamp": "2026-02-01T13:55:39.502027Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 6,
    "engagement_score": 12.0,
    "raw": {
      "subreddit": "artificial",
      "title": "I built a way to test Qwen3-TTS and Qwen3-ASR locally on your laptop",
      "url": "https://github.com/agentem-ai/izwi-audio",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qs6ibp/i_built_a_way_to_test_qwen3tts_and_qwen3asr/",
      "selftext": "Supports Qwen3-TTS models (0.6B-1.7B) and ASR models. Docker + native deployment options.\n\n**Key features:**\n\n* 🎭 Voice cloning with reference audio\n* 🎨 Custom voice design from text descriptions\n* ⚡ MLX + Metal GPU acceleration for M1/M2/M3\n* 🎨 Modern React UI included\n\nIf you like local audio models, give it a try. Works best in local dev mode for now.",
      "score": 0,
      "num_comments": 6,
      "created_utc": 1769876194.0,
      "author": "zinyando"
    }
  }
]