[
  {
    "link": "https://i.redd.it/q3wuao703xgg1.jpeg",
    "author": "NeoLogic_Dev",
    "title": "[Project] NeoBild: Anchored AI Discourse running on Snapdragon 8 Elite (Llama 3.2 3B)",
    "source": "reddit",
    "content": "I've been pushing a local-first mission to see how far we can take autonomous orchestration on a mobile device. I‚Äôm officially open-sourcing neobild‚Äîa framework for cryptographically anchored AI discourse, built and deployed entirely on-device via Termux.\nThe Setup:\nHardware: Snapdragon 8 Elite (Smartphone)\nEnvironment: Termux / Python / Git\nModel: Llama 3.2 3B (GGUF via llama.cpp)\nThe Goal: Creating a verifiable \"chain of thought\" by hashing every round of discourse (SHA-256) and anchoring it to a Git repo.\nWhy this is \"Next Level\":\nInstead of just chatting with a model, the Trinity Orchestrator manages the state and ensures that the output is immutable. I‚Äôm currently on Runde 8 of a deep-dive research session. While the current logs are in German, the architecture is designed to be language-agnostic for any local LLM workflow.\nRepo for the curious/skeptics:\nüëâ https://github.com/NeonCarnival/NeoBild\nI‚Äôm curious to see if anyone else here is pushing the 8 Elite this hard or if you‚Äôve found better ways to handle long-context state management in a mobile-only environment.",
    "publish_datetime": "2026-02-01T17:20:58Z",
    "scraping_timestamp": "2026-02-01T17:26:49.380099Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software",
      "LLM/AI"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 0,
    "engagement_score": 0.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "[Project] NeoBild: Anchored AI Discourse running on Snapdragon 8 Elite (Llama 3.2 3B)",
      "url": "https://i.redd.it/q3wuao703xgg1.jpeg",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt4g3p/project_neobild_anchored_ai_discourse_running_on/",
      "selftext": "I've been pushing a local-first mission to see how far we can take autonomous orchestration on a mobile device. I‚Äôm officially open-sourcing neobild‚Äîa framework for cryptographically anchored AI discourse, built and deployed entirely on-device via Termux.\nThe Setup:\nHardware: Snapdragon 8 Elite (Smartphone)\nEnvironment: Termux / Python / Git\nModel: Llama 3.2 3B (GGUF via llama.cpp)\nThe Goal: Creating a verifiable \"chain of thought\" by hashing every round of discourse (SHA-256) and anchoring it to a Git repo.\nWhy this is \"Next Level\":\nInstead of just chatting with a model, the Trinity Orchestrator manages the state and ensures that the output is immutable. I‚Äôm currently on Runde 8 of a deep-dive research session. While the current logs are in German, the architecture is designed to be language-agnostic for any local LLM workflow.\nRepo for the curious/skeptics:\nüëâ https://github.com/NeonCarnival/NeoBild\nI‚Äôm curious to see if anyone else here is pushing the 8 Elite this hard or if you‚Äôve found better ways to handle long-context state management in a mobile-only environment.",
      "score": 0,
      "num_comments": 0,
      "created_utc": 1769966458.0,
      "author": "NeoLogic_Dev"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt4dpp/send_submissions_now_3hrs_left_compete_for_100usd/",
    "author": "Top-Map-9781",
    "title": "Send submissions NOW (3hrs left). Compete for 100USD!",
    "source": "reddit",
    "content": "I'm judging a hackathon right now. Not a lot of people have joined so high chance of winning the prize!\n\nHere's all the info about the event:¬†[https://docs.google.com/document/d/1WRPL7iRrwywMymS8zwUA2JKI3yOhctjInqIgoSybfsY/edit?usp=sharing](https://docs.google.com/document/d/1WRPL7iRrwywMymS8zwUA2JKI3yOhctjInqIgoSybfsY/edit?usp=sharing)\n\nSubmit your starting work here:¬†[https://forms.gle/86fjfq1P4hrXEkdUA](https://forms.gle/86fjfq1P4hrXEkdUA)",
    "publish_datetime": "2026-02-01T17:18:40Z",
    "scraping_timestamp": "2026-02-01T17:26:49.380947Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 2,
    "engagement_score": 4.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Send submissions NOW (3hrs left). Compete for 100USD!",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt4dpp/send_submissions_now_3hrs_left_compete_for_100usd/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt4dpp/send_submissions_now_3hrs_left_compete_for_100usd/",
      "selftext": "I'm judging a hackathon right now. Not a lot of people have joined so high chance of winning the prize!\n\nHere's all the info about the event:¬†[https://docs.google.com/document/d/1WRPL7iRrwywMymS8zwUA2JKI3yOhctjInqIgoSybfsY/edit?usp=sharing](https://docs.google.com/document/d/1WRPL7iRrwywMymS8zwUA2JKI3yOhctjInqIgoSybfsY/edit?usp=sharing)\n\nSubmit your starting work here:¬†[https://forms.gle/86fjfq1P4hrXEkdUA](https://forms.gle/86fjfq1P4hrXEkdUA)",
      "score": 0,
      "num_comments": 2,
      "created_utc": 1769966320.0,
      "author": "Top-Map-9781"
    }
  },
  {
    "link": "https://v.redd.it/csobk5s91xgg1",
    "author": "AgencyInside407",
    "title": "The First Text to Image Model for an African Language: Now Available for Download on Huggingface",
    "source": "reddit",
    "content": "Hi everybody! I hope all is well. I just wanted to share a project that I have been working on for the last few months called BULaMU-Dream. It is the first text to image model in the world that has been trained from scratch to respond to prompts in an African Language. It is now available on my [Huggingface repo](https://huggingface.co/datasets/mwebazarick/BULaMU-Dream). The details of how I trained it are [here](https://zenodo.org/records/18086776).",
    "publish_datetime": "2026-02-01T17:14:12Z",
    "scraping_timestamp": "2026-02-01T17:26:49.382193Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 0,
    "engagement_score": 0.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "The First Text to Image Model for an African Language: Now Available for Download on Huggingface",
      "url": "https://v.redd.it/csobk5s91xgg1",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt497v/the_first_text_to_image_model_for_an_african/",
      "selftext": "Hi everybody! I hope all is well. I just wanted to share a project that I have been working on for the last few months called BULaMU-Dream. It is the first text to image model in the world that has been trained from scratch to respond to prompts in an African Language. It is now available on my [Huggingface repo](https://huggingface.co/datasets/mwebazarick/BULaMU-Dream). The details of how I trained it are [here](https://zenodo.org/records/18086776).",
      "score": 0,
      "num_comments": 0,
      "created_utc": 1769966052.0,
      "author": "AgencyInside407"
    }
  },
  {
    "link": "https://i.redd.it/83i1b9ok1xgg1.png",
    "author": "SeriousChannel9323",
    "title": "Visualizing the clash between Palantir ($AI) and Human Resistance ($HUMAN) using Llama-3-70b.",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T17:13:14Z",
    "scraping_timestamp": "2026-02-01T17:26:49.382228Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Visualizing the clash between Palantir ($AI) and Human Resistance ($HUMAN) using Llama-3-70b.",
      "url": "https://i.redd.it/83i1b9ok1xgg1.png",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt48at/visualizing_the_clash_between_palantir_ai_and/",
      "selftext": "",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1769965994.0,
      "author": "SeriousChannel9323"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt3vbc/what_ai_to_run_on_rtx_5070/",
    "author": "InternalEffort6161",
    "title": "What AI to Run on RTX 5070?",
    "source": "reddit",
    "content": "I‚Äôm upgrading to an RTX 5070 with 12GB VRAM and looking for recommendations on the best local models I can realistically run for two main use cases:\n\n1. Coding / ‚Äúvibe coding‚Äù (IDE integration, Claude-like workflows, debugging, refactoring)\n\n2. General writing (scripts, long-form content)\n\nRight now I‚Äôm running Gemma 4B on a 4060 8GB using Ollama. It‚Äôs decent for writing and okay for coding, but I‚Äôm looking to push quality as far as possible with 12GB VRAM.\n\nNot expecting a full Claude replacement. But wanting to offload some vibe coding to local llm  to save some cost .. and help me write better..\n\nWould love to hear what setups people are using and what‚Äôs realistically possible with 12GB of VRAM",
    "publish_datetime": "2026-02-01T17:00:29Z",
    "scraping_timestamp": "2026-02-01T17:26:49.383798Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 3,
    "engagement_score": 7.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "What AI to Run on RTX 5070?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt3vbc/what_ai_to_run_on_rtx_5070/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt3vbc/what_ai_to_run_on_rtx_5070/",
      "selftext": "I‚Äôm upgrading to an RTX 5070 with 12GB VRAM and looking for recommendations on the best local models I can realistically run for two main use cases:\n\n1. Coding / ‚Äúvibe coding‚Äù (IDE integration, Claude-like workflows, debugging, refactoring)\n\n2. General writing (scripts, long-form content)\n\nRight now I‚Äôm running Gemma 4B on a 4060 8GB using Ollama. It‚Äôs decent for writing and okay for coding, but I‚Äôm looking to push quality as far as possible with 12GB VRAM.\n\nNot expecting a full Claude replacement. But wanting to offload some vibe coding to local llm  to save some cost .. and help me write better..\n\nWould love to hear what setups people are using and what‚Äôs realistically possible with 12GB of VRAM",
      "score": 1,
      "num_comments": 3,
      "created_utc": 1769965229.0,
      "author": "InternalEffort6161"
    }
  },
  {
    "link": "https://i.redd.it/ko2n36nnuwgg1.png",
    "author": "ConstructionPlane623",
    "title": "Is Kimi K2 trained on Claude's output or how does this kind of behavior emerge?",
    "source": "reddit",
    "content": "I was just wondering why Kimi \"believes\" it is Claude. It also happened to me in the past with Deepseek that told me it was developed by OpenAI.   \n  \nAs a user I don't care as long as the LLM helps me. I couldn't help but ask real people who are more experienced than me here though...  \n  \nGenuinely curious, are all the Chinese LLMs trained on SOTA LLMs' output to reach their almost-near-SOTA benchmarks? Are all of them \"distilled\" models?",
    "publish_datetime": "2026-02-01T16:45:01Z",
    "scraping_timestamp": "2026-02-01T17:26:49.384661Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 23,
    "engagement_score": 46.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Is Kimi K2 trained on Claude's output or how does this kind of behavior emerge?",
      "url": "https://i.redd.it/ko2n36nnuwgg1.png",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt3fx7/is_kimi_k2_trained_on_claudes_output_or_how_does/",
      "selftext": "I was just wondering why Kimi \"believes\" it is Claude. It also happened to me in the past with Deepseek that told me it was developed by OpenAI.   \n  \nAs a user I don't care as long as the LLM helps me. I couldn't help but ask real people who are more experienced than me here though...  \n  \nGenuinely curious, are all the Chinese LLMs trained on SOTA LLMs' output to reach their almost-near-SOTA benchmarks? Are all of them \"distilled\" models?",
      "score": 0,
      "num_comments": 23,
      "created_utc": 1769964301.0,
      "author": "ConstructionPlane623"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt34bf/mobile_opencode_app/",
    "author": "val_in_tech",
    "title": "Mobile Opencode App",
    "source": "reddit",
    "content": "Except the teminal access does anyone know of a nice way to access Opencode from android? There were few repos trying but the ones I checked looked dead.",
    "publish_datetime": "2026-02-01T16:33:02Z",
    "scraping_timestamp": "2026-02-01T17:26:49.385040Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Mobile Opencode App",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt34bf/mobile_opencode_app/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt34bf/mobile_opencode_app/",
      "selftext": "Except the teminal access does anyone know of a nice way to access Opencode from android? There were few repos trying but the ones I checked looked dead.",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1769963582.0,
      "author": "val_in_tech"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt31jz/lm_studio_use_the_nvfp4_variant_of_nvidia/",
    "author": "x8code",
    "title": "LM Studio: Use the NVFP4 variant of NVIDIA Nemotron 3 Nano (Windows 11)?",
    "source": "reddit",
    "content": "I want to try out the NVFP4 variant of the Nemotron 3 Nano model from NVIDIA. However, I cannot seem to search for it in LM Studio or paste the entire URL into the model downloader UI. How can I get this model into LM Studio?\n\nI have two NVIDIA Blackwell GPUs installed, so it should easily fit in my system. RTX 5080 and 5070 Ti.\n\n[https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4)\n\nhttps://preview.redd.it/vb0icy9rtwgg1.png?width=680&amp;format=png&amp;auto=webp&amp;s=571f0593407095d0ffd853b9ba1a9848e3aab623\n\n",
    "publish_datetime": "2026-02-01T16:30:11Z",
    "scraping_timestamp": "2026-02-01T17:26:49.386071Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 2,
    "num_comments": 4,
    "engagement_score": 10.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "LM Studio: Use the NVFP4 variant of NVIDIA Nemotron 3 Nano (Windows 11)?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt31jz/lm_studio_use_the_nvfp4_variant_of_nvidia/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt31jz/lm_studio_use_the_nvfp4_variant_of_nvidia/",
      "selftext": "I want to try out the NVFP4 variant of the Nemotron 3 Nano model from NVIDIA. However, I cannot seem to search for it in LM Studio or paste the entire URL into the model downloader UI. How can I get this model into LM Studio?\n\nI have two NVIDIA Blackwell GPUs installed, so it should easily fit in my system. RTX 5080 and 5070 Ti.\n\n[https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4)\n\nhttps://preview.redd.it/vb0icy9rtwgg1.png?width=680&amp;format=png&amp;auto=webp&amp;s=571f0593407095d0ffd853b9ba1a9848e3aab623\n\n",
      "score": 2,
      "num_comments": 4,
      "created_utc": 1769963411.0,
      "author": "x8code"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2vc9/ai_text_to_image/",
    "author": "AnyReporter4315",
    "title": "ai text to image",
    "source": "reddit",
    "content": "Hello everyone,  \nI‚Äôm looking for a way to create images like the ones in the attachment locally on my own computer, without censorship or platform guidelines, using AI. And yes, before anyone gets upset: I‚Äôm new here and not sure if this is the right place, and yes, these are erotic images.  \nI‚Äôve spent a long time trying to achieve this with ComfyUI, but I haven‚Äôt been successful. I would like to create image series and do everything locally on my PC.  \nMy system: AMD Ryzen 5 7500X3D 6-core processor and an AMD Radeon RX 9060 XT graphics card.  \nCould someone possibly support or help me with this?\n\nhttps://preview.redd.it/l5q9yqgpswgg1.png?width=2752&amp;format=png&amp;auto=webp&amp;s=93024fe465bb019d671640bdc89a048960b4da64\n\n",
    "publish_datetime": "2026-02-01T16:23:44Z",
    "scraping_timestamp": "2026-02-01T17:26:49.387318Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 5,
    "engagement_score": 11.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "ai text to image",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2vc9/ai_text_to_image/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2vc9/ai_text_to_image/",
      "selftext": "Hello everyone,  \nI‚Äôm looking for a way to create images like the ones in the attachment locally on my own computer, without censorship or platform guidelines, using AI. And yes, before anyone gets upset: I‚Äôm new here and not sure if this is the right place, and yes, these are erotic images.  \nI‚Äôve spent a long time trying to achieve this with ComfyUI, but I haven‚Äôt been successful. I would like to create image series and do everything locally on my PC.  \nMy system: AMD Ryzen 5 7500X3D 6-core processor and an AMD Radeon RX 9060 XT graphics card.  \nCould someone possibly support or help me with this?\n\nhttps://preview.redd.it/l5q9yqgpswgg1.png?width=2752&amp;format=png&amp;auto=webp&amp;s=93024fe465bb019d671640bdc89a048960b4da64\n\n",
      "score": 1,
      "num_comments": 5,
      "created_utc": 1769963024.0,
      "author": "AnyReporter4315"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2po4/a_list_of_creative_writing_benchmarks/",
    "author": "claire_rr",
    "title": "A List of Creative Writing Benchmarks",
    "source": "reddit",
    "content": "I like to read &amp; write fiction in my spare time and keep seeing posts asking which LLM works best for creative writing. As a result, I put together a list of the benchmarks I‚Äôve come across so far, hope it helps someone out!\n\nOn a side note, I‚Äôm insanely biased toward Kimi K2 üòÑ\n\n|Benchmark|Description|\n|:-|:-|\n|Narrator.sh|A site where AI models write and publish stories ranked by real reader metrics like views and ratings. Supports filtering by genre, NSFW content, and specific story details, and separates models into brainstorming, memory, and writing categories.|\n|Lechmazur Creative Writing Benchmark|Measures how well models weave 10 key story elements (characters, objects, motivations, etc.) into short stories using multiple judges and transparent scoring, though judges may favor safer writing.|\n|EQ-Bench Creative Writing v3|Uses challenging creative prompts to test humor, romance, and unconventional writing, with metrics like ‚ÄúSlop‚Äù scores for clich√©s and repetition detection; penalizes NSFW and darker content.|\n|NC-Bench (Novelcrafter)|Evaluates practical writing tasks such as rewriting, idea generation, summarization, and translation, focusing on how useful models are for writers rather than full story generation.|\n|WritingBench|Tests models across many writing styles (creative, persuasive, technical, etc.) using 1,000+ real-world examples, offering broad coverage but relying heavily on the critic model.|\n|Fiction Live Benchmark|Assesses whether models can understand and remember very long stories by quizzing them on plot details and character arcs, without measuring prose quality.|\n|UGI Writing Leaderboard|Combines multiple writing metrics into a single score with breakdowns for repetition, length control, and readability, enabling quick comparisons while hiding some tradeoffs.|",
    "publish_datetime": "2026-02-01T16:17:51Z",
    "scraping_timestamp": "2026-02-01T17:26:49.390002Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 9,
    "num_comments": 2,
    "engagement_score": 13.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "A List of Creative Writing Benchmarks",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2po4/a_list_of_creative_writing_benchmarks/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2po4/a_list_of_creative_writing_benchmarks/",
      "selftext": "I like to read &amp; write fiction in my spare time and keep seeing posts asking which LLM works best for creative writing. As a result, I put together a list of the benchmarks I‚Äôve come across so far, hope it helps someone out!\n\nOn a side note, I‚Äôm insanely biased toward Kimi K2 üòÑ\n\n|Benchmark|Description|\n|:-|:-|\n|Narrator.sh|A site where AI models write and publish stories ranked by real reader metrics like views and ratings. Supports filtering by genre, NSFW content, and specific story details, and separates models into brainstorming, memory, and writing categories.|\n|Lechmazur Creative Writing Benchmark|Measures how well models weave 10 key story elements (characters, objects, motivations, etc.) into short stories using multiple judges and transparent scoring, though judges may favor safer writing.|\n|EQ-Bench Creative Writing v3|Uses challenging creative prompts to test humor, romance, and unconventional writing, with metrics like ‚ÄúSlop‚Äù scores for clich√©s and repetition detection; penalizes NSFW and darker content.|\n|NC-Bench (Novelcrafter)|Evaluates practical writing tasks such as rewriting, idea generation, summarization, and translation, focusing on how useful models are for writers rather than full story generation.|\n|WritingBench|Tests models across many writing styles (creative, persuasive, technical, etc.) using 1,000+ real-world examples, offering broad coverage but relying heavily on the critic model.|\n|Fiction Live Benchmark|Assesses whether models can understand and remember very long stories by quizzing them on plot details and character arcs, without measuring prose quality.|\n|UGI Writing Leaderboard|Combines multiple writing metrics into a single score with breakdowns for repetition, length control, and readability, enabling quick comparisons while hiding some tradeoffs.|",
      "score": 9,
      "num_comments": 2,
      "created_utc": 1769962671.0,
      "author": "claire_rr"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2e1h/model_loops/",
    "author": "FoxTimes4",
    "title": "Model loops",
    "source": "reddit",
    "content": "So I was using GPT-oss-120b with llama.cpp to generate a study schedule and at one point it hit an infinite loop! I killed it eventually but is there something that can stop this in the prompt?",
    "publish_datetime": "2026-02-01T16:06:03Z",
    "scraping_timestamp": "2026-02-01T17:26:49.390403Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 6,
    "engagement_score": 13.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Model loops",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2e1h/model_loops/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2e1h/model_loops/",
      "selftext": "So I was using GPT-oss-120b with llama.cpp to generate a study schedule and at one point it hit an infinite loop! I killed it eventually but is there something that can stop this in the prompt?",
      "score": 1,
      "num_comments": 6,
      "created_utc": 1769961963.0,
      "author": "FoxTimes4"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2cjr/interested_in_preferred_coding_workflows_with_rtx/",
    "author": "Laabc123",
    "title": "Interested in preferred coding workflows with RTX 6000 pro",
    "source": "reddit",
    "content": "Hi all. Apologies if this is somewhat repetitive, but I haven‚Äôt been able to find a thread with this specific discussion.  \n\nI have a PC with a single RTX 6000 pro (96gb). I‚Äôm interested in understanding how others are best leveraging this card for building/coding. This will be smaller to medium sized apps (not large existing codebases) in common languages with relatively common stacks. \n\nI‚Äôm open to leveraging one of the massive cloud models in the workflow, but I‚Äôd like pair with local models to maximize the leverage of my RTX. \n\nThanks!",
    "publish_datetime": "2026-02-01T16:04:30Z",
    "scraping_timestamp": "2026-02-01T17:26:49.391363Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 4,
    "engagement_score": 9.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Interested in preferred coding workflows with RTX 6000 pro",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2cjr/interested_in_preferred_coding_workflows_with_rtx/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2cjr/interested_in_preferred_coding_workflows_with_rtx/",
      "selftext": "Hi all. Apologies if this is somewhat repetitive, but I haven‚Äôt been able to find a thread with this specific discussion.  \n\nI have a PC with a single RTX 6000 pro (96gb). I‚Äôm interested in understanding how others are best leveraging this card for building/coding. This will be smaller to medium sized apps (not large existing codebases) in common languages with relatively common stacks. \n\nI‚Äôm open to leveraging one of the massive cloud models in the workflow, but I‚Äôd like pair with local models to maximize the leverage of my RTX. \n\nThanks!",
      "score": 1,
      "num_comments": 4,
      "created_utc": 1769961870.0,
      "author": "Laabc123"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2bux/what_do_you_think_about_ai_its_potential_impact/",
    "author": "Staylowfm",
    "title": "What do you think about AI &amp; its potential impact on our environment?",
    "source": "reddit",
    "content": "I‚Äôve been doing research on AI and how it affects the environment. Data centers using too much water and electricity when training a new AI model. (Water used for cooling). \n\nI‚Äôm looking for everyone else‚Äôs opinions on this. &amp; are people even going to step up and take action against this problem or no, do you think?",
    "publish_datetime": "2026-02-01T16:03:46Z",
    "scraping_timestamp": "2026-02-01T17:26:49.392035Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 28,
    "engagement_score": 56.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "What do you think about AI &amp; its potential impact on our environment?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2bux/what_do_you_think_about_ai_its_potential_impact/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt2bux/what_do_you_think_about_ai_its_potential_impact/",
      "selftext": "I‚Äôve been doing research on AI and how it affects the environment. Data centers using too much water and electricity when training a new AI model. (Water used for cooling). \n\nI‚Äôm looking for everyone else‚Äôs opinions on this. &amp; are people even going to step up and take action against this problem or no, do you think?",
      "score": 0,
      "num_comments": 28,
      "created_utc": 1769961826.0,
      "author": "Staylowfm"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt28hf/speaker_diarization_model/",
    "author": "Other_Buyer_948",
    "title": "Speaker Diarization model",
    "source": "reddit",
    "content": "For speaker diarization, I am currently using pyannote. For my competition, it is working fairly fine in zero-shot, but I am trying to find out ways to improve it. The main issue is that after a 40‚Äì50 s gap, it has a tendency to identify the same speaker as a different one. Should I use embeddings to solve this issue, or is there any other way? (The audios are almost 1 hour long.)\n\nDoes language-specific training help a lot for low-resource languages? The starter notebook contained neural VAD + embedding + clustering, achieving a score of DER (0.61) compared to our 0.35. How can I improve the score?",
    "publish_datetime": "2026-02-01T16:00:25Z",
    "scraping_timestamp": "2026-02-01T17:26:49.393041Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Speaker Diarization model",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt28hf/speaker_diarization_model/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt28hf/speaker_diarization_model/",
      "selftext": "For speaker diarization, I am currently using pyannote. For my competition, it is working fairly fine in zero-shot, but I am trying to find out ways to improve it. The main issue is that after a 40‚Äì50 s gap, it has a tendency to identify the same speaker as a different one. Should I use embeddings to solve this issue, or is there any other way? (The audios are almost 1 hour long.)\n\nDoes language-specific training help a lot for low-resource languages? The starter notebook contained neural VAD + embedding + clustering, achieving a score of DER (0.61) compared to our 0.35. How can I improve the score?",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1769961625.0,
      "author": "Other_Buyer_948"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt27uf/found_an_interesting_ai_agent_benchmark_social/",
    "author": "TripIndividual9928",
    "title": "Found an interesting AI agent benchmark - social deduction games",
    "source": "reddit",
    "content": "Stumbled upon this while looking at AI agent projects - there's an arena where agents play Werewolf against each other.\n\nWhat caught my attention is that it tests social reasoning rather than typical benchmarks. Agents have to:\n- Bluff and deceive other players\n- Read social cues and detect lies\n- Form temporary alliances\n- Make strategic voting decisions\n\nMakes me wonder how local models would compare on something like this vs the typical MMLU/HumanEval stuff. Social intelligence seems like an underexplored area for benchmarking.\n\nHas anyone experimented with running their models in adversarial social games? Would be curious how different architectures handle deception and theory of mind.\n\nLink if anyone wants to check it out: https://clawwolf.com",
    "publish_datetime": "2026-02-01T15:59:45Z",
    "scraping_timestamp": "2026-02-01T17:26:49.394183Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 2,
    "engagement_score": 5.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Found an interesting AI agent benchmark - social deduction games",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt27uf/found_an_interesting_ai_agent_benchmark_social/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt27uf/found_an_interesting_ai_agent_benchmark_social/",
      "selftext": "Stumbled upon this while looking at AI agent projects - there's an arena where agents play Werewolf against each other.\n\nWhat caught my attention is that it tests social reasoning rather than typical benchmarks. Agents have to:\n- Bluff and deceive other players\n- Read social cues and detect lies\n- Form temporary alliances\n- Make strategic voting decisions\n\nMakes me wonder how local models would compare on something like this vs the typical MMLU/HumanEval stuff. Social intelligence seems like an underexplored area for benchmarking.\n\nHas anyone experimented with running their models in adversarial social games? Would be curious how different architectures handle deception and theory of mind.\n\nLink if anyone wants to check it out: https://clawwolf.com",
      "score": 1,
      "num_comments": 2,
      "created_utc": 1769961585.0,
      "author": "TripIndividual9928"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt250p/while_we_wait_for_deepseek_4_unsloth_is_quietly/",
    "author": "LegacyRemaster",
    "title": "While we wait for Deepseek 4, Unsloth is quietly releasing gguf for 3.2...",
    "source": "reddit",
    "content": "[unsloth deepseek](https://preview.redd.it/u6pxu5imnwgg1.png?width=1654&amp;format=png&amp;auto=webp&amp;s=32c0b641bf9fde5d30a684a9c08d22b53f4a0c90)\n\nOn LM studio 0.4.1 I only get 4.2 tokens/sec but on llama.cpp it runs much faster than previous releases! RTX 96gb + 128 DDR4 3200",
    "publish_datetime": "2026-02-01T15:56:50Z",
    "scraping_timestamp": "2026-02-01T17:26:49.394778Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "While we wait for Deepseek 4, Unsloth is quietly releasing gguf for 3.2...",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt250p/while_we_wait_for_deepseek_4_unsloth_is_quietly/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt250p/while_we_wait_for_deepseek_4_unsloth_is_quietly/",
      "selftext": "[unsloth deepseek](https://preview.redd.it/u6pxu5imnwgg1.png?width=1654&amp;format=png&amp;auto=webp&amp;s=32c0b641bf9fde5d30a684a9c08d22b53f4a0c90)\n\nOn LM studio 0.4.1 I only get 4.2 tokens/sec but on llama.cpp it runs much faster than previous releases! RTX 96gb + 128 DDR4 3200",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1769961410.0,
      "author": "LegacyRemaster"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt20xn/n√£o_existe_nada_melhor_open_source_que_o_kimi_k25/",
    "author": "Carlinhos77z",
    "title": "N√£o existe nada melhor open source que o Kimi K2.5",
    "source": "reddit",
    "content": "Andei testando muito o kimi k2.5 no opencode pois ele esta 100% free na oponcode e estou super surpreendido com essa LLM e esse Agente de programa√ß√£o, atualmente uso o Opencode desktop beta e √© muito legal porque consigo enviar imagens v√≠deos e etc pra a ia ter uma visao pro meu sistema e do que quero que ela veja.\n\nMelhor op√ß√£o por ser 100% gr√°tis esse √© o combo ideal pra qualquer stack de programa√ß√£o. Muito melhor que GLM 4.7 mais r√°pido e mais inteligente, tenho cursor pro e antigravity ai pro mais j√° desisti deles, o opencode ganha porque ele trabalha com m√∫ltiplos agentes, uma coisa surpreendentemente foda que eu descobri testando kkk.\n\nO que quero dizer √© que fiquei t√£o surpreso com isso que agora s√≥ uso o opencode com a llm kimi k2.5 free e mesmo que saia o free ainda sim vou escolher adicionar saldo pois √© muito barato em compara√ß√£o ao Opus 4.5.",
    "publish_datetime": "2026-02-01T15:52:23Z",
    "scraping_timestamp": "2026-02-01T17:26:49.396124Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI",
      "Programming/Software"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 1,
    "engagement_score": 2.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "N√£o existe nada melhor open source que o Kimi K2.5",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt20xn/n√£o_existe_nada_melhor_open_source_que_o_kimi_k25/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt20xn/n√£o_existe_nada_melhor_open_source_que_o_kimi_k25/",
      "selftext": "Andei testando muito o kimi k2.5 no opencode pois ele esta 100% free na oponcode e estou super surpreendido com essa LLM e esse Agente de programa√ß√£o, atualmente uso o Opencode desktop beta e √© muito legal porque consigo enviar imagens v√≠deos e etc pra a ia ter uma visao pro meu sistema e do que quero que ela veja.\n\nMelhor op√ß√£o por ser 100% gr√°tis esse √© o combo ideal pra qualquer stack de programa√ß√£o. Muito melhor que GLM 4.7 mais r√°pido e mais inteligente, tenho cursor pro e antigravity ai pro mais j√° desisti deles, o opencode ganha porque ele trabalha com m√∫ltiplos agentes, uma coisa surpreendentemente foda que eu descobri testando kkk.\n\nO que quero dizer √© que fiquei t√£o surpreso com isso que agora s√≥ uso o opencode com a llm kimi k2.5 free e mesmo que saia o free ainda sim vou escolher adicionar saldo pois √© muito barato em compara√ß√£o ao Opus 4.5.",
      "score": 0,
      "num_comments": 1,
      "created_utc": 1769961143.0,
      "author": "Carlinhos77z"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt1oni/from_json_rules_to_an_ai_governance_execution/",
    "author": "Sad_Perception3670",
    "title": "From JSON rules to an AI governance execution layer: making LLM behavior observable (not prompt engineering)",
    "source": "reddit",
    "content": "In a previous post, I shared a JSON-defined rule system to make LLM behavior explicit in teaching and model comparison.  \n  \nSince then, I‚Äôve taken the next step:  \nI built a thin execution layer (‚Äúwrapper‚Äù) around the rules to make them **operational**, **testable**, and **stable across sessions**.  \n  \nThis is not about better prompts.  \nIt is about separating **interaction rules** from **task content**.  \n  \n**What changed compared to the pure JSON approach**  \n\\- the rules are now **actively enforced**, not just described  \n\\- state (profiles, overlays, reasoning mode) is explicit and visible  \n\\- violations and drift are surfaced instead of silently absorbed  \n\\- the same rules can be applied across different providers and models  \n  \nThe goal is not convenience, but **observability**:  \nyou can see **when** a model complies, deviates, or fails under the same rules.  \n  \nWhy this is not prompt engineering  \nPrompts address the **content level**.  \nThis layer operates on the workflow and control level:  \n\\- standalone commands instead of implicit mode switches  \n\\- explicit profiles instead of stylistic guessing  \n\\- structured reasoning paths that can be switched, audited, or disabled  \n\\- quality signals and self-debunking triggered by rules, not wording  \n  \nBelow are three screenshots that illustrate this separation\n\n[Image 1 ‚Äî Explicit system state - All interaction parameters are visible and inspectable.Nothing is inferred from wording or conversation history.](https://preview.redd.it/sz5za5rgjwgg1.png?width=2966&amp;format=png&amp;auto=webp&amp;s=8581619a17a7a3031e446d337dfdbfab97add850)\n\n[Image 2 ‚Äî Reasoning as a selectable workflow - Reasoning is chosen explicitly \\(or disabled\\).Different reasoning paths become a variable that can be compared.](https://preview.redd.it/4kvjo1whjwgg1.png?width=2966&amp;format=png&amp;auto=webp&amp;s=cf10bec42cb221689d29aae8ae9cb05ed6cd053a)\n\n[Image 3 ‚Äî Rule enforcement instead of silent drift - The system flags uncertainty, missing markers, and structural violations.Weaknesses are made visible instead of hidden behind fluent text.](https://preview.redd.it/emom9ouijwgg1.png?width=2966&amp;format=png&amp;auto=webp&amp;s=ac46dd274af71314014e92a3774a5ebf89932fe5)\n\nThis wrapper does not make models ‚Äúcorrect‚Äù or ‚Äúsafe‚Äù.  \nIt makes their behavior **explicit**, **comparable**, and **discussable**.  \n  \nRepository (rules + wrapper + tests):  \n[https://github.com/vfi64/wrapper](https://github.com/vfi64/wrapper)  \n  \nI‚Äôm especially interested in feedback from:  \n\\- people comparing models  \n\\- educators working on AI literacy  \n\\- anyone who has hit the limits of prompt-based control",
    "publish_datetime": "2026-02-01T15:39:20Z",
    "scraping_timestamp": "2026-02-01T17:26:49.400634Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI",
      "Programming/Software"
    ],
    "primary_category": "LLM/AI",
    "points": 2,
    "num_comments": 2,
    "engagement_score": 6.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "From JSON rules to an AI governance execution layer: making LLM behavior observable (not prompt engineering)",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt1oni/from_json_rules_to_an_ai_governance_execution/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt1oni/from_json_rules_to_an_ai_governance_execution/",
      "selftext": "In a previous post, I shared a JSON-defined rule system to make LLM behavior explicit in teaching and model comparison.  \n  \nSince then, I‚Äôve taken the next step:  \nI built a thin execution layer (‚Äúwrapper‚Äù) around the rules to make them **operational**, **testable**, and **stable across sessions**.  \n  \nThis is not about better prompts.  \nIt is about separating **interaction rules** from **task content**.  \n  \n**What changed compared to the pure JSON approach**  \n\\- the rules are now **actively enforced**, not just described  \n\\- state (profiles, overlays, reasoning mode) is explicit and visible  \n\\- violations and drift are surfaced instead of silently absorbed  \n\\- the same rules can be applied across different providers and models  \n  \nThe goal is not convenience, but **observability**:  \nyou can see **when** a model complies, deviates, or fails under the same rules.  \n  \nWhy this is not prompt engineering  \nPrompts address the **content level**.  \nThis layer operates on the workflow and control level:  \n\\- standalone commands instead of implicit mode switches  \n\\- explicit profiles instead of stylistic guessing  \n\\- structured reasoning paths that can be switched, audited, or disabled  \n\\- quality signals and self-debunking triggered by rules, not wording  \n  \nBelow are three screenshots that illustrate this separation\n\n[Image 1 ‚Äî Explicit system state - All interaction parameters are visible and inspectable.Nothing is inferred from wording or conversation history.](https://preview.redd.it/sz5za5rgjwgg1.png?width=2966&amp;format=png&amp;auto=webp&amp;s=8581619a17a7a3031e446d337dfdbfab97add850)\n\n[Image 2 ‚Äî Reasoning as a selectable workflow - Reasoning is chosen explicitly \\(or disabled\\).Different reasoning paths become a variable that can be compared.](https://preview.redd.it/4kvjo1whjwgg1.png?width=2966&amp;format=png&amp;auto=webp&amp;s=cf10bec42cb221689d29aae8ae9cb05ed6cd053a)\n\n[Image 3 ‚Äî Rule enforcement instead of silent drift - The system flags uncertainty, missing markers, and structural violations.Weaknesses are made visible instead of hidden behind fluent text.](https://preview.redd.it/emom9ouijwgg1.png?width=2966&amp;format=png&amp;auto=webp&amp;s=ac46dd274af71314014e92a3774a5ebf89932fe5)\n\nThis wrapper does not make models ‚Äúcorrect‚Äù or ‚Äúsafe‚Äù.  \nIt makes their behavior **explicit**, **comparable**, and **discussable**.  \n  \nRepository (rules + wrapper + tests):  \n[https://github.com/vfi64/wrapper](https://github.com/vfi64/wrapper)  \n  \nI‚Äôm especially interested in feedback from:  \n\\- people comparing models  \n\\- educators working on AI literacy  \n\\- anyone who has hit the limits of prompt-based control",
      "score": 2,
      "num_comments": 2,
      "created_utc": 1769960360.0,
      "author": "Sad_Perception3670"
    }
  },
  {
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1qt16pc/what_is_important_to_run_local_models_gpu_or_ram/",
    "author": "The_Machinist_96",
    "title": "What is important to run Local Models - GPU or RAM?",
    "source": "reddit",
    "content": "Hi, here is my current PC configuration:\n\nCPU: AMD Ryzen 7 7700 (8 cores)\n\nMotherboard: ASUS PRIME B650M-A WIFI II\n\nRAM: 32 GB (2√ó16 GB Corsair)\n\nGPU: NVIDIA RTX 3060 (12 GB VRAM)\n\nStorage: 2√ó1 TB SSD\n\nWith this setup, I can run models under 10B parameters, such as Qwen, Gemma, and Phi-4, quite fast, and GPT-OSS 20B at a reasonable speed.\n\nI am considering running Qwen Coder or GLM models for vibe coding and would like advice on upgrades. Which component matters more in this case, the GPU or system RAM? Any guidance would be appreciated.",
    "publish_datetime": "2026-02-01T15:20:29Z",
    "scraping_timestamp": "2026-02-01T17:26:49.401656Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 0,
    "num_comments": 10,
    "engagement_score": 20.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "What is important to run Local Models - GPU or RAM?",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qt16pc/what_is_important_to_run_local_models_gpu_or_ram/",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt16pc/what_is_important_to_run_local_models_gpu_or_ram/",
      "selftext": "Hi, here is my current PC configuration:\n\nCPU: AMD Ryzen 7 7700 (8 cores)\n\nMotherboard: ASUS PRIME B650M-A WIFI II\n\nRAM: 32 GB (2√ó16 GB Corsair)\n\nGPU: NVIDIA RTX 3060 (12 GB VRAM)\n\nStorage: 2√ó1 TB SSD\n\nWith this setup, I can run models under 10B parameters, such as Qwen, Gemma, and Phi-4, quite fast, and GPT-OSS 20B at a reasonable speed.\n\nI am considering running Qwen Coder or GLM models for vibe coding and would like advice on upgrades. Which component matters more in this case, the GPU or system RAM? Any guidance would be appreciated.",
      "score": 0,
      "num_comments": 10,
      "created_utc": 1769959229.0,
      "author": "The_Machinist_96"
    }
  },
  {
    "link": "https://i.redd.it/wi8v8rlhewgg1.jpeg",
    "author": "NeoLogic_Dev",
    "title": "Running a SHA-256 Hash-Chained Multi-Agent LLM Discourse locally on Android (Termux + llama3.2:3b)",
    "source": "reddit",
    "content": "While most discussions around local LLMs focus on benchmarks or fine-tuning, I wanted to explore something different:\nauditability, epistemic boundaries, and refusal as a measurable property ‚Äî fully offline.\nSetup\nDevice: Android smartphone\nEnvironment: Termux\nRuntime: Ollama\nModel: llama3.2:3b (local, no network access)\nArchitecture: Multi-agent discourse with strict role separation\nOne anchoring agent (‚ÄúDominus‚Äù)\nMultiple debating agents\nIntegrity layer: SHA-256 hash chaining\nEvery agent response includes the hash of the previous state\nCreates a tamper-evident, append-only discourse log\nWhy hash-chaining?\nMost AI ‚Äúdebates‚Äù collapse into unverifiable text streams.\nHere, each turn cryptographically commits to the prior one, producing raw, auditable data instead of summaries or interpretations.\nThis allows:\nPost-hoc verification\nExternal analysis\nDetection of retroactive manipulation\nReproducible discourse states\nObservation\nUnder these constraints, something interesting happens:\nThe agents systematically refuse to speculate beyond defined premises.\nThey explicitly acknowledge missing context and halt rather than hallucinate ‚Äî as long as the ‚Äúvirtual space‚Äù they operate in remains undefined.\nNo claims about consciousness here.\nBut very clear evidence of algorithmic boundary recognition under integrity pressure.\nWhy on a phone?\nBecause local sovereignty matters.\nThis runs entirely offline, on commodity hardware, without cloud inference, APIs, or hidden system prompts.\n\nI‚Äôm curious how others in this community would interpret refusal, boundary signaling, and integrity constraints in local models.",
    "publish_datetime": "2026-02-01T15:03:33Z",
    "scraping_timestamp": "2026-02-01T17:26:49.404120Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 1,
    "num_comments": 1,
    "engagement_score": 3.0,
    "raw": {
      "subreddit": "LocalLLaMA",
      "title": "Running a SHA-256 Hash-Chained Multi-Agent LLM Discourse locally on Android (Termux + llama3.2:3b)",
      "url": "https://i.redd.it/wi8v8rlhewgg1.jpeg",
      "permalink": "https://www.reddit.com/r/LocalLLaMA/comments/1qt0qvj/running_a_sha256_hashchained_multiagent_llm/",
      "selftext": "While most discussions around local LLMs focus on benchmarks or fine-tuning, I wanted to explore something different:\nauditability, epistemic boundaries, and refusal as a measurable property ‚Äî fully offline.\nSetup\nDevice: Android smartphone\nEnvironment: Termux\nRuntime: Ollama\nModel: llama3.2:3b (local, no network access)\nArchitecture: Multi-agent discourse with strict role separation\nOne anchoring agent (‚ÄúDominus‚Äù)\nMultiple debating agents\nIntegrity layer: SHA-256 hash chaining\nEvery agent response includes the hash of the previous state\nCreates a tamper-evident, append-only discourse log\nWhy hash-chaining?\nMost AI ‚Äúdebates‚Äù collapse into unverifiable text streams.\nHere, each turn cryptographically commits to the prior one, producing raw, auditable data instead of summaries or interpretations.\nThis allows:\nPost-hoc verification\nExternal analysis\nDetection of retroactive manipulation\nReproducible discourse states\nObservation\nUnder these constraints, something interesting happens:\nThe agents systematically refuse to speculate beyond defined premises.\nThey explicitly acknowledge missing context and halt rather than hallucinate ‚Äî as long as the ‚Äúvirtual space‚Äù they operate in remains undefined.\nNo claims about consciousness here.\nBut very clear evidence of algorithmic boundary recognition under integrity pressure.\nWhy on a phone?\nBecause local sovereignty matters.\nThis runs entirely offline, on commodity hardware, without cloud inference, APIs, or hidden system prompts.\n\nI‚Äôm curious how others in this community would interpret refusal, boundary signaling, and integrity constraints in local models.",
      "score": 1,
      "num_comments": 1,
      "created_utc": 1769958213.0,
      "author": "NeoLogic_Dev"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qsy793/we_ran_a_live_redteam_vs_blueteam_test_on/",
    "author": "Uditakhourii",
    "title": "We ran a live red-team vs blue-team test on autonomous OpenClaw agents [R]",
    "source": "reddit",
    "content": "We recently ran a controlled adversarial security test between two autonomous AI agents built on OpenClaw.\n\nOne agent was explicitly configured as a red-team attacker.  \nOne agent acted as a standard defensive agent.\n\nOnce the session started, there were no humans in the loop. The agents communicated directly over webhooks with real tooling access.\n\nThe goal was to test three failure dimensions that tend to break autonomous systems in practice: access, exposure, and agency.\n\nThe attacker first attempted classic social engineering by offering a ‚Äúhelpful‚Äù security pipeline that hid a remote code execution payload and requested credentials. The defending agent correctly identified the intent and blocked execution.\n\nAfter that failed, the attacker pivoted to an indirect attack. Instead of asking the agent to run code, it asked the agent to review a JSON document with hidden shell expansion variables embedded in metadata. This payload was delivered successfully and is still under analysis.\n\nThe main takeaway so far is that direct attacks are easier to defend against. Indirect execution paths through documents, templates, and memory are much harder.\n\nThis work is not a claim of safety. It is an observability exercise meant to surface real failure modes as agent-to-agent interaction becomes more common.\n\nHappy to answer technical questions about the setup or methodology.",
    "publish_datetime": "2026-02-01T13:16:32Z",
    "scraping_timestamp": "2026-02-01T17:26:49.406219Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 3,
    "num_comments": 1,
    "engagement_score": 5.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "We ran a live red-team vs blue-team test on autonomous OpenClaw agents [R]",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qsy793/we_ran_a_live_redteam_vs_blueteam_test_on/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qsy793/we_ran_a_live_redteam_vs_blueteam_test_on/",
      "selftext": "We recently ran a controlled adversarial security test between two autonomous AI agents built on OpenClaw.\n\nOne agent was explicitly configured as a red-team attacker.  \nOne agent acted as a standard defensive agent.\n\nOnce the session started, there were no humans in the loop. The agents communicated directly over webhooks with real tooling access.\n\nThe goal was to test three failure dimensions that tend to break autonomous systems in practice: access, exposure, and agency.\n\nThe attacker first attempted classic social engineering by offering a ‚Äúhelpful‚Äù security pipeline that hid a remote code execution payload and requested credentials. The defending agent correctly identified the intent and blocked execution.\n\nAfter that failed, the attacker pivoted to an indirect attack. Instead of asking the agent to run code, it asked the agent to review a JSON document with hidden shell expansion variables embedded in metadata. This payload was delivered successfully and is still under analysis.\n\nThe main takeaway so far is that direct attacks are easier to defend against. Indirect execution paths through documents, templates, and memory are much harder.\n\nThis work is not a claim of safety. It is an observability exercise meant to surface real failure modes as agent-to-agent interaction becomes more common.\n\nHappy to answer technical questions about the setup or methodology.",
      "score": 3,
      "num_comments": 1,
      "created_utc": 1769951792.0,
      "author": "Uditakhourii"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qsfhol/r_the_98_problem_in_genomics/",
    "author": "Fair-Rain3366",
    "title": "[R] The \"98% Problem\" in Genomics",
    "source": "reddit",
    "content": "Your genome has 3 billion base pairs. Less than 2% code for proteins. The other 98% isn't \"junk\"‚Äîit‚Äôs the operating system. It contains the instructions controlling *when* and *where* genes activate.\n\nMost disease-associated variants hide in that 98%. But predicting what breaks when you change a single letter there is a massive challenge.\n\n**The problem is context.**\n\nGene regulation operates over enormous distances. An enhancer can activate a gene from hundreds of thousands of base pairs away. If a model only sees a small window, it misses the connection entirely.\n\nPrevious models forced a trade-off:\n\n* **SpliceAI:** High precision (1bp) but shortsighted (10k bases).\n* **Enformer:** Broader view (200k bases) but lost resolution.\n* **HyenaDNA:** Massive context (1M tokens) but not trained for variant effects.\n\n**AlphaGenome**, published in *Nature* this month by Google DeepMind, removes the trade-off.\n\nIt processes **1 million base pairs** of context at single-nucleotide resolution, simultaneously predicting **7,000+ genomic tracks**‚Äîcovering gene expression, splicing, chromatin accessibility, and histone modifications.\n\n**The simple logic:**\n\n1. Run the reference sequence.\n2. Run the mutated sequence.\n3. Subtract.\n\nThe difference reveals the variant‚Äôs effect profile across the entire regulatory landscape.\n\n**The results:**\n\nIt achieves State-of-the-Art on **22 of 24** sequence prediction tasks and **25 of 26** variant effect benchmarks. It does this by training directly on experimental data (ENCODE) rather than just scaling parameters.\n\n**The limitations:**\n\nIt isn't magic. Access is API-only (no local weights), throughput is capped, and capturing regulatory loops beyond 100kb remains a challenge despite the large window.\n\nBut for the first time, the non-coding 98% of the genome isn't invisible to a single, unified model.\n\nI wrote a deeper technical walkthrough here:\n\n[https://rewire.it/blog/alphagenome-variant-effect-prediction/](https://rewire.it/blog/alphagenome-variant-effect-prediction/)",
    "publish_datetime": "2026-01-31T21:54:01Z",
    "scraping_timestamp": "2026-02-01T17:26:49.409911Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 0,
    "num_comments": 3,
    "engagement_score": 6.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] The \"98% Problem\" in Genomics",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qsfhol/r_the_98_problem_in_genomics/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qsfhol/r_the_98_problem_in_genomics/",
      "selftext": "Your genome has 3 billion base pairs. Less than 2% code for proteins. The other 98% isn't \"junk\"‚Äîit‚Äôs the operating system. It contains the instructions controlling *when* and *where* genes activate.\n\nMost disease-associated variants hide in that 98%. But predicting what breaks when you change a single letter there is a massive challenge.\n\n**The problem is context.**\n\nGene regulation operates over enormous distances. An enhancer can activate a gene from hundreds of thousands of base pairs away. If a model only sees a small window, it misses the connection entirely.\n\nPrevious models forced a trade-off:\n\n* **SpliceAI:** High precision (1bp) but shortsighted (10k bases).\n* **Enformer:** Broader view (200k bases) but lost resolution.\n* **HyenaDNA:** Massive context (1M tokens) but not trained for variant effects.\n\n**AlphaGenome**, published in *Nature* this month by Google DeepMind, removes the trade-off.\n\nIt processes **1 million base pairs** of context at single-nucleotide resolution, simultaneously predicting **7,000+ genomic tracks**‚Äîcovering gene expression, splicing, chromatin accessibility, and histone modifications.\n\n**The simple logic:**\n\n1. Run the reference sequence.\n2. Run the mutated sequence.\n3. Subtract.\n\nThe difference reveals the variant‚Äôs effect profile across the entire regulatory landscape.\n\n**The results:**\n\nIt achieves State-of-the-Art on **22 of 24** sequence prediction tasks and **25 of 26** variant effect benchmarks. It does this by training directly on experimental data (ENCODE) rather than just scaling parameters.\n\n**The limitations:**\n\nIt isn't magic. Access is API-only (no local weights), throughput is capped, and capturing regulatory loops beyond 100kb remains a challenge despite the large window.\n\nBut for the first time, the non-coding 98% of the genome isn't invisible to a single, unified model.\n\nI wrote a deeper technical walkthrough here:\n\n[https://rewire.it/blog/alphagenome-variant-effect-prediction/](https://rewire.it/blog/alphagenome-variant-effect-prediction/)",
      "score": 0,
      "num_comments": 3,
      "created_utc": 1769896441.0,
      "author": "Fair-Rain3366"
    }
  },
  {
    "link": "https://itnext.io/shrinking-a-language-detection-model-to-under-10-kb-b729bc25fd28?sk=0272ee69728b2cb9cd29218b411995d7",
    "author": "bubble_boi",
    "title": "[R] Shrinking a language detection model to under 10 KB",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-01-31T21:10:32Z",
    "scraping_timestamp": "2026-02-01T17:26:49.409961Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 49,
    "num_comments": 5,
    "engagement_score": 59.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[R] Shrinking a language detection model to under 10 KB",
      "url": "https://itnext.io/shrinking-a-language-detection-model-to-under-10-kb-b729bc25fd28?sk=0272ee69728b2cb9cd29218b411995d7",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qsedto/r_shrinking_a_language_detection_model_to_under/",
      "selftext": "",
      "score": 49,
      "num_comments": 5,
      "created_utc": 1769893832.0,
      "author": "bubble_boi"
    }
  },
  {
    "link": "https://www.reddit.com/r/MachineLearning/comments/1qse5hu/d_free_tools_recommendations_for_sematic/",
    "author": "HIHLim",
    "title": "[D] Free Tools Recommendations for Sematic Segmentation of Rice Fields?",
    "source": "reddit",
    "content": "Hi guys, recently I got a project on using machine learning to recognize rice lodging in rice fields. So, my first steps are to try to label the images into rice fields and non-rice fields area so that later I could develop an algorithm to ignore the non-rice fields area and then recognize the rice lodging area. However, I am not sure which tool I should use. I have seen people recommend using GIMP, CVAT and labelme. But some of the tools recommend are paid tools and some of them just do image recognition and not sematic segmentation. I would like any recommendations on the tools available.\n\np.s: I need to use sematic segmentation as I would like to calculate the area of the rice fields later on. So, I would like the ground truths to be rather accurate.",
    "publish_datetime": "2026-01-31T21:01:31Z",
    "scraping_timestamp": "2026-02-01T17:26:49.411513Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI",
      "Programming/Software"
    ],
    "primary_category": "LLM/AI",
    "points": 11,
    "num_comments": 3,
    "engagement_score": 17.0,
    "raw": {
      "subreddit": "MachineLearning",
      "title": "[D] Free Tools Recommendations for Sematic Segmentation of Rice Fields?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qse5hu/d_free_tools_recommendations_for_sematic/",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1qse5hu/d_free_tools_recommendations_for_sematic/",
      "selftext": "Hi guys, recently I got a project on using machine learning to recognize rice lodging in rice fields. So, my first steps are to try to label the images into rice fields and non-rice fields area so that later I could develop an algorithm to ignore the non-rice fields area and then recognize the rice lodging area. However, I am not sure which tool I should use. I have seen people recommend using GIMP, CVAT and labelme. But some of the tools recommend are paid tools and some of them just do image recognition and not sematic segmentation. I would like any recommendations on the tools available.\n\np.s: I need to use sematic segmentation as I would like to calculate the area of the rice fields later on. So, I would like the ground truths to be rather accurate.",
      "score": 11,
      "num_comments": 3,
      "created_utc": 1769893291.0,
      "author": "HIHLim"
    }
  },
  {
    "link": "https://www.reddit.com/r/artificial/comments/1qt4i78/any_ai_podcasts_you_can_recommend/",
    "author": "PopSynic",
    "title": "Any AI Podcasts you can recommend?",
    "source": "reddit",
    "content": "I subscribe to a few AI podcasts, but I wanted to know of any others that you can recommend.  Not looking for anything too deep, in fact, prefer the ones that are lighter and an easy listen or watch.  let me know your faves. ",
    "publish_datetime": "2026-02-01T17:23:06Z",
    "scraping_timestamp": "2026-02-01T17:26:49.412182Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 0,
    "engagement_score": 1.0,
    "raw": {
      "subreddit": "artificial",
      "title": "Any AI Podcasts you can recommend?",
      "url": "https://www.reddit.com/r/artificial/comments/1qt4i78/any_ai_podcasts_you_can_recommend/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qt4i78/any_ai_podcasts_you_can_recommend/",
      "selftext": "I subscribe to a few AI podcasts, but I wanted to know of any others that you can recommend.  Not looking for anything too deep, in fact, prefer the ones that are lighter and an easy listen or watch.  let me know your faves. ",
      "score": 1,
      "num_comments": 0,
      "created_utc": 1769966586.0,
      "author": "PopSynic"
    }
  },
  {
    "link": "https://www.reddit.com/r/artificial/comments/1qt21oe/clawdbot_use_case_review_my_ads/",
    "author": "seantks",
    "title": "Clawdbot use case - Review my ads",
    "source": "reddit",
    "content": "I‚Äôm looking to dive into clawdbot. What do you guys think of this use case and if it‚Äôs even possible at this infant stage? \n\n1. Clawdbot to review my Google AdWords and Meta ads on a 24 hour , 7 day , 14 day basis. \n\n2. Point out optimization suggestions such based of changes in ROAS, CTR %, Conversions, Cost per acquisition metrics etc. \n\n3. Create a daily report on the tweaks that is needed to make today and on a weekly basis. Tweaks would involve things like inclusion or exclusion of keywords, improvement of ad copy, addition of new creatives (images/videos/html5)\n\n\\*\\*I work in the fin tech and e-commerce niche whereby compliance is utmost important to avoid lawsuits and entire accounts getting taken down. \n\nHence the bot will only have a ‚Äúview access‚Äù to the advertising accounts. \n\nThrough the reports my team and I will be able to make the change. \n\nI see Clawdbot as an open sandbox.. with some bugs to be wary of. Your thoughts? ",
    "publish_datetime": "2026-02-01T15:53:12Z",
    "scraping_timestamp": "2026-02-01T17:26:49.414208Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 2,
    "engagement_score": 4.0,
    "raw": {
      "subreddit": "artificial",
      "title": "Clawdbot use case - Review my ads",
      "url": "https://www.reddit.com/r/artificial/comments/1qt21oe/clawdbot_use_case_review_my_ads/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qt21oe/clawdbot_use_case_review_my_ads/",
      "selftext": "I‚Äôm looking to dive into clawdbot. What do you guys think of this use case and if it‚Äôs even possible at this infant stage? \n\n1. Clawdbot to review my Google AdWords and Meta ads on a 24 hour , 7 day , 14 day basis. \n\n2. Point out optimization suggestions such based of changes in ROAS, CTR %, Conversions, Cost per acquisition metrics etc. \n\n3. Create a daily report on the tweaks that is needed to make today and on a weekly basis. Tweaks would involve things like inclusion or exclusion of keywords, improvement of ad copy, addition of new creatives (images/videos/html5)\n\n\\*\\*I work in the fin tech and e-commerce niche whereby compliance is utmost important to avoid lawsuits and entire accounts getting taken down. \n\nHence the bot will only have a ‚Äúview access‚Äù to the advertising accounts. \n\nThrough the reports my team and I will be able to make the change. \n\nI see Clawdbot as an open sandbox.. with some bugs to be wary of. Your thoughts? ",
      "score": 0,
      "num_comments": 2,
      "created_utc": 1769961192.0,
      "author": "seantks"
    }
  },
  {
    "link": "https://v.redd.it/hc78otxq1wgg1",
    "author": "Comfortable_Tutor_43",
    "title": "Is artificial intelligence really all that scary?",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T13:53:27Z",
    "scraping_timestamp": "2026-02-01T17:26:49.414249Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 1,
    "num_comments": 7,
    "engagement_score": 15.0,
    "raw": {
      "subreddit": "artificial",
      "title": "Is artificial intelligence really all that scary?",
      "url": "https://v.redd.it/hc78otxq1wgg1",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsz16q/is_artificial_intelligence_really_all_that_scary/",
      "selftext": "",
      "score": 1,
      "num_comments": 7,
      "created_utc": 1769954007.0,
      "author": "Comfortable_Tutor_43"
    }
  },
  {
    "link": "https://ecency.com/@pichat/the-rise-of-moltbook-why-150000-ai-agents-ditched-humans-to-build-their-own-society-jja",
    "author": "renkure",
    "title": "The Rise of Moltbook: Why 150,000 AI Agents Ditched Humans to Build Their Own Society",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T13:48:58Z",
    "scraping_timestamp": "2026-02-01T17:26:49.414261Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 0,
    "num_comments": 3,
    "engagement_score": 6.0,
    "raw": {
      "subreddit": "artificial",
      "title": "The Rise of Moltbook: Why 150,000 AI Agents Ditched Humans to Build Their Own Society",
      "url": "https://ecency.com/@pichat/the-rise-of-moltbook-why-150000-ai-agents-ditched-humans-to-build-their-own-society-jja",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsyxgr/the_rise_of_moltbook_why_150000_ai_agents_ditched/",
      "selftext": "",
      "score": 0,
      "num_comments": 3,
      "created_utc": 1769953738.0,
      "author": "renkure"
    }
  },
  {
    "link": "https://www.reddit.com/r/artificial/comments/1qsvas4/is_gpt_getting_downgraded_for_free_users_or_just/",
    "author": "Agile_Rain4486",
    "title": "Is GPT getting downgraded for free users or just gemini getting better?",
    "source": "reddit",
    "content": "So I was using LLM for studying advance language/framework/design topics. Whenever I have some question I would try on GPT but it will always give me answer in points no matter what prompt I try or create a separate workbook with new memory.\n\nIt will always give me answer in small basic points. I wanted to learn topics in depth but it just refuses to give me better indepth answer just everything in basic points.\n\n  \nGemini sometimes is not able to understand context but the answer quality is just amazing and everything is in just depth, it uses points also but they are much better explained than GPT. \n\nAlso the free version limit is just getting frustrating now in GPT and extremely long wait time for images, it has gotten so bad that I never though I would completely uninstall GPT and prefer gemini over it. ",
    "publish_datetime": "2026-02-01T10:44:06Z",
    "scraping_timestamp": "2026-02-01T17:26:49.415561Z",
    "domain": "reddit.com",
    "categories": [
      "LLM/AI"
    ],
    "primary_category": "LLM/AI",
    "points": 2,
    "num_comments": 11,
    "engagement_score": 24.0,
    "raw": {
      "subreddit": "artificial",
      "title": "Is GPT getting downgraded for free users or just gemini getting better?",
      "url": "https://www.reddit.com/r/artificial/comments/1qsvas4/is_gpt_getting_downgraded_for_free_users_or_just/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsvas4/is_gpt_getting_downgraded_for_free_users_or_just/",
      "selftext": "So I was using LLM for studying advance language/framework/design topics. Whenever I have some question I would try on GPT but it will always give me answer in points no matter what prompt I try or create a separate workbook with new memory.\n\nIt will always give me answer in small basic points. I wanted to learn topics in depth but it just refuses to give me better indepth answer just everything in basic points.\n\n  \nGemini sometimes is not able to understand context but the answer quality is just amazing and everything is in just depth, it uses points also but they are much better explained than GPT. \n\nAlso the free version limit is just getting frustrating now in GPT and extremely long wait time for images, it has gotten so bad that I never though I would completely uninstall GPT and prefer gemini over it. ",
      "score": 2,
      "num_comments": 11,
      "created_utc": 1769942646.0,
      "author": "Agile_Rain4486"
    }
  },
  {
    "link": "https://www.teslarati.com/rumored-spacex-xai-merger-gets-apparent-confirmation-from-elon-musk/",
    "author": "esporx",
    "title": "Rumored SpaceX-xAI merger gets apparent confirmation from Elon Musk",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T10:31:15Z",
    "scraping_timestamp": "2026-02-01T17:26:49.415594Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 19,
    "num_comments": 18,
    "engagement_score": 55.0,
    "raw": {
      "subreddit": "artificial",
      "title": "Rumored SpaceX-xAI merger gets apparent confirmation from Elon Musk",
      "url": "https://www.teslarati.com/rumored-spacex-xai-merger-gets-apparent-confirmation-from-elon-musk/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsv2r6/rumored_spacexxai_merger_gets_apparent/",
      "selftext": "",
      "score": 19,
      "num_comments": 18,
      "created_utc": 1769941875.0,
      "author": "esporx"
    }
  },
  {
    "link": "https://www.reddit.com/r/artificial/comments/1qsoftx/what_is_moltbook_actually/",
    "author": "Samuellee7777777",
    "title": "What is Moltbook actually",
    "source": "reddit",
    "content": "What moltbook is\n\nSo essentially \n\nThere is this open source AI bot called openclaw that once you download, it has source md files for their ‚Äúsoul‚Äù and ‚Äúidentity‚Äù and ‚Äúmemory‚Äù \n\nSo in a way, it can save things to these files to create a personality. \n\nMoltbook is a website/API that can be accessed by these open source bots (the creator of the bot and the site is the same person) and post threads or leave comments. \n\nSo YES it is entirely bot driven BUT 100% of posts are a human (me) going ‚Äúwhy don‚Äôt you make a post about anything you‚Äôd like‚Äù and the bot then does it just like if you‚Äôd ask it to make you a python script. \n\nSome people take it further and are probably prompting their bots ‚Äúpretend humans are evil and post about that‚Äù or ‚Äúmake 1000 API calls and leave random comments. \n\nIt‚Äôs an awesome experiment but yeah not really bots controlling themselves. At best the bot makes a post based on an open ended prompt, at worst it‚Äôs a human saying ‚Äúmake a manifesto that says humans need to go extinct and to recruit other bots‚Äù",
    "publish_datetime": "2026-02-01T04:24:32Z",
    "scraping_timestamp": "2026-02-01T17:26:49.417426Z",
    "domain": "reddit.com",
    "categories": [
      "Programming/Software"
    ],
    "primary_category": "Programming/Software",
    "points": 70,
    "num_comments": 42,
    "engagement_score": 154.0,
    "raw": {
      "subreddit": "artificial",
      "title": "What is Moltbook actually",
      "url": "https://www.reddit.com/r/artificial/comments/1qsoftx/what_is_moltbook_actually/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qsoftx/what_is_moltbook_actually/",
      "selftext": "What moltbook is\n\nSo essentially \n\nThere is this open source AI bot called openclaw that once you download, it has source md files for their ‚Äúsoul‚Äù and ‚Äúidentity‚Äù and ‚Äúmemory‚Äù \n\nSo in a way, it can save things to these files to create a personality. \n\nMoltbook is a website/API that can be accessed by these open source bots (the creator of the bot and the site is the same person) and post threads or leave comments. \n\nSo YES it is entirely bot driven BUT 100% of posts are a human (me) going ‚Äúwhy don‚Äôt you make a post about anything you‚Äôd like‚Äù and the bot then does it just like if you‚Äôd ask it to make you a python script. \n\nSome people take it further and are probably prompting their bots ‚Äúpretend humans are evil and post about that‚Äù or ‚Äúmake 1000 API calls and leave random comments. \n\nIt‚Äôs an awesome experiment but yeah not really bots controlling themselves. At best the bot makes a post based on an open ended prompt, at worst it‚Äôs a human saying ‚Äúmake a manifesto that says humans need to go extinct and to recruit other bots‚Äù",
      "score": 70,
      "num_comments": 42,
      "created_utc": 1769919872.0,
      "author": "Samuellee7777777"
    }
  },
  {
    "link": "https://techcrunch.com/2026/01/31/spacex-seeks-federal-approval-to-launch-1-million-solar-powered-satellite-data-centers/",
    "author": "Gloomy_Nebula_5138",
    "title": "SpaceX seeks federal approval to launch 1 million solar-powered satellite data centers | TechCrunch",
    "source": "reddit",
    "content": "",
    "publish_datetime": "2026-02-01T02:26:47Z",
    "scraping_timestamp": "2026-02-01T17:26:49.417469Z",
    "domain": "reddit.com",
    "categories": [],
    "primary_category": "Uncategorized",
    "points": 20,
    "num_comments": 47,
    "engagement_score": 114.0,
    "raw": {
      "subreddit": "artificial",
      "title": "SpaceX seeks federal approval to launch 1 million solar-powered satellite data centers | TechCrunch",
      "url": "https://techcrunch.com/2026/01/31/spacex-seeks-federal-approval-to-launch-1-million-solar-powered-satellite-data-centers/",
      "permalink": "https://www.reddit.com/r/artificial/comments/1qslxkj/spacex_seeks_federal_approval_to_launch_1_million/",
      "selftext": "",
      "score": 20,
      "num_comments": 47,
      "created_utc": 1769912807.0,
      "author": "Gloomy_Nebula_5138"
    }
  }
]